{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from epitome.models import *\n",
    "from epitome.functions import *\n",
    "# from epitome.generators import *\n",
    "from epitome.dataset import *\n",
    "\n",
    "\n",
    "import tempfile\n",
    "import os\n",
    "import time\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_runtime(data,\n",
    "                 label_cell_types,  # used for labels. Should be all for train/eval and subset for test\n",
    "                 eval_cell_types,   # used for rotating features. Should be all - test for train/eval\n",
    "                 matrix,\n",
    "                 targetmap,\n",
    "                 cellmap,\n",
    "                 radii,\n",
    "                 similarity_targets = ['DNase'],\n",
    "                 mode = Dataset.RUNTIME,\n",
    "                 similarity_matrix = None,\n",
    "                 indices = None,\n",
    "                 return_feature_names = False,\n",
    "                 **kwargs):\n",
    "    \"\"\"\n",
    "    Takes Deepsea data and calculates distance metrics from cell types whose locations\n",
    "    are specified by label_cell_indices, and the other cell types in the set. Label space is only one cell type.\n",
    "    :param data: dictionary of matrices. Should have keys x and y. x contains n by 1000 rows. y contains n by 919 labels.\n",
    "    :param label_cell_types: list of cell types to be rotated through and used as labels (subset of eval_cell_types)\n",
    "    :param eval_cell_types: list of cell types to be used in evaluation (includes label_cell_types)\n",
    "    :param matrix: matrix of celltype, target positions\n",
    "    :param targetmap: map of column target positions in matrix\n",
    "    :param cellmap: map of row cell type positions in matrix\n",
    "    :param radii: radii to compute similarity distances from\n",
    "    :param similarity_targets: list of targets used to measure cell type similarity (default is DNase-seq)\n",
    "    :param mode: Dataset.TRAIN, VALID, TEST or RUNTIME\n",
    "    :param similarity_matrix: matrix with shape (len(similarity_targets), genome_size) containing binary 0/1s of peaks for similarity_targets\n",
    "    to be compared in the CASV.\n",
    "    :param indices: indices in genome to generate records for.\n",
    "    :param return_feature_names: boolean whether to return string names of features\n",
    "    :param kwargs: kargs\n",
    "\n",
    "    :returns: generator of data with three elements:\n",
    "        1. record features\n",
    "        2. record labels for a given cell type\n",
    "        3. 0/1 mask of labels that have validation data. For example, if this record is for celltype A549,\n",
    "        and A549 does not have data for ATF3, there will be a 0 in the position corresponding to the label space.\n",
    "    \"\"\"\n",
    "\n",
    "    if similarity_matrix is not None:\n",
    "        if len(similarity_matrix.shape) == 1:\n",
    "            similarity_matrix = similarity_matrix[None,:]\n",
    "\n",
    "    if type(similarity_targets) is not list:\n",
    "        similarity_targets = [similarity_targets]\n",
    "\n",
    "    if len(similarity_targets) == 0 and len(radii) > 0:\n",
    "        raise ValueError(\"Cannot set radii to anything if there are no similarity assays, but found len(radii)=%i\" % len(radii))\n",
    "\n",
    "    # get indices for features. rows are cells and cols are targets\n",
    "    cellmap_idx = [cellmap[c] for c in list(eval_cell_types)]\n",
    "    feature_cell_indices = matrix[cellmap_idx,:]\n",
    "\n",
    "    # indices to be deleted. used for similarity comparison, not predictions.\n",
    "    delete_indices = np.array([targetmap[s] for s in similarity_targets]).astype(int)\n",
    "\n",
    "    # make sure no similarity comparison data is missing for all cell types\n",
    "    assert np.invert(np.any(feature_cell_indices[:,delete_indices] == -1)), \\\n",
    "        \"missing data for similarity target at %s\" % (np.where(feature_cell_indices[:,delete_indices] == -1)[0])\n",
    "\n",
    "    # names of labels that are being predicted\n",
    "    feature_targets = [a for a in list(targetmap)] # targets used as features for each evaluation cell type\n",
    "    label_targets = [a for a in feature_targets if a not in similarity_targets]\n",
    "\n",
    "    if (not isinstance(mode, Dataset)):\n",
    "        raise ValueError(\"mode is not a Dataset enum\")\n",
    "\n",
    "    if (not isinstance(indices, np.ndarray) and not isinstance(indices, list)):\n",
    "        # model performs better when there are less 0s\n",
    "        if mode == Dataset.TRAIN:\n",
    "            feature_indices = np.concatenate(list(map(lambda c: EpitomeDataset.get_y_indices_for_cell(matrix, cellmap, c),\n",
    "                                     list(cellmap))))\n",
    "            feature_indices = feature_indices[feature_indices != -1]\n",
    "\n",
    "            # need to re-proportion the indices to oversample underrepresented labels\n",
    "            if (len(list(targetmap)) > 2):\n",
    "                # configure y: label matrix of ChIP for all targets from all cell lines in train\n",
    "                indices = np.concatenate([EpitomeDataset.get_y_indices_for_target(matrix, targetmap, target) for target in label_targets])\n",
    "                indices = indices[indices != -1]\n",
    "                y = data[indices, :].T\n",
    "                m = MLSMOTE(y)\n",
    "                indices = m.fit_resample()\n",
    "\n",
    "            else:\n",
    "                # single TF model\n",
    "                # get indices for DNase and chip for this mark\n",
    "                feature_indices = np.concatenate(list(map(lambda c: EpitomeDataset.get_y_indices_for_cell(matrix, cellmap, c),\n",
    "                                                     list(cellmap))))\n",
    "\n",
    "                # chop off targets being used in similarity metric\n",
    "                not_similarity_indices = np.array([v for k,v in targetmap.items() if k not in similarity_targets])\n",
    "                TF_indices = feature_indices.reshape([len(cellmap),len(targetmap)])[:,not_similarity_indices]\n",
    "\n",
    "                TF_indices =  TF_indices[TF_indices != -1]\n",
    "                feature_indices = feature_indices[feature_indices != -1]\n",
    "\n",
    "                # sites where TF is bound in at least 2 cell line\n",
    "                positive_indices = np.where(np.sum(data[TF_indices,:], axis=0) > 1)[0]\n",
    "                indices_probs = np.ones([data.shape[1]])\n",
    "                indices_probs[positive_indices] = 0\n",
    "                indices_probs = indices_probs/np.sum(indices_probs, keepdims=1)\n",
    "\n",
    "                # If there are nans, it means there were no 0 cases.\n",
    "                # We use this for testing so models converge quickly\n",
    "                # with all ones.\n",
    "                if np.any(np.isnan(indices_probs)):\n",
    "                  print(\"Warning: no negative examples in dataset!!!\")\n",
    "                  indices_probs[:] = 1/indices_probs.shape[0]\n",
    "\n",
    "                # randomly select 10 fold sites where TF is not in any cell line\n",
    "                negative_indices = np.random.choice(np.arange(0,data.shape[1]),\n",
    "                                                    positive_indices.shape[0] * 10,\n",
    "                                                    p=indices_probs)\n",
    "                indices = np.sort(np.concatenate([negative_indices, positive_indices]))\n",
    "\n",
    "        else:\n",
    "            indices = range(0, data.shape[-1]) # not training mode, set to all points\n",
    "\n",
    "    if (mode == Dataset.RUNTIME):\n",
    "        label_cell_types = [\"PLACEHOLDER_CELL\"]\n",
    "        if similarity_matrix is None:\n",
    "            raise Exception(\"similarity_matrix must be defined in runtime mode\")\n",
    "        assert similarity_matrix.shape[0] == len(similarity_targets), \\\n",
    "            \"similarity_matrix is missing data for targets (should have %i rows)\" % (len(similarity_targets))\n",
    "        random_cell = list(cellmap)[0] # placeholder to get label vector length\n",
    "\n",
    "    print(\"using %s as labels for mode %s\" % (label_cell_types, mode))\n",
    "\n",
    "    # string of radii for meta data labeling\n",
    "    radii_str = list(map(lambda x: \"RADII_%i\" % x, radii))\n",
    "\n",
    "    def g():\n",
    "        for i in indices: # for all records specified\n",
    "\n",
    "            for (cell) in label_cell_types: # for all cell types to be used in labels\n",
    "\n",
    "                # labels for this cell\n",
    "                if (mode != Dataset.RUNTIME):\n",
    "                    label_cell_indices = EpitomeDataset.get_y_indices_for_cell(matrix, cellmap, cell)\n",
    "\n",
    "                    # delete all indices being used in the similarity computation\n",
    "                    label_cell_indices_no_similarities = np.delete(label_cell_indices, delete_indices)\n",
    "\n",
    "                    # Copy target_index_no_similarities and turn into mask of 0/1 for whether data for this cell type for\n",
    "                    # a given label is available.\n",
    "                    target_mask = np.copy(label_cell_indices_no_similarities)\n",
    "                    target_mask[target_mask > -1] = 1\n",
    "                    target_mask[target_mask == -1] = 0\n",
    "\n",
    "                else:\n",
    "                    label_count = len(EpitomeDataset.get_y_indices_for_cell(matrix, cellmap, random_cell))-len(similarity_targets)\n",
    "\n",
    "                    # Mask and labels are all 0's because labels are missing during runtime\n",
    "                    garbage_labels = target_mask = np.zeros(label_count)\n",
    "\n",
    "\n",
    "                # get indices for targets used in similarity computation\n",
    "                # for cell types that are going to be features\n",
    "                similarity_indices = feature_cell_indices[:, delete_indices]\n",
    "\n",
    "\n",
    "                # get indices for each radius in radii\n",
    "                radius_ranges = list(map(lambda x: get_radius_indices(radii, x, i, data.shape[-1]), range(len(radii))))\n",
    "\n",
    "                if len(radius_ranges) > 0:\n",
    "                    radius_indices = np.concatenate(radius_ranges)\n",
    "\n",
    "                    cell_train_data = data[similarity_indices[:,:,None],radius_indices]\n",
    "\n",
    "                    if mode == Dataset.RUNTIME:\n",
    "\n",
    "                        pos = cell_train_data*similarity_matrix[:,radius_indices]\n",
    "#                         agree = cell_train_data == similarity_matrix[:,radius_indices]\n",
    "\n",
    "                    else:\n",
    "                        cell_label_data = data[label_cell_indices[delete_indices][:,None],radius_indices]\n",
    "\n",
    "                        # remove middle dimension and flatten similarity targets\n",
    "                        pos = (cell_train_data*cell_label_data)\n",
    "#                         agree = (cell_train_data == cell_label_data)\n",
    "\n",
    "                    # get indices to split on. remove last because it is empty\n",
    "                    split_indices = np.cumsum([len(i) for i in radius_ranges])[:-1]\n",
    "                    # slice arrays by radii\n",
    "                    pos_arrays = np.split(pos, split_indices, axis= -1 )\n",
    "#                     agree_arrays = np.split(agree, split_indices, axis = -1)\n",
    "\n",
    "                    similarities = np.stack(list(map(lambda x: np.average(x, axis = -1), pos_arrays)),axis=1)\n",
    "                else:\n",
    "                    # no radius, so no similarities. just an empty placeholder\n",
    "                    similarities = np.zeros((len(eval_cell_types),0,0))\n",
    "\n",
    "                # reshape similarities to flatten 1st dimension, which are the targets\n",
    "                # results in the odering:\n",
    "                ## row 1: cell 1: pos for each target and agree for each target for each radius\n",
    "                similarities = similarities.reshape(similarities.shape[0], similarities.shape[1]*similarities.shape[2])\n",
    "\n",
    "                ##### Concatenate all cell type features together ####\n",
    "                final_features = np.concatenate([data[feature_cell_indices,i], similarities],axis=1).flatten()\n",
    "\n",
    "                # mask missing data\n",
    "                f_mask = np.concatenate([feature_cell_indices!=-1,\n",
    "                                         np.ones(similarities.shape)],axis=1).flatten()\n",
    "                final_features = final_features[f_mask != 0]\n",
    "\n",
    "                if (mode != Dataset.RUNTIME):\n",
    "                    labels = data[label_cell_indices_no_similarities,i]\n",
    "\n",
    "                else: # used when just predicting\n",
    "                    # The features going into the example.\n",
    "                    labels = garbage_labels # all 0's\n",
    "\n",
    "                # append labels and targetmask\n",
    "                final = final_features\n",
    "\n",
    "                #### Finish appending feature labels together ####\n",
    "                # if (return_feature_names):\n",
    "                all_labels = []\n",
    "                feature_names = []\n",
    "                similarity_labels_agreement = ['r%i_%s' % (radius, 'agree') for radius in radii]\n",
    "#                 similarity_labels_dp = ['r%i_%s' % (radius, 'dp') for radius in radii]\n",
    "                similarity_labels = similarity_labels_agreement\n",
    "\n",
    "                # concatenate together feature names\n",
    "                for j,c in enumerate(eval_cell_types):\n",
    "                    tmp = np.array(feature_targets)[feature_cell_indices[j,:] != -1]\n",
    "                    al = ['%s_%s' % (c, a) for a in tmp]\n",
    "                    sl = ['%s_%s' % (c, s) for s in similarity_labels]\n",
    "\n",
    "                    feature_names.append(al)\n",
    "                    feature_names.append(sl)\n",
    "\n",
    "                all_labels.append(np.concatenate(feature_names))\n",
    "                # all_labels.append(['lbl_%s_%s' % (cell, a) for a in label_targets]) # of form lbl_cellline_target\n",
    "                # all_labels.append(['mask_%s_%s' % (cell, a) for a in label_targets]) # of form mask_cellline_target\n",
    "\n",
    "                    # yield (final, tuple(all_labels))\n",
    "                all_labels = all_labels[0]\n",
    "                indx_to_keep = []\n",
    "                for i in range(len(all_labels)):\n",
    "                    for a in similarity_targets:\n",
    "                        # print(a, all_labels[i])\n",
    "                        if a in all_labels[i]:\n",
    "                            \n",
    "                            indx_to_keep.append(final[i])\n",
    "\n",
    "                yield np.array(indx_to_keep)\n",
    "\n",
    "\n",
    "    return g\n",
    "\n",
    "def load_data_no_label_mask(data,\n",
    "                 label_cell_types,  # used for labels. Should be all for train/eval and subset for test\n",
    "                 eval_cell_types,   # used for rotating features. Should be all - test for train/eval\n",
    "                 matrix,\n",
    "                 targetmap,\n",
    "                 cellmap,\n",
    "                 radii,\n",
    "                 similarity_targets = ['DNase'],\n",
    "                 mode = Dataset.TRAIN,\n",
    "                 similarity_matrix = None,\n",
    "                 indices = None,\n",
    "                 return_feature_names = False,\n",
    "                 **kwargs):\n",
    "    \"\"\"\n",
    "    Takes Deepsea data and calculates distance metrics from cell types whose locations\n",
    "    are specified by label_cell_indices, and the other cell types in the set. Label space is only one cell type.\n",
    "    :param data: dictionary of matrices. Should have keys x and y. x contains n by 1000 rows. y contains n by 919 labels.\n",
    "    :param label_cell_types: list of cell types to be rotated through and used as labels (subset of eval_cell_types)\n",
    "    :param eval_cell_types: list of cell types to be used in evaluation (includes label_cell_types)\n",
    "    :param matrix: matrix of celltype, target positions\n",
    "    :param targetmap: map of column target positions in matrix\n",
    "    :param cellmap: map of row cell type positions in matrix\n",
    "    :param radii: radii to compute similarity distances from\n",
    "    :param similarity_targets: list of targets used to measure cell type similarity (default is DNase-seq)\n",
    "    :param mode: Dataset.TRAIN, VALID, TEST or RUNTIME\n",
    "    :param similarity_matrix: matrix with shape (len(similarity_targets), genome_size) containing binary 0/1s of peaks for similarity_targets\n",
    "    to be compared in the CASV.\n",
    "    :param indices: indices in genome to generate records for.\n",
    "    :param return_feature_names: boolean whether to return string names of features\n",
    "    :param kwargs: kargs\n",
    "\n",
    "    :returns: generator of data with three elements:\n",
    "        1. record features\n",
    "        2. record labels for a given cell type\n",
    "        3. 0/1 mask of labels that have validation data. For example, if this record is for celltype A549,\n",
    "        and A549 does not have data for ATF3, there will be a 0 in the position corresponding to the label space.\n",
    "    \"\"\"\n",
    "\n",
    "    # reshape similarity_matrix to a matrix if there is only one target\n",
    "    if similarity_matrix is not None:\n",
    "        if len(similarity_matrix.shape) == 1:\n",
    "            similarity_matrix = similarity_matrix[None,:]\n",
    "\n",
    "    if type(similarity_targets) is not list:\n",
    "        similarity_targets = [similarity_targets]\n",
    "\n",
    "    if len(similarity_targets) == 0 and len(radii) > 0:\n",
    "        raise ValueError(\"Cannot set radii to anything if there are no similarity assays, but found len(radii)=%i\" % len(radii))\n",
    "\n",
    "    # get indices for features. rows are cells and cols are targets\n",
    "    cellmap_idx = [cellmap[c] for c in list(eval_cell_types)]\n",
    "    feature_cell_indices = matrix[cellmap_idx,:]\n",
    "\n",
    "    # indices to be deleted. used for similarity comparison, not predictions.\n",
    "    delete_indices = np.array([targetmap[s] for s in similarity_targets]).astype(int)\n",
    "\n",
    "    # make sure no similarity comparison data is missing for all cell types\n",
    "    assert np.invert(np.any(feature_cell_indices[:,delete_indices] == -1)), \\\n",
    "        \"missing data for similarity target at %s\" % (np.where(feature_cell_indices[:,delete_indices] == -1)[0])\n",
    "\n",
    "    # names of labels that are being predicted\n",
    "    feature_targets = [a for a in list(targetmap)] # targets used as features for each evaluation cell type\n",
    "    label_targets = [a for a in feature_targets if a not in similarity_targets]\n",
    "\n",
    "    if (not isinstance(mode, Dataset)):\n",
    "        raise ValueError(\"mode is not a Dataset enum\")\n",
    "\n",
    "    if (not isinstance(indices, np.ndarray) and not isinstance(indices, list)):\n",
    "        # model performs better when there are less 0s\n",
    "        if mode == Dataset.TRAIN:\n",
    "            feature_indices = np.concatenate(list(map(lambda c: EpitomeDataset.get_y_indices_for_cell(matrix, cellmap, c),\n",
    "                                     list(cellmap))))\n",
    "            feature_indices = feature_indices[feature_indices != -1]\n",
    "\n",
    "            # need to re-proportion the indices to oversample underrepresented labels\n",
    "            if (len(list(targetmap)) > 2):\n",
    "                # configure y: label matrix of ChIP for all targets from all cell lines in train\n",
    "                indices = np.concatenate([EpitomeDataset.get_y_indices_for_target(matrix, targetmap, target) for target in label_targets])\n",
    "                indices = indices[indices != -1]\n",
    "                y = data[indices, :].T\n",
    "                m = MLSMOTE(y)\n",
    "                indices = m.fit_resample()\n",
    "\n",
    "            else:\n",
    "                # single TF model\n",
    "                # get indices for DNase and chip for this mark\n",
    "                feature_indices = np.concatenate(list(map(lambda c: EpitomeDataset.get_y_indices_for_cell(matrix, cellmap, c),\n",
    "                                                     list(cellmap))))\n",
    "\n",
    "                # chop off targets being used in similarity metric\n",
    "                not_similarity_indices = np.array([v for k,v in targetmap.items() if k not in similarity_targets])\n",
    "                TF_indices = feature_indices.reshape([len(cellmap),len(targetmap)])[:,not_similarity_indices]\n",
    "\n",
    "                TF_indices =  TF_indices[TF_indices != -1]\n",
    "                feature_indices = feature_indices[feature_indices != -1]\n",
    "\n",
    "                # sites where TF is bound in at least 2 cell line\n",
    "                positive_indices = np.where(np.sum(data[TF_indices,:], axis=0) > 1)[0]\n",
    "                indices_probs = np.ones([data.shape[1]])\n",
    "                indices_probs[positive_indices] = 0\n",
    "                indices_probs = indices_probs/np.sum(indices_probs, keepdims=1)\n",
    "\n",
    "                # If there are nans, it means there were no 0 cases.\n",
    "                # We use this for testing so models converge quickly\n",
    "                # with all ones.\n",
    "                if np.any(np.isnan(indices_probs)):\n",
    "                  print(\"Warning: no negative examples in dataset!!!\")\n",
    "                  indices_probs[:] = 1/indices_probs.shape[0]\n",
    "\n",
    "                # randomly select 10 fold sites where TF is not in any cell line\n",
    "                negative_indices = np.random.choice(np.arange(0,data.shape[1]),\n",
    "                                                    positive_indices.shape[0] * 10,\n",
    "                                                    p=indices_probs)\n",
    "                indices = np.sort(np.concatenate([negative_indices, positive_indices]))\n",
    "\n",
    "        else:\n",
    "            indices = range(0, data.shape[-1]) # not training mode, set to all points\n",
    "\n",
    "    if (mode == Dataset.RUNTIME):\n",
    "        label_cell_types = [\"PLACEHOLDER_CELL\"]\n",
    "        if similarity_matrix is None:\n",
    "            raise Exception(\"similarity_matrix must be defined in runtime mode\")\n",
    "        assert similarity_matrix.shape[0] == len(similarity_targets), \\\n",
    "            \"similarity_matrix is missing data for targets (should have %i rows)\" % (len(similarity_targets))\n",
    "        random_cell = list(cellmap)[0] # placeholder to get label vector length\n",
    "\n",
    "    print(\"using %s as labels for mode %s\" % (label_cell_types, mode))\n",
    "\n",
    "    # string of radii for meta data labeling\n",
    "    radii_str = list(map(lambda x: \"RADII_%i\" % x, radii))\n",
    "\n",
    "    def g():\n",
    "        for i in indices: # for all records specified\n",
    "\n",
    "            for (cell) in label_cell_types: # for all cell types to be used in labels\n",
    "\n",
    "                # labels for this cell\n",
    "                if (mode != Dataset.RUNTIME):\n",
    "                    label_cell_indices = EpitomeDataset.get_y_indices_for_cell(matrix, cellmap, cell)\n",
    "\n",
    "                    # delete all indices being used in the similarity computation\n",
    "                    label_cell_indices_no_similarities = np.delete(label_cell_indices, delete_indices)\n",
    "\n",
    "                    # Copy target_index_no_similarities and turn into mask of 0/1 for whether data for this cell type for\n",
    "                    # a given label is available.\n",
    "                    target_mask = np.copy(label_cell_indices_no_similarities)\n",
    "                    target_mask[target_mask > -1] = 1\n",
    "                    target_mask[target_mask == -1] = 0\n",
    "\n",
    "                else:\n",
    "                    label_count = len(EpitomeDataset.get_y_indices_for_cell(matrix, cellmap, random_cell))-len(similarity_targets)\n",
    "\n",
    "                    # Mask and labels are all 0's because labels are missing during runtime\n",
    "                    garbage_labels = target_mask = np.zeros(label_count)\n",
    "\n",
    "\n",
    "                # get indices for targets used in similarity computation\n",
    "                # for cell types that are going to be features\n",
    "                similarity_indices = feature_cell_indices[:, delete_indices]\n",
    "\n",
    "\n",
    "                # get indices for each radius in radii\n",
    "                radius_ranges = list(map(lambda x: get_radius_indices(radii, x, i, data.shape[-1]), range(len(radii))))\n",
    "\n",
    "                if len(radius_ranges) > 0:\n",
    "                    radius_indices = np.concatenate(radius_ranges)\n",
    "\n",
    "                    cell_train_data = data[similarity_indices[:,:,None],radius_indices]\n",
    "\n",
    "                    if mode == Dataset.RUNTIME:\n",
    "\n",
    "                        pos = cell_train_data*similarity_matrix[:,radius_indices]\n",
    "#                         agree = cell_train_data == similarity_matrix[:,radius_indices]\n",
    "\n",
    "                    else:\n",
    "                        cell_label_data = data[label_cell_indices[delete_indices][:,None],radius_indices]\n",
    "\n",
    "                        # remove middle dimension and flatten similarity targets\n",
    "                        pos = (cell_train_data*cell_label_data)\n",
    "#                         agree = (cell_train_data == cell_label_data)\n",
    "\n",
    "                    # get indices to split on. remove last because it is empty\n",
    "                    split_indices = np.cumsum([len(i) for i in radius_ranges])[:-1]\n",
    "                    # slice arrays by radii\n",
    "                    pos_arrays = np.split(pos, split_indices, axis= -1 )\n",
    "#                     agree_arrays = np.split(agree, split_indices, axis = -1)\n",
    "\n",
    "                    similarities = np.stack(list(map(lambda x: np.average(x, axis = -1), pos_arrays)),axis=1)\n",
    "                else:\n",
    "                    # no radius, so no similarities. just an empty placeholder\n",
    "                    similarities = np.zeros((len(eval_cell_types),0,0))\n",
    "\n",
    "                # reshape similarities to flatten 1st dimension, which are the targets\n",
    "                # results in the odering:\n",
    "                ## row 1: cell 1: pos for each target and agree for each target for each radius\n",
    "                similarities = similarities.reshape(similarities.shape[0], similarities.shape[1]*similarities.shape[2])\n",
    "\n",
    "                ##### Concatenate all cell type features together ####\n",
    "                final_features = np.concatenate([data[feature_cell_indices,i], similarities],axis=1).flatten()\n",
    "\n",
    "                # mask missing data\n",
    "                f_mask = np.concatenate([feature_cell_indices!=-1,\n",
    "                                         np.ones(similarities.shape)],axis=1).flatten()\n",
    "                final_features = final_features[f_mask != 0]\n",
    "\n",
    "                if (mode != Dataset.RUNTIME):\n",
    "                    labels = data[label_cell_indices_no_similarities,i]\n",
    "\n",
    "                else: # used when just predicting\n",
    "                    # The features going into the example.\n",
    "                    labels = garbage_labels # all 0's\n",
    "\n",
    "                # append labels and targetmask\n",
    "                final = np.array(final_features)\n",
    "\n",
    "                #### Finish appending feature labels together ####\n",
    "                if (return_feature_names):\n",
    "                    all_labels = []\n",
    "                    feature_names = []\n",
    "                    similarity_labels_agreement = ['r%i_%s' % (radius, 'agree') for radius in radii]\n",
    "#                     similarity_labels_dp = ['r%i_%s' % (radius, 'dp') for radius in radii]\n",
    "                    similarity_labels = similarity_labels_agreement\n",
    "\n",
    "                    # concatenate together feature names\n",
    "                    for j,c in enumerate(eval_cell_types):\n",
    "                        tmp = np.array(feature_targets)[feature_cell_indices[j,:] != -1]\n",
    "                        al = ['%s_%s' % (c, a) for a in tmp]\n",
    "                        sl = ['%s_%s' % (c, s) for s in similarity_labels]\n",
    "\n",
    "                        feature_names.append(al)\n",
    "                        feature_names.append(sl)\n",
    "\n",
    "                    all_labels.append(np.concatenate(feature_names))\n",
    "                    all_labels.append(['lbl_%s_%s' % (cell, a) for a in label_targets]) # of form lbl_cellline_target\n",
    "                    all_labels.append(['mask_%s_%s' % (cell, a) for a in label_targets]) # of form mask_cellline_target\n",
    "\n",
    "                    yield (final, tuple(all_labels))\n",
    "                else:\n",
    "                    yield final\n",
    "\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapperModel(EpitomeModel):\n",
    "    def score_matrix_fast(self, accessibility_peak_matrix, regions):\n",
    "        \"\"\" Runs predictions on a matrix of accessibility peaks, where columns are samples and\n",
    "        rows are regions from regions_peak_file. rows in accessilibility_peak_matrix should matching\n",
    "\n",
    "        :param numpy.matrix accessilibility_peak_matrix:  of (samples by genomic regions)\n",
    "        :param str regions: either narrowpeak or bed file containing regions to score, OR a pyranges object\n",
    "            with columns [Chomosome, Start, End, idx]. Index matches each genomic region to a row in\n",
    "            accessilibility_peak_matrix. In both cases, number of regions Should\n",
    "            match rows in accessilibility_peak_matrix\n",
    "\n",
    "        :return: 3-dimensional numpy matrix of predictions: sized (samples by regions by ChIP-seq targets)\n",
    "        :rtype: numpy matrix\n",
    "        \"\"\"\n",
    "\n",
    "        conversionObject = RegionConversion(self.dataset.regions, regions)\n",
    "\n",
    "        results = []\n",
    "        # print(accessibility_peak_matrix.shape)\n",
    "        matrix, indices = conversionObject.get_binary_vector(vector = accessibility_peak_matrix[0,:])\n",
    "        gen = load_data_runtime(data=self.dataset.get_data(Dataset.ALL),\n",
    "                 label_cell_types=self.test_celltypes,   # used for labels. Should be all for train/eval and subset for test\n",
    "                 eval_cell_types=self.eval_cell_types,   # used for rotating features. Should be all - test for train/eval\n",
    "                 matrix=self.dataset.matrix,\n",
    "                 targetmap=self.dataset.targetmap,\n",
    "                 cellmap=self.dataset.cellmap,\n",
    "                 radii = self.radii,\n",
    "                 mode = Dataset.RUNTIME,\n",
    "                 similarity_matrix = matrix,\n",
    "                 similarity_targets = ['DNase'],\n",
    "                 indices = indices,\n",
    "                 return_feature_names=False)\n",
    "\n",
    "        to_stack = load_data_no_label_mask(data=self.dataset.get_data(Dataset.ALL),\n",
    "                 label_cell_types=self.test_celltypes,   # used for labels. Should be all for train/eval and subset for test\n",
    "                 eval_cell_types=self.eval_cell_types,   # used for rotating features. Should be all - test for train/eval\n",
    "                 matrix=self.dataset.matrix,\n",
    "                 targetmap=self.dataset.targetmap,\n",
    "                 cellmap=self.dataset.cellmap,\n",
    "                 radii = self.radii,\n",
    "                 mode = Dataset.RUNTIME,\n",
    "                 similarity_matrix = matrix,\n",
    "                 similarity_targets = ['DNase'],\n",
    "                 indices = indices,\n",
    "                 return_feature_names=True)\n",
    "\n",
    "        gen_to_list = list(gen())\n",
    "        to_stack = list(to_stack())\n",
    "        gen_to_list = np.array(gen_to_list)\n",
    "\n",
    "        # reshape to n_regions [from regions] x nassays [acc dim 1] x n_samples\n",
    "        radii = self.radii\n",
    "\n",
    "        stacked = np.stack([to_stack] * accessibility_peak_matrix.shape[0], axis=0)\n",
    "        names = stacked[:, :, 1]\n",
    "        to_stack = stacked[:, :, 0]\n",
    "        to_stack = np.expand_dims(to_stack, axis=-1)\n",
    "\n",
    "        same_size = accessibility_peak_matrix.shape[1] == len(conversionObject.joined.idx_base)\n",
    "\n",
    "        if not same_size:\n",
    "            added_indices = []\n",
    "            old_idx, counter, old_i = 0, 0, 0\n",
    "            indices_to_merge = []\n",
    "            for ctr, (i, i_base) in enumerate(zip(conversionObject.joined.idx, conversionObject.joined.idx_base)):\n",
    "                if i_base == -1:\n",
    "                    continue\n",
    "                if i != old_i:\n",
    "                    indices_to_merge.append((old_idx, counter))\n",
    "                    old_idx = counter\n",
    "                added_indices.append(accessibility_peak_matrix[:, i])\n",
    "                counter += 1\n",
    "                old_i = i\n",
    "            indices_to_merge.append((old_idx, len(conversionObject.joined.idx)))\n",
    "            \n",
    "            a = np.stack(added_indices)\n",
    "        else:\n",
    "            a = np.transpose(accessibility_peak_matrix, axes=[1, 0])\n",
    "        \n",
    "        a = a[:, None, :]\n",
    "        \n",
    "        out = compute_casv(gen_to_list, a, radii)\n",
    "\n",
    "        casv_len = out.shape[1]\n",
    "        num_cells = out.shape[3]\n",
    "        num_regions = out.shape[0]\n",
    "        num_celltypes = out.shape[2]\n",
    "        num_targets = len(self.dataset.targets) if 'DNase' in self.dataset.targets else len(self.dataset.targets) + 1\n",
    "\n",
    "        for region in range(num_regions):\n",
    "            for cell in range(num_cells):\n",
    "\n",
    "                selected_gen = to_stack[cell, region, :]\n",
    "                selected_casv = out[region, :, :, cell]\n",
    "\n",
    "                len_feats_per_celltype = int(selected_gen[0].shape[0] / num_celltypes) # 24 / 2 = 12\n",
    "\n",
    "                old_sg = selected_gen\n",
    "\n",
    "                for celltype in range(num_celltypes):\n",
    "                    idx = len_feats_per_celltype * celltype\n",
    "                    casv_cell = selected_casv[:, celltype]\n",
    "                    selected_gen[0][idx + num_targets : idx + len_feats_per_celltype] = casv_cell\n",
    "        \n",
    "        results = []\n",
    "        for c in range(num_cells):\n",
    "            for r in range(num_regions):\n",
    "                results.append(self._predict(to_stack[c, r, :][0][None, :]))\n",
    "        \n",
    "        results = np.stack(results)\n",
    "        results = results.reshape((to_stack.shape[0], to_stack.shape[1], num_targets-1)) # 4 x 5\n",
    "\n",
    "        if not same_size:\n",
    "            final = []\n",
    "            final = np.empty((accessibility_peak_matrix.shape[0], accessibility_peak_matrix.shape[1], num_targets-1))\n",
    "            final.fill(np.nan)\n",
    "            for i, tup in enumerate(indices_to_merge):\n",
    "                final[:, i, :] = np.mean(results[:, tup[0]:tup[1], :], axis=1)\n",
    "            \n",
    "            # final = np.stack(final)\n",
    "            # final = final.reshape((accessibility_peak_matrix.shape[0], accessibility_peak_matrix.shape[1], 1))\n",
    "            return final\n",
    "        results = results.reshape((results.shape[0], results.shape[1], num_targets-1))\n",
    "        return results\n",
    "    \n",
    "    def score_matrix(self, accessilibility_peak_matrix, regions):\n",
    "        \"\"\" Runs predictions on a matrix of accessibility peaks, where columns are samples and\n",
    "        rows are regions from regions_peak_file. rows in accessilibility_peak_matrix should matching\n",
    "        :param numpy.matrix accessilibility_peak_matrix:  of (samples by genomic regions)\n",
    "        :param str regions: either narrowpeak or bed file containing regions to score, OR a pyranges object\n",
    "            with columns [Chomosome, Start, End, idx]. Index matches each genomic region to a row in\n",
    "            accessilibility_peak_matrix. In both cases, number of regions Should\n",
    "            match rows in accessilibility_peak_matrix\n",
    "        :return: 3-dimensional numpy matrix of predictions: sized (samples by regions by ChIP-seq targets)\n",
    "        :rtype: numpy matrix\n",
    "        \"\"\"\n",
    "\n",
    "        conversionObject = RegionConversion(self.dataset.regions, regions)\n",
    "\n",
    "        results = []\n",
    "\n",
    "        # TODO 9/10/2020: should do something more efficiently than a for loop\n",
    "        for sample_i in tqdm.tqdm(range(accessilibility_peak_matrix.shape[0])):\n",
    "\n",
    "            peaks_i, idx = conversionObject.get_binary_vector(vector = accessilibility_peak_matrix[sample_i,:])\n",
    "\n",
    "            preds = self.eval_vector(peaks_i, idx)\n",
    "\n",
    "            # group preds by joined['idx']\n",
    "            results.append(preds)\n",
    "\n",
    "        # stack all samples along 0th axis\n",
    "        # shape: samples x regions x TFs\n",
    "        tmp = np.stack(results)\n",
    "\n",
    "        # mean and merge along 1st axis\n",
    "        return conversionObject.merge(tmp, axis = 1)\n",
    "\n",
    "def compute_casv(m1, m2, radii, indices= None):\n",
    "    '''\n",
    "    Computes CASV between two matrices. CASV indiciates how similar\n",
    "    two binary matrices are to eachother. m1 and m2 should have the\n",
    "    same number of rows and columns, where rows indicate regions and\n",
    "    columns indicate the assays used to compute the casv (ie DNase-seq, H3K27ac)\n",
    "    :param np.matrix m1: 2D or 3D numpy matrix 2D shape (nregions x (nassays x ncelltypes))\n",
    "      where 2nd dimension is blocked by cells (i.e. cell1assay1, cell1assay2, cell2assay1, cell2assay2)\n",
    "      OR 3D: (nregions x nassays x ncells)\n",
    "    :param np.matrix m2: 3D numpy matrix shape (nregions x nassays x nsamples)\n",
    "    :param radii: list of radii to access surrounding region\n",
    "    :param indices: indices on 0th axis of m1 and m2 to compute casv for\n",
    "    :return numpy matrix of size (len(indices) x CASV dimension x ncelltypes x ncells)\n",
    "    '''\n",
    "\n",
    "    if indices is None:\n",
    "        indices = range(m1.shape[0])\n",
    "\n",
    "    # if only one sample, extend m2 along 2nd axis\n",
    "    if len(m2.shape) == 2:\n",
    "        m2 = m2[:,:,None]\n",
    "\n",
    "    # if needed, reshape m1 to put all assay/train cells on the last axis\n",
    "    if len(m1.shape) == 3:\n",
    "      ncells = m1.shape[-1]\n",
    "      m1 = m1.reshape(m1.shape[0],m1.shape[1]*m1.shape[2])\n",
    "    else:\n",
    "      denom = 1 if m2.shape[1]==0 else m2.shape[1]\n",
    "      ncells = int(m1.shape[-1]/denom)\n",
    "\n",
    "    if m2.shape[1] == 0:\n",
    "      # in this case, there is no CASV to compute, so we just return\n",
    "      return np.zeros((len(indices),0, ncells,m2.shape[-1]))\n",
    "\n",
    "    print(m1.shape, m2.shape)\n",
    "    assert m1.shape[0] == m2.shape[0]\n",
    "    # verify number of assays match\n",
    "    assert m2.shape[1] == m1.shape[-1]/ncells\n",
    "    # print('HERE')\n",
    "    \n",
    "#     set_trace()\n",
    "\n",
    "    def f(i):\n",
    "        \n",
    "#         set_trace()\n",
    "        # get indices for each radius in radii\n",
    "        radius_ranges = list(map(lambda x: get_radius_indices(radii, x, i, m1.shape[0]), range(len(radii))))\n",
    "\n",
    "        if len(radius_ranges) > 0:\n",
    "            radius_indices = np.concatenate(radius_ranges)\n",
    "\n",
    "            # data from known cell types (m1 portion)\n",
    "            m1_slice = m1[radius_indices, :]\n",
    "            m2_slice = np.repeat(m2[radius_indices, :, :],axis=1, repeats = ncells)\n",
    "            \n",
    "\n",
    "            # shape: radius size x (nassaysxncells) by nsamples\n",
    "            pos = (m1_slice.T*m2_slice.T).T\n",
    "#             agree = (m1_slice.T == m2_slice.T).T\n",
    "\n",
    "            # split pos and agree arrays to create new dimension for ncells\n",
    "            # the new dimension will be 4D: (radius x nassays x ncells x nsamples)\n",
    "            pos = np.stack(np.split(pos, ncells, axis=1), axis=2)\n",
    "#             agree = np.stack(np.split(agree, ncells, axis=1), axis=2)\n",
    "            \n",
    "            # get indices to split on. remove last because it is empty\n",
    "            split_indices = np.cumsum([len(i) for i in radius_ranges])[:-1]\n",
    "            # slice arrays by radii\n",
    "            pos_arrays = np.split(pos, split_indices, axis= 0 )\n",
    "#             agree_arrays = np.split(agree, split_indices, axis = 0)\n",
    "\n",
    "            # average over the radius (0th axis)\n",
    "            tmp1 = list(map(lambda x: np.average(x, axis = 0), pos_arrays)) # this line is problematic\n",
    "            # final concatenation combines agree, nassays, and radii on the 0th axis\n",
    "            # this axis is ordered by (1) pos/agree, then (2) radii, then (2) n assays.\n",
    "            # See ordering example when there are 2 radii (r1, r2):\n",
    "            # - pos: r1, nassays | pos: r2, nassays | agree: r1: nassays | agree: r1: nassays\n",
    "            tmp = np.concatenate(tmp1, axis=0)\n",
    "            return tmp\n",
    "        else:\n",
    "            # no radius, so no similarities. just an empty placeholder\n",
    "            # shaped with the number of cells (last dim of m1)\n",
    "            return np.zeros((0,ncells,m2.shape[-1]))\n",
    "\n",
    "    # for every region of interest\n",
    "    # TODO: maybe something more efficient?\n",
    "\n",
    "    # set_trace()\n",
    "    tmp = []\n",
    "    for i in indices:\n",
    "        tmp.append(f(i))\n",
    "    \n",
    "    return np.stack(tmp)\n",
    "#     return np.stack([f(i) for i in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using ['HepG2', 'HeLa-S3', 'H1', 'A549'] as labels for mode Dataset.TRAIN\n",
      "using ['HepG2', 'HeLa-S3', 'H1', 'A549'] as labels for mode Dataset.VALID\n",
      "using ['K562'] as labels for mode Dataset.TEST\n"
     ]
    }
   ],
   "source": [
    "eligible_cells = ['K562','HepG2','H1','A549','HeLa-S3']\n",
    "eligible_targets = ['DNase','CTCF', 'RAD21']\n",
    "\n",
    "dataset = EpitomeDataset(targets = eligible_targets,\n",
    "    cells = eligible_cells)\n",
    "\n",
    "\n",
    "model = WrapperModel(dataset,\n",
    "    test_celltypes = ['K562'], radii=[1])\n",
    "\n",
    "# model.radii = [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_peak_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "\n",
    "# Create dummy data\n",
    "regions_dict = {'Chromosome': ['chr1'] * 7,\n",
    "                    'Start': [50, 10000, 30000, 10300, 10600, 11000, 11300],\n",
    "                    'End': [100, 10300, 31200, 10500, 10900, 11200, 11600]}\n",
    "\n",
    "regions_pr = pr.from_dict(regions_dict)\n",
    "\n",
    "# Write to tmp bed file\n",
    "regions_pr.to_bed(regions_peak_file.name)\n",
    "regions_peak_file.flush()\n",
    "\n",
    "accessilibility_peak_matrix = np.random.uniform(low=0., high=1., size=(4, 7))\n",
    "# accessilibility_peak_matrix = np.zeros((21,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8587562,)\n",
      "+--------------+-----------+-----------+-----------+--------------+------------+------------+\n",
      "| Chromosome   |     Start |       End |       idx |   Start_base |   End_base |   idx_base |\n",
      "| (category)   |   (int64) |   (int64) |   (int64) |      (int64) |    (int64) |    (int64) |\n",
      "|--------------+-----------+-----------+-----------+--------------+------------+------------|\n",
      "| chr1         |     10000 |     10300 |         1 |        10000 |      10200 |          0 |\n",
      "| chr1         |     10000 |     10300 |         1 |        10200 |      10400 |          1 |\n",
      "| chr1         |     10300 |     10500 |         3 |        10200 |      10400 |          1 |\n",
      "| chr1         |     10300 |     10500 |         3 |        10400 |      10600 |          2 |\n",
      "| chr1         |     30000 |     31200 |         2 |        30600 |      30800 |          6 |\n",
      "| chr1         |     30000 |     31200 |         2 |        30800 |      31000 |          7 |\n",
      "| chr1         |     30000 |     31200 |         2 |        31000 |      31200 |          8 |\n",
      "+--------------+-----------+-----------+-----------+--------------+------------+------------+\n",
      "Unstranded PyRanges object has 7 rows and 7 columns from 1 chromosomes.\n",
      "For printing, the PyRanges was sorted on Chromosome.\n",
      "bv [0. 0. 0. 0. 0. 0. 0.]\n",
      "v [0.22324328 0.22324328 0.84826792 0.84826792 0.66358895 0.66358895\n",
      " 0.66358895]\n",
      "using ['PLACEHOLDER_CELL'] as labels for mode Dataset.RUNTIME\n",
      "using ['PLACEHOLDER_CELL'] as labels for mode Dataset.RUNTIME\n",
      "(7, 4) (7, 1, 4)\n",
      "TIME 0.42188549041748047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eecs/rvkoodli/miniconda3/envs/EpitomeEnv3/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/eecs/rvkoodli/miniconda3/envs/EpitomeEnv3/lib/python3.6/site-packages/numpy/core/_methods.py:154: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret, rcount, out=ret, casting='unsafe', subok=False)\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "res = model.score_matrix_fast(accessilibility_peak_matrix, regions_peak_file.name)\n",
    "print('TIME', time.time() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 7, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.40it/s]\u001b[A\n",
      " 25%|██▌       | 1/4 [00:00<00:00,  6.90it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 28.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8587562,)\n",
      "+--------------+-----------+-----------+-----------+--------------+------------+------------+\n",
      "| Chromosome   |     Start |       End |       idx |   Start_base |   End_base |   idx_base |\n",
      "| (category)   |   (int64) |   (int64) |   (int64) |      (int64) |    (int64) |    (int64) |\n",
      "|--------------+-----------+-----------+-----------+--------------+------------+------------|\n",
      "| chr1         |     10000 |     10300 |         1 |        10000 |      10200 |          0 |\n",
      "| chr1         |     10000 |     10300 |         1 |        10200 |      10400 |          1 |\n",
      "| chr1         |     10300 |     10500 |         3 |        10200 |      10400 |          1 |\n",
      "| chr1         |     10300 |     10500 |         3 |        10400 |      10600 |          2 |\n",
      "| chr1         |     30000 |     31200 |         2 |        30600 |      30800 |          6 |\n",
      "| chr1         |     30000 |     31200 |         2 |        30800 |      31000 |          7 |\n",
      "| chr1         |     30000 |     31200 |         2 |        31000 |      31200 |          8 |\n",
      "+--------------+-----------+-----------+-----------+--------------+------------+------------+\n",
      "Unstranded PyRanges object has 7 rows and 7 columns from 1 chromosomes.\n",
      "For printing, the PyRanges was sorted on Chromosome.\n",
      "bv [0. 0. 0. 0. 0. 0. 0.]\n",
      "v [0.22324328 0.22324328 0.84826792 0.84826792 0.66358895 0.66358895\n",
      " 0.66358895]\n",
      "using ['PLACEHOLDER_CELL'] as labels for mode Dataset.RUNTIME\n",
      "(8587562,)\n",
      "+--------------+-----------+-----------+-----------+--------------+------------+------------+\n",
      "| Chromosome   |     Start |       End |       idx |   Start_base |   End_base |   idx_base |\n",
      "| (category)   |   (int64) |   (int64) |   (int64) |      (int64) |    (int64) |    (int64) |\n",
      "|--------------+-----------+-----------+-----------+--------------+------------+------------|\n",
      "| chr1         |     10000 |     10300 |         1 |        10000 |      10200 |          0 |\n",
      "| chr1         |     10000 |     10300 |         1 |        10200 |      10400 |          1 |\n",
      "| chr1         |     10300 |     10500 |         3 |        10200 |      10400 |          1 |\n",
      "| chr1         |     10300 |     10500 |         3 |        10400 |      10600 |          2 |\n",
      "| chr1         |     30000 |     31200 |         2 |        30600 |      30800 |          6 |\n",
      "| chr1         |     30000 |     31200 |         2 |        30800 |      31000 |          7 |\n",
      "| chr1         |     30000 |     31200 |         2 |        31000 |      31200 |          8 |\n",
      "+--------------+-----------+-----------+-----------+--------------+------------+------------+\n",
      "Unstranded PyRanges object has 7 rows and 7 columns from 1 chromosomes.\n",
      "For printing, the PyRanges was sorted on Chromosome.\n",
      "bv [0. 0. 0. 0. 0. 0. 0.]\n",
      "v [0.51733131 0.51733131 0.39290772 0.39290772 0.95165607 0.95165607\n",
      " 0.95165607]\n",
      "using ['PLACEHOLDER_CELL'] as labels for mode Dataset.RUNTIME\n",
      "(8587562,)\n",
      "+--------------+-----------+-----------+-----------+--------------+------------+------------+\n",
      "| Chromosome   |     Start |       End |       idx |   Start_base |   End_base |   idx_base |\n",
      "| (category)   |   (int64) |   (int64) |   (int64) |      (int64) |    (int64) |    (int64) |\n",
      "|--------------+-----------+-----------+-----------+--------------+------------+------------|\n",
      "| chr1         |     10000 |     10300 |         1 |        10000 |      10200 |          0 |\n",
      "| chr1         |     10000 |     10300 |         1 |        10200 |      10400 |          1 |\n",
      "| chr1         |     10300 |     10500 |         3 |        10200 |      10400 |          1 |\n",
      "| chr1         |     10300 |     10500 |         3 |        10400 |      10600 |          2 |\n",
      "| chr1         |     30000 |     31200 |         2 |        30600 |      30800 |          6 |\n",
      "| chr1         |     30000 |     31200 |         2 |        30800 |      31000 |          7 |\n",
      "| chr1         |     30000 |     31200 |         2 |        31000 |      31200 |          8 |\n",
      "+--------------+-----------+-----------+-----------+--------------+------------+------------+\n",
      "Unstranded PyRanges object has 7 rows and 7 columns from 1 chromosomes.\n",
      "For printing, the PyRanges was sorted on Chromosome."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 31.94it/s]\n",
      " 75%|███████▌  | 3/4 [00:00<00:00, 12.03it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 22.99it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 11.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bv [0. 0. 0. 0. 0. 0. 0.]\n",
      "v [0.68918137 0.68918137 0.08731115 0.08731115 0.54871061 0.54871061\n",
      " 0.54871061]\n",
      "using ['PLACEHOLDER_CELL'] as labels for mode Dataset.RUNTIME\n",
      "(8587562,)\n",
      "+--------------+-----------+-----------+-----------+--------------+------------+------------+\n",
      "| Chromosome   |     Start |       End |       idx |   Start_base |   End_base |   idx_base |\n",
      "| (category)   |   (int64) |   (int64) |   (int64) |      (int64) |    (int64) |    (int64) |\n",
      "|--------------+-----------+-----------+-----------+--------------+------------+------------|\n",
      "| chr1         |     10000 |     10300 |         1 |        10000 |      10200 |          0 |\n",
      "| chr1         |     10000 |     10300 |         1 |        10200 |      10400 |          1 |\n",
      "| chr1         |     10300 |     10500 |         3 |        10200 |      10400 |          1 |\n",
      "| chr1         |     10300 |     10500 |         3 |        10400 |      10600 |          2 |\n",
      "| chr1         |     30000 |     31200 |         2 |        30600 |      30800 |          6 |\n",
      "| chr1         |     30000 |     31200 |         2 |        30800 |      31000 |          7 |\n",
      "| chr1         |     30000 |     31200 |         2 |        31000 |      31200 |          8 |\n",
      "+--------------+-----------+-----------+-----------+--------------+------------+------------+\n",
      "Unstranded PyRanges object has 7 rows and 7 columns from 1 chromosomes.\n",
      "For printing, the PyRanges was sorted on Chromosome.\n",
      "bv [0. 0. 0. 0. 0. 0. 0.]\n",
      "v [0.86555236 0.86555236 0.04498745 0.04498745 0.75169553 0.75169553\n",
      " 0.75169553]\n",
      "using ['PLACEHOLDER_CELL'] as labels for mode Dataset.RUNTIME\n",
      "TIME 0.5296375751495361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "res2 = model.score_matrix(accessilibility_peak_matrix, regions_peak_file.name)\n",
    "print('TIME', time.time() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[       nan,        nan],\n",
       "        [0.50433779, 0.5046258 ],\n",
       "        [0.48336053, 0.51213257],\n",
       "        [0.5       , 0.5       ],\n",
       "        [       nan,        nan],\n",
       "        [       nan,        nan],\n",
       "        [       nan,        nan]],\n",
       "\n",
       "       [[       nan,        nan],\n",
       "        [0.50202215, 0.5021553 ],\n",
       "        [0.47961418, 0.50922008],\n",
       "        [0.5       , 0.5       ],\n",
       "        [       nan,        nan],\n",
       "        [       nan,        nan],\n",
       "        [       nan,        nan]],\n",
       "\n",
       "       [[       nan,        nan],\n",
       "        [0.50045013, 0.5004797 ],\n",
       "        [0.47719312, 0.5072062 ],\n",
       "        [0.5       , 0.5       ],\n",
       "        [       nan,        nan],\n",
       "        [       nan,        nan],\n",
       "        [       nan,        nan]],\n",
       "\n",
       "       [[       nan,        nan],\n",
       "        [0.50023192, 0.50024718],\n",
       "        [0.47686474, 0.50692415],\n",
       "        [0.5       , 0.5       ],\n",
       "        [       nan,        nan],\n",
       "        [       nan,        nan],\n",
       "        [       nan,        nan]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int8)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dataset.get_data(Dataset.ALL)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "+--------------+-----------+-----------+-----------+\n",
       "| Chromosome   | Start     | End       | idx       |\n",
       "| (category)   | (int64)   | (int64)   | (int64)   |\n",
       "|--------------+-----------+-----------+-----------|\n",
       "| chr1         | 10000     | 10200     | 0         |\n",
       "| chr1         | 10200     | 10400     | 1         |\n",
       "| chr1         | 10400     | 10600     | 2         |\n",
       "| chr1         | 17400     | 17600     | 3         |\n",
       "| ...          | ...       | ...       | ...       |\n",
       "| chr22        | 51223800  | 51224000  | 8460585   |\n",
       "| chr22        | 51225400  | 51225600  | 8460586   |\n",
       "| chr22        | 51234400  | 51234600  | 8460587   |\n",
       "| chr22        | 51234600  | 51234800  | 8460588   |\n",
       "+--------------+-----------+-----------+-----------+\n",
       "Unstranded PyRanges object has 8,587,562 rows and 4 columns from 22 chromosomes.\n",
       "For printing, the PyRanges was sorted on Chromosome."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dataset.regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
