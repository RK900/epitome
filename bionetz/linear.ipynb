{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "# interactive notebook session\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chunk_iterator(array, chunk_size=1):\n",
    "    # iterates and chunks along first dimension\n",
    "    for i in range(array.shape[0] // chunk_size):\n",
    "        yield array[chunk_size * i: (i + 1) * chunk_size]\n",
    "        \n",
    "\n",
    "def batch_iterator(source, batch_size=32, num_epochs=1):\n",
    "    # shuffle batch iterator for hdf5 data\n",
    "    tmp = h5py.File(source, 'r')\n",
    "    seq, atac = tmp['seq'], tmp['atac']\n",
    "    label = tmp['label']\n",
    "    num_examples = label.shape[0]\n",
    "    for epoch in range(num_epochs):\n",
    "        permutation = np.random.permutation(num_examples)\n",
    "        for indices in chunk_iterator(permutation, batch_size):\n",
    "            # h5py datasets require sorted point-wise indexing\n",
    "            indices = sorted(indices)\n",
    "            yield seq[indices], atac[indices], label[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training hyper-parameters\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "LOGGING_FREQUENCY = 100\n",
    "\n",
    "# import data as a shuffle-batch iterator\n",
    "CEBPB_A549 = batch_iterator(\n",
    "    source='../../deleteme/results-hdf5/CEBPB-A549.hdf5',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic-regression sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define symbolic placeholders\n",
    "sy_seq_n = tf.placeholder(dtype=tf.float32, shape=[None, 1000, 5])\n",
    "sy_atac_n = tf.placeholder(dtype=tf.float32, shape=[None, 1000])\n",
    "sy_label_n = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "\n",
    "# concatenate one-hot encoded seq with atac counts\n",
    "sy_input_n = tf.concat([sy_seq_n, tf.expand_dims(sy_atac_n, axis=-1)], axis=-1)\n",
    "\n",
    "# flatten input tensor data\n",
    "sy_input_n = tf.contrib.layers.flatten(sy_input_n)\n",
    "# logistic regression to sanity-check data\n",
    "sy_logit_n = tf.layers.dense(sy_input_n, units=1, activation=None)\n",
    "\n",
    "# optimizer configuration\n",
    "sy_loss = tf.losses.sigmoid_cross_entropy(\n",
    "    multi_class_labels=sy_label_n,\n",
    "    logits=sy_logit_n)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE)\n",
    "train_op = optimizer.minimize(sy_loss)\n",
    "\n",
    "# define logging metrics\n",
    "sy_accuracy = tf.reduce_mean(tf.cast(\n",
    "    tf.equal(x=sy_label_n, y=sy_logit_n),\n",
    "    dtype=tf.float32))\n",
    "\n",
    "# initialize variables\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# begin the training loop\n",
    "for iteration, (seq, atac, label) in enumerate(CEBPB_A549):\n",
    "    # perform a single gradient step for the batch\n",
    "    loss, accuracy, _ = sess.run(\n",
    "        fetches=[sy_loss, sy_accuracy, train_op],\n",
    "        feed_dict={\n",
    "            sy_seq_n: seq,\n",
    "            sy_atac_n: atac,\n",
    "            sy_label_n: label,\n",
    "        })\n",
    "    # log the cross-entropy loss and accuracy\n",
    "    if iteration % LOGGING_FREQUENCY:\n",
    "        print('iteration %i: loss=%.4f, accuracy=%.4f' % (iteration, loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
