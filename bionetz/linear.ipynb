{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dna_encoder(seq, bases='ACTG'):\n",
    "    # one-hot-encoding for sequence data\n",
    "    # enumerates base in a sequence\n",
    "    indices = map(\n",
    "        lambda x: bases.index(x) if x in bases else -1,\n",
    "        seq)\n",
    "    # one extra index for unknown\n",
    "    eye = np.eye(len(bases) + 1)\n",
    "    return eye[indices].astype(np.float32)\n",
    "\n",
    "def tf_dna_encoder(seq, bases='ACTG'):\n",
    "    # wraps `dna_encoder` with a `py_func`\n",
    "    return tf.py_func(dna_encoder, [seq, bases], [tf.float32])[0]\n",
    "\n",
    "def dataset_input_fn(filenames,\n",
    "                     buffer_size=10000,\n",
    "                     batch_size=32,\n",
    "                     num_epochs=20,\n",
    "                     ):\n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    "    \n",
    "    # Use `tf.parse_single_example()` to extract data from a `tf.Example`\n",
    "    # protocol buffer, and perform any additional per-record preprocessing.\n",
    "    def parser(record):\n",
    "        keys_to_features = {\n",
    "            \"sequence\": tf.FixedLenFeature((), tf.string),\n",
    "            \"atacCounts\": tf.FixedLenFeature((1000,), tf.int64),\n",
    "            \"Labels\": tf.FixedLenFeature((1,), tf.int64),\n",
    "        }\n",
    "        parsed = tf.parse_single_example(record, keys_to_features)\n",
    "\n",
    "        # Perform additional preprocessing on the parsed data.\n",
    "        seq = tf_dna_encoder(parsed[\"sequence\"])\n",
    "        seq = tf.reshape(seq, [1000, 5])\n",
    "        atac = parsed[\"atacCounts\"]\n",
    "        label = parsed[\"Labels\"]\n",
    "\n",
    "        # add more here if needed\n",
    "        return {'seq': seq, 'atac': atac}, label\n",
    "    \n",
    "    # Use `Dataset.map()` to build a pair of a feature dictionary and a label\n",
    "    # tensor for each example.\n",
    "    dataset = dataset.map(parser)\n",
    "    dataset = dataset.shuffle(buffer_size)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    # `features` is a dictionary in which each value is a batch of values for\n",
    "    # that feature; `labels` is a batch of labels.\n",
    "    features, labels = iterator.get_next()\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training hyper-parameters\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "# for shuffled batches\n",
    "BUFFER_SIZE = 10000\n",
    "# weighting positive examples\n",
    "POS_WEIGHT = 1\n",
    "\n",
    "# logging parameters\n",
    "CONSOLE_LOG_STEPS = 200\n",
    "SUMMARY_SAVE_SECS = 20\n",
    "CHECKPOINT_SAVE_SECS = 200\n",
    "CHECKPOINT_DIR = './deleteme/checkpoint_dir/'\n",
    "OUTPUT_DIR = './deleteme/output_dir/'\n",
    "\n",
    "# percent of data to use for training\n",
    "TRAIN_PROPORTION = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset default graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Creates a variable to hold the global_step.\n",
    "global_step_tensor = tf.Variable(0, trainable=False, name='global_step')\n",
    "\n",
    "# import data as a shuffle-batch iterator\n",
    "# https://www.tensorflow.org/programmers_guide/datasets\n",
    "filenames = glob.glob('../../deleteme/CEBPB-A549-hg38.txt/part-r-*')\n",
    "num_train_files = int(len(filenames) * TRAIN_PROPORTION)\n",
    "train_filenames = filenames[:num_train_files]\n",
    "valid_filenames = filenames[num_train_files:]\n",
    "\n",
    "# training data-flow tensors\n",
    "train_features, train_labels = dataset_input_fn(\n",
    "    filenames=train_filenames,\n",
    "    buffer_size=BUFFER_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=NUM_EPOCHS)\n",
    "\n",
    "# validation data-flow tensors\n",
    "valid_features, valid_labels = dataset_input_fn(\n",
    "    filenames=valid_filenames,\n",
    "    buffer_size=BUFFER_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multi_layer_perceptron(x, units, activations, name='mlp',\n",
    "                           reuse=None):\n",
    "    # `x` is a tensor with shape [batch, features]\n",
    "    # `units` is a list of ints, corresponding to hidden units\n",
    "    # `activations` is a list of activation functions\n",
    "    with tf.variable_scope(name, reuse=reuse):\n",
    "        assert len(units) == len(activations)\n",
    "        for u, a in zip(units, activations):\n",
    "            x = tf.layers.dense(x, units=u, activation=a)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def preprocess_inputs(features, labels):\n",
    "    # define symbolic inputs (see previous cell)\n",
    "    sy_seq_n = tf.cast(features['seq'], tf.float32)\n",
    "    sy_atac_n = tf.cast(features['atac'], tf.float32)\n",
    "    sy_label_n = tf.cast(labels, tf.float32)\n",
    "    \n",
    "    # concatenate one-hot encoded seq with atac counts\n",
    "    sy_input_n = tf.concat(\n",
    "        [sy_seq_n, tf.expand_dims(sy_atac_n, axis=-1)],\n",
    "        axis=-1)\n",
    "    return sy_input_n, sy_label_n\n",
    "    \n",
    "    \n",
    "def simple_model(features, labels, name='model', reuse=None,\n",
    "                 is_train=False):\n",
    "    # wrapper function for scoping\n",
    "    # variable sharing across train/valid\n",
    "    \n",
    "    # preprocess inputs to neural-network\n",
    "    sy_input_n, sy_label_n = preprocess_inputs(features,labels)\n",
    "    \n",
    "    # pass inputs through multi-layer perceptron\n",
    "    # in this case, just a logistic-regression\n",
    "    sy_logit_n = multi_layer_perceptron(\n",
    "        tf.contrib.layers.flatten(sy_input_n),\n",
    "        units=[256, 128, 1],\n",
    "        activations=[tf.nn.relu, tf.nn.relu, None],\n",
    "        name=name,\n",
    "        reuse=reuse)\n",
    "    \n",
    "    # pass logits through sigmoid to get predictions\n",
    "    sy_pred_n = tf.nn.sigmoid(sy_logit_n)\n",
    "    \n",
    "    # computing weighted cross-entropy loss\n",
    "    sy_loss = tf.reduce_mean(\n",
    "        tf.nn.weighted_cross_entropy_with_logits(\n",
    "            logits=sy_logit_n,\n",
    "            targets=sy_label_n,\n",
    "            pos_weight=POS_WEIGHT))\n",
    "    \n",
    "    # computing area under ROC\n",
    "    sy_auc, auc_op = tf.metrics.auc(\n",
    "        labels=sy_label_n,\n",
    "        predictions=sy_pred_n)\n",
    "    \n",
    "    # dictionary of results\n",
    "    results = {}\n",
    "    results['pred'] = sy_pred_n\n",
    "    results['loss'] = sy_loss\n",
    "    results['auc'] = (sy_auc, auc_op)\n",
    "    \n",
    "    if is_train:\n",
    "        # optimizer configuration\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\n",
    "        train_op = optimizer.minimize(sy_loss, global_step=global_step_tensor)\n",
    "        results['train_op'] = train_op\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic-regression sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass through simple model\n",
    "sy_train_dict = simple_model(train_features, train_labels, name='model', is_train=True)\n",
    "sy_valid_dict = simple_model(valid_features, valid_labels, name='model', reuse=True)\n",
    "\n",
    "# tensorboard summaries\n",
    "tf.summary.scalar('loss/train', sy_train_dict['loss'])\n",
    "tf.summary.scalar('loss/valid', sy_valid_dict['loss'])\n",
    "tf.summary.scalar('auc/train', sy_train_dict['auc'][0])\n",
    "tf.summary.scalar('auc/valid', sy_valid_dict['auc'][0])\n",
    "\n",
    "summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maybe delete existing checkpoint file if\n",
    "# the graph structure/variables have changed\n",
    "!rm -r ./deleteme/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saver hook periodically checkpoints model\n",
    "# session creator can restore from these\n",
    "saver_hook = tf.train.CheckpointSaverHook(checkpoint_dir=CHECKPOINT_DIR,\n",
    "                                          save_secs=CHECKPOINT_SAVE_SECS)\n",
    "\n",
    "# summary hook allows you to use tensorboard\n",
    "# specify the metrics you want to log above\n",
    "summary_hook = tf.train.SummarySaverHook(summary_writer=tf.summary.FileWriter(OUTPUT_DIR),\n",
    "                                         save_secs=SUMMARY_SAVE_SECS, summary_op=summary_op)\n",
    "\n",
    "# begin the training loop\n",
    "with tf.train.MonitoredSession(session_creator=tf.train.ChiefSessionCreator(\n",
    "    checkpoint_dir=CHECKPOINT_DIR), hooks=[summary_hook, saver_hook]) as sess:\n",
    "    \n",
    "    while not sess.should_stop():\n",
    "        # step forward everything\n",
    "        _, _, summary = sess.run([sy_train_dict, sy_valid_dict, summary_op])\n",
    "        global_step = tf.train.global_step(sess, global_step_tensor)\n",
    "        \n",
    "        # maybe log to console\n",
    "        if CONSOLE_LOG_STEPS and global_step % CONSOLE_LOG_STEPS == 0:\n",
    "            # parse the protocol-buffer string\n",
    "            summary_proto = tf.summary.Summary()\n",
    "            summary_proto.ParseFromString(summary)\n",
    "            print('Iteration %s' % global_step)\n",
    "            for value in summary_proto.value:\n",
    "                print(value.tag + ': ' + str(value.simple_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
