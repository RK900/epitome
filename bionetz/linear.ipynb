{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dna_encoder(seq, bases='ACTG'):\n",
    "    # one-hot-encoding for sequence data\n",
    "    # enumerates base in a sequence\n",
    "    indices = map(\n",
    "        lambda x: bases.index(x) if x in bases else -1,\n",
    "        seq)\n",
    "    # one extra index for unknown\n",
    "    eye = np.eye(len(bases) + 1)\n",
    "    return eye[indices].astype(np.float32)\n",
    "\n",
    "\n",
    "def np_sigmoid(x):\n",
    "    # logistic sigmoid function\n",
    "    return 1 / (1 + np.e**-x)\n",
    "\n",
    "\n",
    "def tf_dna_encoder(seq, bases='ACTG'):\n",
    "    # wraps `dna_encoder` with a `py_func`\n",
    "    return tf.py_func(dna_encoder, [seq, bases], [tf.float32])[0]\n",
    "\n",
    "\n",
    "def dataset_input_fn(filenames,\n",
    "                     buffer_size=10000,\n",
    "                     batch_size=32,\n",
    "                     num_epochs=20,\n",
    "                     ):\n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    "    \n",
    "    # Use `tf.parse_single_example()` to extract data from a `tf.Example`\n",
    "    # protocol buffer, and perform any additional per-record preprocessing.\n",
    "    def parser(record):\n",
    "        keys_to_features = {\n",
    "            \"sequence\": tf.FixedLenFeature((), tf.string),\n",
    "            \"atacCounts\": tf.FixedLenFeature((1000,), tf.int64),\n",
    "            \"Labels\": tf.FixedLenFeature((1,), tf.int64),\n",
    "        }\n",
    "        parsed = tf.parse_single_example(record, keys_to_features)\n",
    "\n",
    "        # Perform additional preprocessing on the parsed data.\n",
    "        seq = tf_dna_encoder(parsed[\"sequence\"])\n",
    "        seq = tf.reshape(seq, [1000, 5])\n",
    "\n",
    "        # add more here if needed\n",
    "        return {'seq': seq, 'atac': parsed[\"atacCounts\"]}, parsed[\"Labels\"]\n",
    "    \n",
    "    # Use `Dataset.map()` to build a pair of a feature dictionary and a label\n",
    "    # tensor for each example.\n",
    "    dataset = dataset.map(parser)\n",
    "    dataset = dataset.shuffle(buffer_size)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    # `features` is a dictionary in which each value is a batch of values for\n",
    "    # that feature; `labels` is a batch of labels.\n",
    "    features, labels = iterator.get_next()\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training hyper-parameters\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "LOGGING_FREQUENCY = 100\n",
    "\n",
    "# reset default graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# import data as a shuffle-batch iterator\n",
    "# https://www.tensorflow.org/programmers_guide/datasets\n",
    "filenames = glob.glob('../../deleteme/CEBPB-A549-hg38.txt/part-r-*')\n",
    "features, labels = dataset_input_fn(filenames,\n",
    "    buffer_size=BUFFER_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logistic-regression sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define symbolic inputs (see previous cell)\n",
    "sy_seq_n = tf.cast(features['seq'], tf.float32)\n",
    "sy_atac_n = tf.cast(features['atac'], tf.float32)\n",
    "sy_label_n = tf.cast(labels, tf.float32)\n",
    "\n",
    "# concatenate one-hot encoded seq with atac counts\n",
    "sy_input_n = tf.concat(\n",
    "    [sy_seq_n, tf.expand_dims(sy_atac_n, axis=-1)],\n",
    "    axis=-1)\n",
    "\n",
    "# only use sequence data\n",
    "# sy_input_n = sy_seq_n\n",
    "\n",
    "# only use atac data\n",
    "# sy_input_n = sy_atac_n\n",
    "\n",
    "# flatten input tensor data\n",
    "sy_input_n = tf.contrib.layers.flatten(sy_input_n)\n",
    "\n",
    "# entering neural-network\n",
    "sy_net_n = sy_input_n\n",
    "\n",
    "# multi-layer perceptron\n",
    "# for units in [256, 128, 64]:\n",
    "#     sy_net_n = tf.layers.dense(sy_net_n, units=units)\n",
    "#     sy_net_n = tf.nn.relu(sy_net_n)\n",
    "\n",
    "# exit neural-network to logits\n",
    "sy_logit_n = tf.layers.dense(sy_net_n, units=1, activation=None)\n",
    "sy_prediction_n = tf.nn.sigmoid(sy_logit_n)\n",
    "\n",
    "# positively weighted loss\n",
    "# pos weight is 1 by default\n",
    "POS_WEIGHT = 1\n",
    "\n",
    "# optimizer configuration\n",
    "sy_loss = tf.reduce_mean(\n",
    "    tf.nn.weighted_cross_entropy_with_logits(\n",
    "        logits=sy_logit_n,\n",
    "        targets=sy_label_n,\n",
    "        pos_weight=POS_WEIGHT))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE)\n",
    "train_op = optimizer.minimize(sy_loss)\n",
    "\n",
    "# define logging metrics\n",
    "sy_auc, auc_op = tf.metrics.auc(\n",
    "    labels=sy_label_n,\n",
    "    predictions=sy_prediction_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss=0.6899, auc=0.0000\n",
      "iteration 100: loss=0.6568, auc=0.5085\n",
      "iteration 200: loss=0.6501, auc=0.5137\n",
      "iteration 300: loss=0.5393, auc=0.5125\n",
      "iteration 400: loss=0.6896, auc=0.5159\n",
      "iteration 500: loss=0.5945, auc=0.5198\n",
      "iteration 600: loss=0.7205, auc=0.5193\n",
      "iteration 700: loss=0.7294, auc=0.5206\n",
      "iteration 800: loss=0.7368, auc=0.5222\n",
      "iteration 900: loss=0.4711, auc=0.5226\n",
      "iteration 1000: loss=0.5740, auc=0.5225\n",
      "iteration 1100: loss=0.6925, auc=0.5221\n",
      "iteration 1200: loss=0.6163, auc=0.5228\n",
      "iteration 1300: loss=0.6791, auc=0.5236\n",
      "iteration 1400: loss=0.6539, auc=0.5274\n",
      "iteration 1500: loss=0.6932, auc=0.5271\n",
      "iteration 1600: loss=0.5450, auc=0.5275\n",
      "iteration 1700: loss=0.6548, auc=0.5284\n",
      "iteration 1800: loss=0.4839, auc=0.5280\n",
      "iteration 1900: loss=0.6187, auc=0.5282\n",
      "iteration 2000: loss=0.5966, auc=0.5296\n",
      "iteration 2100: loss=0.5987, auc=0.5305\n",
      "iteration 2200: loss=0.6647, auc=0.5308\n",
      "iteration 2300: loss=0.5106, auc=0.5321\n",
      "iteration 2400: loss=0.7596, auc=0.5328\n",
      "iteration 2500: loss=0.7570, auc=0.5331\n",
      "iteration 2600: loss=0.5850, auc=0.5332\n",
      "iteration 2700: loss=0.6873, auc=0.5340\n",
      "iteration 2800: loss=0.7070, auc=0.5348\n",
      "iteration 2900: loss=0.5844, auc=0.5355\n",
      "iteration 3000: loss=0.5142, auc=0.5361\n",
      "iteration 3100: loss=0.5445, auc=0.5367\n",
      "iteration 3200: loss=0.6325, auc=0.5376\n",
      "iteration 3300: loss=0.6280, auc=0.5380\n",
      "iteration 3400: loss=0.6715, auc=0.5385\n",
      "iteration 3500: loss=0.5707, auc=0.5385\n",
      "iteration 3600: loss=0.4413, auc=0.5388\n",
      "iteration 3700: loss=0.5562, auc=0.5398\n",
      "iteration 3800: loss=0.5716, auc=0.5405\n",
      "iteration 3900: loss=0.6669, auc=0.5410\n",
      "iteration 4000: loss=0.6241, auc=0.5415\n",
      "iteration 4100: loss=0.5168, auc=0.5425\n",
      "iteration 4200: loss=0.5962, auc=0.5431\n",
      "iteration 4300: loss=0.4911, auc=0.5435\n",
      "iteration 4400: loss=0.7877, auc=0.5438\n",
      "iteration 4500: loss=0.6861, auc=0.5441\n",
      "iteration 4600: loss=0.5475, auc=0.5442\n",
      "iteration 4700: loss=0.6531, auc=0.5448\n",
      "iteration 4800: loss=0.5283, auc=0.5449\n",
      "iteration 4900: loss=0.5898, auc=0.5453\n",
      "iteration 5000: loss=0.6807, auc=0.5458\n",
      "iteration 5100: loss=0.5642, auc=0.5463\n",
      "iteration 5200: loss=0.6486, auc=0.5467\n",
      "iteration 5300: loss=0.6916, auc=0.5474\n",
      "iteration 5400: loss=0.6617, auc=0.5478\n",
      "iteration 5500: loss=0.7698, auc=0.5484\n",
      "iteration 5600: loss=0.5116, auc=0.5488\n",
      "iteration 5700: loss=0.6030, auc=0.5492\n",
      "iteration 5800: loss=0.7117, auc=0.5495\n",
      "iteration 5900: loss=0.6129, auc=0.5499\n",
      "iteration 6000: loss=0.6026, auc=0.5499\n",
      "iteration 6100: loss=0.5838, auc=0.5502\n",
      "iteration 6200: loss=0.5518, auc=0.5506\n",
      "iteration 6300: loss=0.6046, auc=0.5513\n",
      "iteration 6400: loss=0.5150, auc=0.5514\n",
      "iteration 6500: loss=0.6559, auc=0.5519\n",
      "iteration 6600: loss=0.5998, auc=0.5522\n",
      "iteration 6700: loss=0.6567, auc=0.5526\n",
      "iteration 6800: loss=0.6960, auc=0.5529\n",
      "iteration 6900: loss=0.6942, auc=0.5532\n",
      "iteration 7000: loss=0.4837, auc=0.5536\n",
      "iteration 7100: loss=0.7179, auc=0.5540\n",
      "iteration 7200: loss=0.8243, auc=0.5544\n",
      "iteration 7300: loss=0.6494, auc=0.5549\n",
      "iteration 7400: loss=0.5404, auc=0.5549\n",
      "iteration 7500: loss=0.6801, auc=0.5551\n",
      "iteration 7600: loss=0.6229, auc=0.5554\n",
      "iteration 7700: loss=0.7196, auc=0.5559\n",
      "iteration 7800: loss=0.6514, auc=0.5562\n",
      "iteration 7900: loss=0.7528, auc=0.5565\n",
      "iteration 8000: loss=0.5746, auc=0.5568\n",
      "iteration 8100: loss=0.5557, auc=0.5570\n",
      "iteration 8200: loss=0.6551, auc=0.5572\n",
      "iteration 8300: loss=0.8228, auc=0.5576\n",
      "iteration 8400: loss=0.6558, auc=0.5580\n",
      "iteration 8500: loss=0.5408, auc=0.5584\n",
      "iteration 8600: loss=0.4362, auc=0.5586\n",
      "iteration 8700: loss=0.5406, auc=0.5590\n",
      "iteration 8800: loss=0.5459, auc=0.5594\n",
      "iteration 8900: loss=0.6496, auc=0.5596\n",
      "iteration 9000: loss=0.7015, auc=0.5600\n",
      "iteration 9100: loss=0.5986, auc=0.5603\n",
      "iteration 9200: loss=0.5600, auc=0.5608\n",
      "iteration 9300: loss=0.5341, auc=0.5611\n",
      "iteration 9400: loss=0.5805, auc=0.5614\n",
      "iteration 9500: loss=0.6130, auc=0.5618\n",
      "iteration 9600: loss=0.5693, auc=0.5619\n",
      "iteration 9700: loss=0.6890, auc=0.5623\n",
      "iteration 9800: loss=0.6118, auc=0.5626\n",
      "iteration 9900: loss=0.5778, auc=0.5631\n",
      "iteration 10000: loss=0.6136, auc=0.5634\n",
      "iteration 10100: loss=0.5683, auc=0.5638\n",
      "iteration 10200: loss=0.5444, auc=0.5639\n",
      "iteration 10300: loss=0.5349, auc=0.5642\n",
      "iteration 10400: loss=0.5161, auc=0.5644\n",
      "iteration 10500: loss=0.6368, auc=0.5647\n",
      "iteration 10600: loss=0.5926, auc=0.5649\n",
      "iteration 10700: loss=0.4961, auc=0.5650\n",
      "iteration 10800: loss=0.6318, auc=0.5653\n",
      "iteration 10900: loss=0.6928, auc=0.5656\n",
      "iteration 11000: loss=0.5797, auc=0.5659\n",
      "iteration 11100: loss=0.6370, auc=0.5663\n",
      "iteration 11200: loss=0.6376, auc=0.5665\n",
      "iteration 11300: loss=0.5549, auc=0.5668\n"
     ]
    }
   ],
   "source": [
    "# begin the training loop\n",
    "with tf.train.MonitoredTrainingSession() as sess:\n",
    "    iteration = 0\n",
    "    while not sess.should_stop():\n",
    "        update_ops = [train_op, auc_op]\n",
    "        fetches = [sy_loss, sy_auc, update_ops]\n",
    "        loss, auc, _ = sess.run(fetches)\n",
    "        \n",
    "        # log the cross-entropy loss, AUC, precision, recall\n",
    "        if iteration % LOGGING_FREQUENCY == 0:\n",
    "            print('iteration %i: loss=%.4f, auc=%.4f' % (iteration, loss, auc))\n",
    "        iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
