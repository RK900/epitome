{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"finished tensorflow\")\n",
    "import numpy as np\n",
    "print(\"finished numpy\")\n",
    "import os, glob\n",
    "import h5py\n",
    "print(\"finished h5py\")\n",
    "import logz\n",
    "from scipy.io import loadmat\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, roc_curve, roc_auc_score\n",
    "print(\"finished scipy\")\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"finished matplotlib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lrelu(x, alpha=0.2):\n",
    "    return tf.maximum(alpha*x, x)\n",
    "\n",
    "def fc(x, n_units, dropout, activation=None):\n",
    "    net = tf.layers.dense(x, n_units)\n",
    "    net = tf.contrib.layers.layer_norm(net)\n",
    "    if activation:\n",
    "        net = activation(net)\n",
    "    return tf.layers.dropout(net, dropout)\n",
    "\n",
    "def conv1d(x, hidden_size, kernel_size, stride=1, dilation=1,\n",
    "           pooling_size=0, dropout=0.0, activation=None):\n",
    "    net = tf.layers.conv1d(x, hidden_size, kernel_size, stride, padding='same',\n",
    "                           dilation_rate=dilation, activation=activation)\n",
    "    if pooling_size:\n",
    "        net = tf.layers.max_pooling1d(net, pooling_size, pooling_size, padding=\"same\")\n",
    "    return tf.layers.dropout(net, dropout)\n",
    "\n",
    "def cnn(input_, n_classes, hp):\n",
    "    net = input_\n",
    "    for i in xrange(hp.n_conv_layers):\n",
    "        net = conv1d(net, hp.hidden_sizes[i], hp.kernel_size, hp.stride, dilation=1,\n",
    "                     pooling_size=hp.pooling_sizes[i], dropout=hp.dropout_keep_probs[i],\n",
    "                     activation=hp.activation)\n",
    "    for i in xrange(hp.n_dconv_layers):\n",
    "        dilation= 2**(i + 1)\n",
    "        tmp = conv1d(net, hp.dconv_h_size, hp.kernel_size, hp.stride, dilation=1,\n",
    "                     pooling_size=0, dropout=hp.dropout, activation=hp.activation)\n",
    "        net = tf.concat([net, tmp], axis=2)\n",
    "    net = tf.contrib.layers.flatten(net)\n",
    "    net = fc(net, hp.fc_h_size, hp.dropout, activation=hp.activation)\n",
    "    return fc(net, n_classes, hp.dropout, activation=hp.output_activation)\n",
    "\n",
    "def rnn(input_, n_classes, hp): # RNN\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(hp.hidden_size, forget_bias=1.0)\n",
    "    \n",
    "    values = tf.unstack(input_, axis=1)\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(cell, values, dtype=tf.float32)\n",
    "    return fc(outputs[-1], n_classes, hp.dropout, activation=hp.output_activation)\n",
    "\n",
    "def srnn(input_, n_classes, hp): # Stacked RNN\n",
    "    cells = [tf.contrib.rnn.BasicLSTMCell(hp.hidden_size, forget_bias=1.0)\n",
    "             for _ in xrange(hp.n_layers)]\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "    \n",
    "    values = tf.unstack(input_, axis=1)\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(cell, values, dtype=tf.float32)\n",
    "    return fc(outputs[-1], n_classes, hp.dropout, activation=hp.output_activation)\n",
    "\n",
    "def birnn(input_, n_classes, hp): # Bidirectional RNN\n",
    "    fw = tf.contrib.rnn.BasicLSTMCell(hp.hidden_size, forget_bias=1.0)\n",
    "    bw = tf.contrib.rnn.BasicLSTMCell(hp.hidden_size, forget_bias=1.0)\n",
    "    \n",
    "    values = tf.unstack(input_, axis=1)\n",
    "    outputs, fw_states, bw_states = tf.contrib.rnn.static_bidirectional_rnn(\n",
    "        fw, bw, values, dtype=tf.float32)\n",
    "    return fc(outputs[-1], n_classes, hp.dropout, activation=hp.output_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_hp(**kwargs):\n",
    "    hp = tf.contrib.training.HParams()\n",
    "    hp.n_conv_layers = 4\n",
    "    hp.n_dconv_layers = 4\n",
    "    hp.hidden_sizes = [64, 64, 64, 64]\n",
    "    hp.dconv_h_size = 64\n",
    "    hp.fc_h_size = 925\n",
    "    hp.kernel_size = 8\n",
    "    hp.pooling_sizes = [2, 2, 2, 4]\n",
    "    hp.stride = 1\n",
    "    hp.dropout_keep_probs = [0.9, 0.9, 0.9, 0.9]\n",
    "    hp.dropout = 1\n",
    "    hp.activation = lrelu\n",
    "    hp.output_activation = tf.sigmoid\n",
    "    hp.__dict__.update(kwargs)\n",
    "    return hp\n",
    "\n",
    "def rnn_hp(**kwargs):\n",
    "    hp = tf.contrib.training.HParams()\n",
    "    hp.hidden_size = 64\n",
    "    hp.n_layers = 2 # Stacked RNN\n",
    "    hp.dropout = 0.9\n",
    "    hp.output_activation = tf.sigmoid\n",
    "    hp.__dict__.update(kwargs)\n",
    "    return hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = h5py.File('../genomics/deepsea_train/train.mat', 'r')\n",
    "train_input = tmp['trainxdata']\n",
    "train_target = tmp['traindata']\n",
    "\n",
    "tmp = loadmat('../genomics/deepsea_train/valid.mat')\n",
    "valid_input = tmp['validxdata']\n",
    "valid_target = tmp['validdata']\n",
    "\n",
    "def test_and_valid_batches(batch_size, input_, target):\n",
    "    while True:\n",
    "        for i in xrange(int(input_.shape[0]/batch_size)):\n",
    "            yield input_[i*batch_size:(i+1)*batch_size,:,0:1000].transpose([0,2,1]), target[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "def train_batches(batch_size, input_, target):\n",
    "    while True:\n",
    "        num_samples = input_.shape[2]\n",
    "        num_batches = num_samples / batch_size\n",
    "        batch_order = np.random.permutation(num_batches)\n",
    "        \n",
    "        for i in batch_order:\n",
    "            yield (input_[0:1000,:,i*batch_size:(i+1)*batch_size].transpose([2, 0, 1]),\n",
    "                   target[:,i*batch_size:(i+1)*batch_size].transpose([1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "print(\"reset\")\n",
    "input_placeholder = tf.placeholder(dtype=tf.float32, shape=[None, 1000, 4])\n",
    "target_placeholder = tf.placeholder(dtype=tf.float32, shape=[None, 919])\n",
    "print(1)\n",
    "\n",
    "# CNN\n",
    "# hp = cnn_hp(n_dconv_layers=0)\n",
    "# logits = cnn(input_placeholder, 1, hp)\n",
    "# DeepSea\n",
    "hp = cnn_hp(n_dconv_layers=0,\n",
    "            hidden_sizes=[320,480,960,64])\n",
    "logits = cnn(input_placeholder, 919, hp)\n",
    "print(2)\n",
    "\n",
    "\n",
    "\n",
    "# Dilated CNN\n",
    "# hp = cnn_hp()\n",
    "# logits = cnn(input_placeholder, 919, hp)\n",
    "\n",
    "# RNN\n",
    "# hp = rnn_hp()\n",
    "# logits = rnn(input_placeholder, 919, hp)\n",
    "\n",
    "# Stacked RNN\n",
    "# hp = rnn_hp()\n",
    "# logits = srnn(input_placeholder, 919, hp)\n",
    "\n",
    "# Bidirectional RNN\n",
    "# hp = rnn_hp()\n",
    "# logits = birnn(input_placeholder, 919, hp)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(\n",
    "    logits=logits,targets=target_placeholder, pos_weight=50))\n",
    "optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "print(\"built model\")\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "print(\"Graph compiled.\")\n",
    "\n",
    "experiment = 'cnn_wide_50'\n",
    "logdir = os.path.join(\"/data/jwhughes/logs/\", experiment)\n",
    "save_path = os.path.join(logdir, \"model.ckpt\")\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "logz.configure_output_dir(logdir)\n",
    "log_freq = 1000\n",
    "save_freq = 20000\n",
    "\n",
    "iterations = int(3e6)\n",
    "batch_size = 32\n",
    "\n",
    "valid_size = len(valid_input)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if glob.glob(save_path + '*'):\n",
    "        saver.restore(sess, save_path)\n",
    "        print(\"Model restored.\")\n",
    "    else:\n",
    "        sess.run(init_op)\n",
    "        print(\"Model initialized.\")\n",
    "    \n",
    "    n_parameters = np.sum([np.prod(v.shape) for v in tf.trainable_variables()])\n",
    "    print(\"Number of parameters: %i\" % n_parameters)\n",
    "\n",
    "    batches = train_batches(batch_size, train_input, train_target)\n",
    "    \n",
    "    for i in xrange(iterations):\n",
    "        break\n",
    "        input_, target = batches.next()\n",
    "        _loss, _ = sess.run([loss, optimizer], feed_dict={\n",
    "            input_placeholder: input_,\n",
    "            target_placeholder: target,\n",
    "        })\n",
    "\n",
    "        if i % save_freq == 0:\n",
    "            save_path = saver.save(sess, save_path)\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "        if i % log_freq == 0:\n",
    "            \n",
    "            loss_list = []\n",
    "            valid_batches = test_and_valid_batches(batch_size, valid_input, valid_target)\n",
    "            for j in range(valid_size / batch_size):\n",
    "                b, t = valid_batches.next()\n",
    "                _valid_loss = sess.run(loss, feed_dict={\n",
    "                    input_placeholder: b,\n",
    "                    target_placeholder: t})\n",
    "                loss_list += [_valid_loss]\n",
    "\n",
    "            logz.log_tabular('Iteration', i)\n",
    "            logz.log_tabular('Loss', _loss)\n",
    "            logz.log_tabular('Valid Loss', np.mean(loss_list))\n",
    "            logz.log_tabular('Average AUPRC', -1)\n",
    "            logz.log_tabular('80th percentile AUPRC', -1)\n",
    "            logz.dump_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if glob.glob(save_path + '*'):\n",
    "        saver.restore(sess, save_path)\n",
    "        print(\"Model restored.\")\n",
    "    else:\n",
    "        raise\n",
    "        \n",
    "    batches = test_and_valid_batches(32, valid_input, valid_target)\n",
    "    all_logits = np.array([[0] * 919])\n",
    "    all_targets = np.array([[0] * 919])\n",
    "    valid_batches = test_and_valid_batches(batch_size, valid_input, valid_target)\n",
    "    for j in range(valid_size / batch_size):\n",
    "        b, t = valid_batches.next()\n",
    "        _logits = sess.run(logits, feed_dict={\n",
    "            input_placeholder: b\n",
    "        })\n",
    "#         all_logits += _logits[:,0].tolist()\n",
    "        all_logits = np.append(all_logits, _logits, axis = 0)\n",
    "#         all_targets += t[:,0].tolist()\n",
    "        all_targets = np.append(all_targets, t, axis = 0)\n",
    "\n",
    "all_logits = all_logits[1:]\n",
    "all_targets = all_targets[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(223)\n",
    "\n",
    "our_scores = []\n",
    "for i in np.arange(919):\n",
    "    fpr, tpr, _ = roc_curve(all_targets[:, i],all_logits[:, i])\n",
    "    try:\n",
    "        our_scores += [roc_auc_score(all_targets[:, i],all_logits[:, i])]\n",
    "    except:\n",
    "        continue\n",
    "    plt.plot(fpr, tpr, alpha=0.01, color = \"blue\")\n",
    "\n",
    "\n",
    "    plt.xlabel('fpr')\n",
    "    plt.ylabel('tpr')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('Our CNN')\n",
    "\n",
    "print(np.mean(our_scores))\n",
    "    \n",
    "all_targets = loadmat(\"../genomics/deepsea_train/valid.mat\")[\"validdata\"]\n",
    "print(\"done loading\")\n",
    "plt.subplot(221)\n",
    "probs = []\n",
    "for s in [\"0\", \"1\", \"2\", \"3\"]:\n",
    "    with open(\"../genomics/fastas/infile.fasta\" + s + \".out\", \"r\") as f:\n",
    "        for l in f:\n",
    "            break\n",
    "        for l in f:\n",
    "            probs_for_l = [float(i) for i in l.split(\",\")[2:]]\n",
    "            probs += [probs_for_l]\n",
    "            \n",
    "probs = np.stack(probs)\n",
    "            \n",
    "scores = []\n",
    "for i in np.arange(919):\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    fpr, tpr, _ = roc_curve(all_targets[:,i],probs[:,i])\n",
    "    try:\n",
    "        scores += [roc_auc_score(all_targets[:,i],probs[:,i])]\n",
    "    except:\n",
    "        continue\n",
    "    plt.plot(fpr, tpr, alpha=0.01, color = \"blue\")\n",
    "\n",
    "    plt.ylabel('tpr')\n",
    "    plt.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom='off',      # ticks along the bottom edge are off\n",
    "    top='off',         # ticks along the top edge are off\n",
    "    labelbottom='off') # labels along the bottom edge are off\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('DeepSea')\n",
    "    \n",
    "\n",
    "\n",
    "plt.show()\n",
    "print(np.mean(scores))\n",
    "\n",
    "print(np.count_nonzero(np.array(our_scores) > np.array(scores)))\n",
    "plt.hist(our_scores)\n",
    "plt.hist(scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.array(our_scores) - np.array(scores), bins = np.arange(-1,1.01,.01))\n",
    "plt.title(\"Deepsea AUC - Our CNN AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(our_scores, alpha = .5, bins = np.arange(0,1.01,.01))\n",
    "plt.hist(scores, alpha = .5, bins = np.arange(0,1.01,.01))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = h5py.File(\"../genomics/results-hdf5/CEBPB-A549.hdf5\", \"r\")\n",
    "a5_labels = tmp['label']\n",
    "a5_seq = tmp['seq']\n",
    "tmp = h5py.File(\"../genomics/results-hdf5/CEBPB-H1Hesc.hdf5\", \"r\")\n",
    "h1_labels = tmp['label']\n",
    "h1_seq = tmp['seq']\n",
    "tmp = h5py.File(\"../genomics/results-hdf5/CEBPB-HCT116.hdf5\", \"r\")\n",
    "hc_labels = tmp['label']\n",
    "hc_seq = tmp['seq']\n",
    "tmp = h5py.File(\"../genomics/results-hdf5/CEBPB-IMR90.hdf5\", \"r\")\n",
    "im_labels = tmp['label']\n",
    "im_seq = tmp['seq']\n",
    "\n",
    "def cell_batches(batch_size, input_, target):\n",
    "    for i in xrange(int(input_.shape[0]/batch_size)):\n",
    "        yield input_[i*batch_size:(i+1)*batch_size,0:1000,0:4], target[i*batch_size:(i+1)*batch_size]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicies = []\n",
    "titles = []\n",
    "\n",
    "with open(\"../genomics/DeepSEA-v0.94/resources/feature_name\") as f:\n",
    "    for i, row in enumerate(f):\n",
    "        if \"CEBPB\" in row.strip(\"\\n\").split(\"\\t\")[1]:\n",
    "            indicies += [int(row.strip(\"\\n\").split(\"\\t\")[0])]\n",
    "            titles += [row.strip(\"\\n\").split(\"\\t\")[1]]\n",
    "indicies, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    if glob.glob(save_path + '*'):\n",
    "        saver.restore(sess, save_path)\n",
    "        print(\"Model restored.\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "    for idx in [4]:\n",
    "        input_ = h1_seq\n",
    "        targs = h1_labels\n",
    "\n",
    "        batches = cell_batches(32, input_, targs)\n",
    "        all_logits_cell = []\n",
    "        all_targets_cell = []\n",
    "        i = 0\n",
    "        for b, t in batches:\n",
    "            _logits = sess.run(logits, feed_dict={\n",
    "                input_placeholder: b\n",
    "            })\n",
    "            all_logits_cell += _logits[:,idx].tolist()\n",
    "            all_targets_cell += t.flatten().tolist()\n",
    "            i += 1\n",
    "            if i == 100:\n",
    "                break\n",
    "        all_logits_cell = np.array(all_logits_cell)\n",
    "        all_targets_cell = np.array(all_targets_cell)\n",
    "#         fpr, tpr, _ = roc_curve(all_targets_cell,all_logits_cell)\n",
    "\n",
    "#         plt.plot(fpr, tpr, color = \"blue\")\n",
    "#         print(roc_auc_score(all_targets_cell,all_logits_cell))\n",
    "#         print(all_logits_cell[0:10])\n",
    "#         plt.show()\n",
    "        plt.hist(all_logits_cell[all_targets_cell == 0], alpha = .5, bins = np.arange(0,1.01,.001), density = True, label = \"Our data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(all_logits_cell[all_targets_cell == 0], alpha = .5, bins = np.arange(0,1.01,.001), density = True, label = \"Our data\")\n",
    "plt.hist(all_logits[:,idx][all_targets[:,idx] == 0],alpha = .5, bins = np.arange(0,1.01,.001), density = True, label = \"Their data\")\n",
    "plt.title(\"Negative Examples\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
