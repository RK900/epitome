{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished tensorflow\n",
      "finished numpy\n",
      "finished h5py\n",
      "finished scipy\n",
      "finished matplotlib\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"finished tensorflow\")\n",
    "import numpy as np\n",
    "print(\"finished numpy\")\n",
    "import os, glob\n",
    "import h5py\n",
    "print(\"finished h5py\")\n",
    "import logz\n",
    "from scipy.io import loadmat\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, roc_curve, roc_auc_score\n",
    "print(\"finished scipy\")\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"finished matplotlib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lrelu(x, alpha=0.2):\n",
    "    return tf.maximum(alpha*x, x)\n",
    "\n",
    "def fc(x, n_units, dropout, activation=None):\n",
    "    net = tf.layers.dense(x, n_units)\n",
    "    net = tf.contrib.layers.layer_norm(net)\n",
    "    if activation:\n",
    "        net = activation(net)\n",
    "    return tf.layers.dropout(net, dropout)\n",
    "\n",
    "def conv1d(x, hidden_size, kernel_size, stride=1, dilation=1,\n",
    "           pooling_size=0, dropout=0.0, activation=None):\n",
    "    net = tf.layers.conv1d(x, hidden_size, kernel_size, stride, padding='same',\n",
    "                           dilation_rate=dilation, activation=activation)\n",
    "    if pooling_size:\n",
    "        net = tf.layers.max_pooling1d(net, pooling_size, pooling_size, padding=\"same\")\n",
    "    return tf.layers.dropout(net, dropout)\n",
    "\n",
    "def cnn(input_, n_classes, hp):\n",
    "    net = input_\n",
    "    for i in xrange(hp.n_conv_layers):\n",
    "        net = conv1d(net, hp.hidden_sizes[i], hp.kernel_size, hp.stride, dilation=1,\n",
    "                     pooling_size=hp.pooling_sizes[i], dropout=hp.dropout_keep_probs[i],\n",
    "                     activation=hp.activation)\n",
    "    for i in xrange(hp.n_dconv_layers):\n",
    "        dilation= 2**(i + 1)\n",
    "        tmp = conv1d(net, hp.dconv_h_size, hp.kernel_size, hp.stride, dilation=1,\n",
    "                     pooling_size=0, dropout=hp.dropout, activation=hp.activation)\n",
    "        net = tf.concat([net, tmp], axis=2)\n",
    "    net = tf.contrib.layers.flatten(net)\n",
    "    net = fc(net, hp.fc_h_size, hp.dropout, activation=hp.activation)\n",
    "    return fc(net, n_classes, hp.dropout, activation=hp.output_activation)\n",
    "\n",
    "def rnn(input_, n_classes, hp): # RNN\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(hp.hidden_size, forget_bias=1.0)\n",
    "    \n",
    "    values = tf.unstack(input_, axis=1)\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(cell, values, dtype=tf.float32)\n",
    "    return fc(outputs[-1], n_classes, hp.dropout, activation=hp.output_activation)\n",
    "\n",
    "def srnn(input_, n_classes, hp): # Stacked RNN\n",
    "    cells = [tf.contrib.rnn.BasicLSTMCell(hp.hidden_size, forget_bias=1.0)\n",
    "             for _ in xrange(hp.n_layers)]\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "    \n",
    "    values = tf.unstack(input_, axis=1)\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(cell, values, dtype=tf.float32)\n",
    "    return fc(outputs[-1], n_classes, hp.dropout, activation=hp.output_activation)\n",
    "\n",
    "def birnn(input_, n_classes, hp): # Bidirectional RNN\n",
    "    fw = tf.contrib.rnn.BasicLSTMCell(hp.hidden_size, forget_bias=1.0)\n",
    "    bw = tf.contrib.rnn.BasicLSTMCell(hp.hidden_size, forget_bias=1.0)\n",
    "    \n",
    "    values = tf.unstack(input_, axis=1)\n",
    "    outputs, fw_states, bw_states = tf.contrib.rnn.static_bidirectional_rnn(\n",
    "        fw, bw, values, dtype=tf.float32)\n",
    "    return fc(outputs[-1], n_classes, hp.dropout, activation=hp.output_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_hp(**kwargs):\n",
    "    hp = tf.contrib.training.HParams()\n",
    "    hp.n_conv_layers = 4\n",
    "    hp.n_dconv_layers = 4\n",
    "    hp.hidden_sizes = [64, 64, 64, 64]\n",
    "    hp.dconv_h_size = 64\n",
    "    hp.fc_h_size = 925\n",
    "    hp.kernel_size = 8\n",
    "    hp.pooling_sizes = [2, 2, 2, 4]\n",
    "    hp.stride = 1\n",
    "    hp.dropout_keep_probs = [0.9, 0.9, 0.9, 0.9]\n",
    "    hp.dropout = 1\n",
    "    hp.activation = lrelu\n",
    "    hp.output_activation = tf.sigmoid\n",
    "    hp.__dict__.update(kwargs)\n",
    "    return hp\n",
    "\n",
    "def rnn_hp(**kwargs):\n",
    "    hp = tf.contrib.training.HParams()\n",
    "    hp.hidden_size = 64\n",
    "    hp.n_layers = 2 # Stacked RNN\n",
    "    hp.dropout = 0.9\n",
    "    hp.output_activation = tf.sigmoid\n",
    "    hp.__dict__.update(kwargs)\n",
    "    return hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tmp = h5py.File('../../genomics/deepsea_train/train.mat', 'r')\n",
    "# train_input = tmp['trainxdata']\n",
    "# train_target = tmp['traindata']\n",
    "\n",
    "# tmp = loadmat('../../genomics/deepsea_train/valid.mat')\n",
    "# valid_input = tmp['validxdata']\n",
    "# valid_target = tmp['validdata']\n",
    "\n",
    "# def test_and_valid_batches(batch_size, input_, target):\n",
    "#     while True:\n",
    "#         for i in xrange(int(input_.shape[0]/batch_size)):\n",
    "#             yield input_[i*batch_size:(i+1)*batch_size,:,0:1000].transpose([0,2,1]), target[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "# def train_batches(batch_size, input_, target):\n",
    "#     while True:\n",
    "#         num_samples = input_.shape[2]\n",
    "#         num_batches = num_samples / batch_size\n",
    "#         batch_order = np.random.permutation(num_batches)\n",
    "        \n",
    "#         for i in batch_order:\n",
    "#             yield (input_[0:1000,:,i*batch_size:(i+1)*batch_size].transpose([2, 0, 1]),\n",
    "#                    target[:,i*batch_size:(i+1)*batch_size].transpose([1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dna_encoder(seq, bases='ACTG'):\n",
    "    # one-hot-encoding for sequence data\n",
    "    # enumerates base in a sequence\n",
    "    indices = map(\n",
    "        lambda x: bases.index(x) if x in bases else -1,\n",
    "        seq)\n",
    "    # one extra index for unknown\n",
    "    eye = np.eye(len(bases) + 1)\n",
    "    return eye[indices].astype(np.float32)\n",
    "\n",
    "def np_sigmoid(x):\n",
    "    # logistic sigmoid function\n",
    "    return 1 / (1 + np.e**-x)\n",
    "\n",
    "def tf_dna_encoder(seq, bases='ACTG'):\n",
    "    # wraps `dna_encoder` with a `py_func`\n",
    "    return tf.py_func(dna_encoder, [seq, bases], [tf.float32])[0]\n",
    "\n",
    "\n",
    "def dataset_input_fn(filenames,\n",
    "                     buffer_size=10000,\n",
    "                     batch_size=32,\n",
    "                     num_epochs=20,\n",
    "                     ):\n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    "    \n",
    "    # Use `tf.parse_single_example()` to extract data from a `tf.Example`\n",
    "    # protocol buffer, and perform any additional per-record preprocessing.\n",
    "    def parser(record):\n",
    "        keys_to_features = {\n",
    "            \"sequence\": tf.FixedLenFeature((), tf.string),\n",
    "            \"atacCounts\": tf.FixedLenFeature((1000,), tf.int64),\n",
    "            \"Labels\": tf.FixedLenFeature((1,), tf.int64),\n",
    "        }\n",
    "        parsed = tf.parse_single_example(record, keys_to_features)\n",
    "\n",
    "        # Perform additional preprocessing on the parsed data.\n",
    "        seq = tf_dna_encoder(parsed[\"sequence\"])\n",
    "        seq = tf.reshape(seq, [1000, 5])\n",
    "\n",
    "        # add more here if needed\n",
    "        return {'seq': seq, 'atac': parsed[\"atacCounts\"]}, parsed[\"Labels\"]\n",
    "    \n",
    "    # Use `Dataset.map()` to build a pair of a feature dictionary and a label\n",
    "    # tensor for each example.\n",
    "    dataset = dataset.map(parser)\n",
    "    dataset = dataset.shuffle(buffer_size)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    # `features` is a dictionary in which each value is a batch of values for\n",
    "    # that feature; `labels` is a batch of labels.\n",
    "    features, labels = iterator.get_next()\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~\t       deepsea_data.py\tlogz.py        README.md\r\n",
      "datagen.ipynb  iio.py\t\tlogz.pyc       train.ipynb\r\n",
      "datagen.py     linear.ipynb\tpooling.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training hyper-parameters\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "LOGGING_FREQUENCY = 100\n",
    "\n",
    "# reset default graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# import data as a shuffle-batch iterator\n",
    "# https://www.tensorflow.org/programmers_guide/datasets\n",
    "filenames = glob.glob('~/CEBPB-A549-hg38.txt/part-r-*')\n",
    "features, labels = dataset_input_fn(filenames[:-2],\n",
    "    buffer_size=BUFFER_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "print(\"reset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"IteratorGetNext:1\", shape=(?, 1000, 5), dtype=float32)\n",
      "Tensor(\"Cast_22:0\", shape=(?, 1000), dtype=float32)\n",
      "1\n",
      "1.5\n",
      "2\n",
      "built model\n",
      "Graph compiled.\n",
      "Log dir ~/data/cnn_new_data already exists! Delete it first or use a different dir\n",
      "\u001b[32;1mLogging data to ~/data/cnn_new_data/log.txt\u001b[0m\n",
      "Model initialized.\n",
      "going\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# define symbolic inputs (see previous cell)\n",
    "sy_seq_n = tf.cast(features['seq'], tf.float32)\n",
    "sy_atac_n = tf.cast(features['atac'], tf.float32)\n",
    "sy_label_n = tf.cast(labels, tf.float32)\n",
    "\n",
    "print(sy_seq_n)\n",
    "print(sy_atac_n)\n",
    "\n",
    "# sy_atac_n = tf.reshape(sy_atac_n, shape=[tf.shape(sy_atac_n)[0], 1000, 1])\n",
    "# input_placeholder = tf.concat([sy_seq_n, sy_atac_n], 2)\n",
    "input_placeholder = sy_seq_n\n",
    "target_placeholder = sy_label_n\n",
    "print(1)\n",
    "\n",
    "# CNN\n",
    "hp = cnn_hp(n_dconv_layers=0)\n",
    "print(1.5)\n",
    "logits = cnn(input_placeholder, 1, hp)\n",
    "# DeepSea\n",
    "# hp = cnn_hp(n_dconv_layers=0,\n",
    "#             hidden_sizes=[320,480,960,64])\n",
    "# logits = cnn(input_placeholder, 919, hp)\n",
    "print(2)\n",
    "\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(\n",
    "    logits=logits,targets=target_placeholder, pos_weight=50))\n",
    "optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "print(\"built model\")\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "print(\"Graph compiled.\")\n",
    "\n",
    "experiment = 'cnn_new_data'\n",
    "logdir = os.path.join(\"~/data\", experiment)\n",
    "save_path = os.path.join(logdir, \"model.ckpt\")\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "logz.configure_output_dir(logdir)\n",
    "log_freq = 1000\n",
    "save_freq = 20000\n",
    "\n",
    "iterations = int(3e6)\n",
    "batch_size = 32\n",
    "\n",
    "# begin the training loop\n",
    "with tf.train.MonitoredTrainingSession() as sess:\n",
    "    if glob.glob(save_path + '*'):\n",
    "        saver.restore(sess, save_path)\n",
    "        print(\"Model restored.\")\n",
    "    else:\n",
    "        sess.run(init_op)\n",
    "        print(\"Model initialized.\")\n",
    "        \n",
    "    iteration = 0\n",
    "    while not sess.should_stop():\n",
    "        print(\"going\")\n",
    "        print(sess.should_stop())\n",
    "        fetches = [logits, sy_label_n, loss, optimizer]\n",
    "        logits_, labels_, loss_, _ = sess.run(fetches)\n",
    "\n",
    "        print(\"MADE IT\")\n",
    "\n",
    "        # log the cross-entropy loss and accuracy\n",
    "        if iteration % save_freq == 0:\n",
    "            print(\"saving\")\n",
    "            save_path = saver.save(sess, save_path)\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "        if iteration % log_freq == 0:\n",
    "            print('iteration %i: loss=%.4f' % (iteration, loss_))\n",
    "        iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "print(\"reset\")\n",
    "input_placeholder = tf.placeholder(dtype=tf.float32, shape=[None, 1000, 4])\n",
    "target_placeholder = tf.placeholder(dtype=tf.float32, shape=[None, 919])\n",
    "print(1)\n",
    "\n",
    "# CNN\n",
    "# hp = cnn_hp(n_dconv_layers=0)\n",
    "# logits = cnn(input_placeholder, 1, hp)\n",
    "# DeepSea\n",
    "hp = cnn_hp(n_dconv_layers=0,\n",
    "            hidden_sizes=[320,480,960,64])\n",
    "logits = cnn(input_placeholder, 919, hp)\n",
    "print(2)\n",
    "\n",
    "\n",
    "\n",
    "# Dilated CNN\n",
    "# hp = cnn_hp()\n",
    "# logits = cnn(input_placeholder, 919, hp)\n",
    "\n",
    "# RNN\n",
    "# hp = rnn_hp()\n",
    "# logits = rnn(input_placeholder, 919, hp)\n",
    "\n",
    "# Stacked RNN\n",
    "# hp = rnn_hp()\n",
    "# logits = srnn(input_placeholder, 919, hp)\n",
    "\n",
    "# Bidirectional RNN\n",
    "# hp = rnn_hp()\n",
    "# logits = birnn(input_placeholder, 919, hp)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(\n",
    "    logits=logits,targets=target_placeholder, pos_weight=50))\n",
    "optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "print(\"built model\")\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "print(\"Graph compiled.\")\n",
    "\n",
    "experiment = 'cnn_wide_50'\n",
    "logdir = os.path.join(\"/data/jwhughes/logs/\", experiment)\n",
    "save_path = os.path.join(logdir, \"model.ckpt\")\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "logz.configure_output_dir(logdir)\n",
    "log_freq = 1000\n",
    "save_freq = 20000\n",
    "\n",
    "iterations = int(3e6)\n",
    "batch_size = 32\n",
    "\n",
    "valid_size = len(valid_input)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if glob.glob(save_path + '*'):\n",
    "        saver.restore(sess, save_path)\n",
    "        print(\"Model restored.\")\n",
    "    else:\n",
    "        sess.run(init_op)\n",
    "        print(\"Model initialized.\")\n",
    "    \n",
    "    n_parameters = np.sum([np.prod(v.shape) for v in tf.trainable_variables()])\n",
    "    print(\"Number of parameters: %i\" % n_parameters)\n",
    "\n",
    "    batches = train_batches(batch_size, train_input, train_target)\n",
    "    \n",
    "    for i in xrange(iterations):\n",
    "        break\n",
    "        input_, target = batches.next()\n",
    "        _loss, _ = sess.run([loss, optimizer], feed_dict={\n",
    "            input_placeholder: input_,\n",
    "            target_placeholder: target,\n",
    "        })\n",
    "\n",
    "        if i % save_freq == 0:\n",
    "            save_path = saver.save(sess, save_path)\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "        if i % log_freq == 0:\n",
    "            \n",
    "            loss_list = []\n",
    "            valid_batches = test_and_valid_batches(batch_size, valid_input, valid_target)\n",
    "            for j in range(valid_size / batch_size):\n",
    "                b, t = valid_batches.next()\n",
    "                _valid_loss = sess.run(loss, feed_dict={\n",
    "                    input_placeholder: b,\n",
    "                    target_placeholder: t})\n",
    "                loss_list += [_valid_loss]\n",
    "\n",
    "            logz.log_tabular('Iteration', i)\n",
    "            logz.log_tabular('Loss', _loss)\n",
    "            logz.log_tabular('Valid Loss', np.mean(loss_list))\n",
    "            logz.log_tabular('Average AUPRC', -1)\n",
    "            logz.log_tabular('80th percentile AUPRC', -1)\n",
    "            logz.dump_tabular()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if glob.glob(save_path + '*'):\n",
    "        saver.restore(sess, save_path)\n",
    "        print(\"Model restored.\")\n",
    "    else:\n",
    "        raise\n",
    "        \n",
    "    batches = test_and_valid_batches(32, valid_input, valid_target)\n",
    "    all_logits = np.array([[0] * 919])\n",
    "    all_targets = np.array([[0] * 919])\n",
    "    valid_batches = test_and_valid_batches(batch_size, valid_input, valid_target)\n",
    "    for j in range(valid_size / batch_size):\n",
    "        b, t = valid_batches.next()\n",
    "        _logits = sess.run(logits, feed_dict={\n",
    "            input_placeholder: b\n",
    "        })\n",
    "#         all_logits += _logits[:,0].tolist()\n",
    "        all_logits = np.append(all_logits, _logits, axis = 0)\n",
    "#         all_targets += t[:,0].tolist()\n",
    "        all_targets = np.append(all_targets, t, axis = 0)\n",
    "\n",
    "all_logits = all_logits[1:]\n",
    "all_targets = all_targets[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.subplot(223)\n",
    "\n",
    "our_scores = []\n",
    "for i in np.arange(919):\n",
    "    fpr, tpr, _ = roc_curve(all_targets[:, i],all_logits[:, i])\n",
    "    try:\n",
    "        our_scores += [roc_auc_score(all_targets[:, i],all_logits[:, i])]\n",
    "    except:\n",
    "        continue\n",
    "    plt.plot(fpr, tpr, alpha=0.01, color = \"blue\")\n",
    "\n",
    "\n",
    "    plt.xlabel('fpr')\n",
    "    plt.ylabel('tpr')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('Our CNN')\n",
    "\n",
    "print(np.mean(our_scores))\n",
    "    \n",
    "all_targets = loadmat(\"../genomics/deepsea_train/valid.mat\")[\"validdata\"]\n",
    "print(\"done loading\")\n",
    "plt.subplot(221)\n",
    "probs = []\n",
    "for s in [\"0\", \"1\", \"2\", \"3\"]:\n",
    "    with open(\"../genomics/fastas/infile.fasta\" + s + \".out\", \"r\") as f:\n",
    "        for l in f:\n",
    "            break\n",
    "        for l in f:\n",
    "            probs_for_l = [float(i) for i in l.split(\",\")[2:]]\n",
    "            probs += [probs_for_l]\n",
    "            \n",
    "probs = np.stack(probs)\n",
    "            \n",
    "scores = []\n",
    "for i in np.arange(919):\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    fpr, tpr, _ = roc_curve(all_targets[:,i],probs[:,i])\n",
    "    try:\n",
    "        scores += [roc_auc_score(all_targets[:,i],probs[:,i])]\n",
    "    except:\n",
    "        continue\n",
    "    plt.plot(fpr, tpr, alpha=0.01, color = \"blue\")\n",
    "\n",
    "    plt.ylabel('tpr')\n",
    "    plt.tick_params(\n",
    "    axis='x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom='off',      # ticks along the bottom edge are off\n",
    "    top='off',         # ticks along the top edge are off\n",
    "    labelbottom='off') # labels along the bottom edge are off\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('DeepSea')\n",
    "    \n",
    "\n",
    "\n",
    "plt.show()\n",
    "print(np.mean(scores))\n",
    "\n",
    "print(np.count_nonzero(np.array(our_scores) > np.array(scores)))\n",
    "plt.hist(our_scores)\n",
    "plt.hist(scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(np.array(our_scores) - np.array(scores), bins = np.arange(-1,1.01,.01))\n",
    "plt.title(\"Deepsea AUC - Our CNN AUC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(our_scores, alpha = .5, bins = np.arange(0,1.01,.01))\n",
    "plt.hist(scores, alpha = .5, bins = np.arange(0,1.01,.01))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = h5py.File(\"../genomics/results-hdf5/CEBPB-A549.hdf5\", \"r\")\n",
    "a5_labels = tmp['label']\n",
    "a5_seq = tmp['seq']\n",
    "tmp = h5py.File(\"../genomics/results-hdf5/CEBPB-H1Hesc.hdf5\", \"r\")\n",
    "h1_labels = tmp['label']\n",
    "h1_seq = tmp['seq']\n",
    "tmp = h5py.File(\"../genomics/results-hdf5/CEBPB-HCT116.hdf5\", \"r\")\n",
    "hc_labels = tmp['label']\n",
    "hc_seq = tmp['seq']\n",
    "tmp = h5py.File(\"../genomics/results-hdf5/CEBPB-IMR90.hdf5\", \"r\")\n",
    "im_labels = tmp['label']\n",
    "im_seq = tmp['seq']\n",
    "\n",
    "def cell_batches(batch_size, input_, target):\n",
    "    for i in xrange(int(input_.shape[0]/batch_size)):\n",
    "        yield input_[i*batch_size:(i+1)*batch_size,0:1000,0:4], target[i*batch_size:(i+1)*batch_size]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indicies = []\n",
    "titles = []\n",
    "\n",
    "with open(\"../genomics/DeepSEA-v0.94/resources/feature_name\") as f:\n",
    "    for i, row in enumerate(f):\n",
    "        if \"CEBPB\" in row.strip(\"\\n\").split(\"\\t\")[1]:\n",
    "            indicies += [int(row.strip(\"\\n\").split(\"\\t\")[0])]\n",
    "            titles += [row.strip(\"\\n\").split(\"\\t\")[1]]\n",
    "indicies, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    if glob.glob(save_path + '*'):\n",
    "        saver.restore(sess, save_path)\n",
    "        print(\"Model restored.\")\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "    for idx in [4]:\n",
    "        input_ = h1_seq\n",
    "        targs = h1_labels\n",
    "\n",
    "        batches = cell_batches(32, input_, targs)\n",
    "        all_logits_cell = []\n",
    "        all_targets_cell = []\n",
    "        i = 0\n",
    "        for b, t in batches:\n",
    "            _logits = sess.run(logits, feed_dict={\n",
    "                input_placeholder: b\n",
    "            })\n",
    "            all_logits_cell += _logits[:,idx].tolist()\n",
    "            all_targets_cell += t.flatten().tolist()\n",
    "            i += 1\n",
    "            if i == 100:\n",
    "                break\n",
    "        all_logits_cell = np.array(all_logits_cell)\n",
    "        all_targets_cell = np.array(all_targets_cell)\n",
    "#         fpr, tpr, _ = roc_curve(all_targets_cell,all_logits_cell)\n",
    "\n",
    "#         plt.plot(fpr, tpr, color = \"blue\")\n",
    "#         print(roc_auc_score(all_targets_cell,all_logits_cell))\n",
    "#         print(all_logits_cell[0:10])\n",
    "#         plt.show()\n",
    "        plt.hist(all_logits_cell[all_targets_cell == 0], alpha = .5, bins = np.arange(0,1.01,.001), density = True, label = \"Our data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(all_logits_cell[all_targets_cell == 0], alpha = .5, bins = np.arange(0,1.01,.001), density = True, label = \"Our data\")\n",
    "plt.hist(all_logits[:,idx][all_targets[:,idx] == 0],alpha = .5, bins = np.arange(0,1.01,.001), density = True, label = \"Their data\")\n",
    "plt.title(\"Negative Examples\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
