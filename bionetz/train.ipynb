{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os, glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lrelu(x, alpha=0.2):\n",
    "    return tf.maximum(alpha*x, x)\n",
    "\n",
    "def fc(x, n_units, dropout, activation=None):\n",
    "    net = tf.layers.dense(x, n_units)\n",
    "    net = tf.contrib.layers.layer_norm(net)\n",
    "    if activation:\n",
    "        net = activation(net)\n",
    "    return tf.layers.dropout(net, dropout)\n",
    "\n",
    "def conv1d(x, hidden_size, kernel_size, stride=1, dilation=1,\n",
    "           pooling_size=0, dropout=0.0, activation=None):\n",
    "    net = tf.layers.conv1d(x, hidden_size, kernel_size, stride, padding='same',\n",
    "                           dilation_rate=dilation, activation=activation)\n",
    "    if pooling_size:\n",
    "        net = tf.layers.max_pooling1d(net, pooling_size, pooling_size, padding=\"same\")\n",
    "    return tf.layers.dropout(net, dropout)\n",
    "\n",
    "def cnn(input_, n_classes, hp):\n",
    "    net = input_\n",
    "    for i in xrange(hp.n_conv_layers):\n",
    "        net = conv1d(net, hp.hidden_size, hp.kernel_size, hp.stride, dilation=1,\n",
    "                     pooling_size=hp.pooling_sizes[i], dropout=hp.dropout,\n",
    "                     activation=hp.activation)\n",
    "    for i in xrange(hp.n_dconv_layers):\n",
    "        dilation= 2**(i + 1)\n",
    "        tmp = conv1d(net, hp.hidden_size, hp.kernel_size, hp.stride, dilation=1,\n",
    "                     pooling_size=0, dropout=hp.dropout, activation=hp.activation)\n",
    "        net = tf.concat([net, tmp], axis=2)\n",
    "    net = tf.contrib.layers.flatten(net)\n",
    "    net = fc(net, hp.hidden_size, hp.dropout, activation=hp.activation)\n",
    "    return fc(net, n_classes, hp.dropout, activation=hp.output_activation)\n",
    "\n",
    "def rnn(input_, n_classes, hp): # RNN\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(hp.hidden_size, forget_bias=1.0)\n",
    "    \n",
    "    values = tf.unstack(input_, axis=1)\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(cell, values, dtype=tf.float32)\n",
    "    return fc(outputs[-1], n_classes, hp.dropout, activation=hp.output_activation)\n",
    "\n",
    "def srnn(input_, n_classes, hp): # Stacked RNN\n",
    "    cells = [tf.contrib.rnn.BasicLSTMCell(hp.hidden_size, forget_bias=1.0)\n",
    "             for _ in xrange(hp.n_layers)]\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "    \n",
    "    values = tf.unstack(input_, axis=1)\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(cell, values, dtype=tf.float32)\n",
    "    return fc(outputs[-1], n_classes, hp.dropout, activation=hp.output_activation)\n",
    "\n",
    "def birnn(input_, n_classes, hp): # Bidirectional RNN\n",
    "    fw = tf.contrib.rnn.BasicLSTMCell(hp.hidden_size, forget_bias=1.0)\n",
    "    bw = tf.contrib.rnn.BasicLSTMCell(hp.hidden_size, forget_bias=1.0)\n",
    "    \n",
    "    values = tf.unstack(input_, axis=1)\n",
    "    outputs, fw_states, bw_states = tf.contrib.rnn.static_bidirectional_rnn(\n",
    "        fw, bw, values, dtype=tf.float32)\n",
    "    return fc(outputs[-1], n_classes, hp.dropout, activation=hp.output_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_hp(**kwargs):\n",
    "    hp = tf.contrib.training.HParams()\n",
    "    hp.n_conv_layers = 4\n",
    "    hp.n_dconv_layers = 4\n",
    "    hp.hidden_size = 64\n",
    "    hp.kernel_size = 8\n",
    "    hp.pooling_sizes = [2, 2, 2, 4]\n",
    "    hp.stride = 1\n",
    "    hp.dropout = 0.9\n",
    "    hp.activation = lrelu\n",
    "    hp.output_activation = tf.sigmoid\n",
    "    hp.__dict__.update(kwargs)\n",
    "    return hp\n",
    "\n",
    "def rnn_hp(**kwargs):\n",
    "    hp = tf.contrib.training.HParams()\n",
    "    hp.hidden_size = 64\n",
    "    hp.n_layers = 2 # Stacked RNN\n",
    "    hp.dropout = 0.9\n",
    "    hp.output_activation = tf.sigmoid\n",
    "    hp.__dict__.update(kwargs)\n",
    "    return hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "from scipy.io import loadmat\n",
    "\n",
    "tmp = h5py.File('deepsea_train/train.mat', 'r')\n",
    "train_input = tmp['trainxdata']\n",
    "train_target = tmp['traindata']\n",
    "\n",
    "tmp = loadmat('deepsea_train/valid.mat')\n",
    "valid_input = tmp['validxdata']\n",
    "valid_target = tmp['validdata']\n",
    "\n",
    "def test_and_valid_batches(batch_size, input_, target):\n",
    "    while True:\n",
    "        for i in xrange(int(input_.shape[0]/batch_size)):\n",
    "            yield input_[i*batch_size:(i+1)*batch_size], target[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "def train_batches(batch_size, input_, target):\n",
    "    while True:\n",
    "        for i in xrange(int(input_.shape[-1]/batch_size)):\n",
    "            yield (input_[:,:,i*batch_size:(i+1)*batch_size].transpose([2, 0, 1]),\n",
    "                   target[:,i*batch_size:(i+1)*batch_size].transpose([1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "input_placeholder = tf.placeholder(dtype=tf.float32, shape=[None, 1000, 4])\n",
    "target_placeholder = tf.placeholder(dtype=tf.float32, shape=[None, 919])\n",
    "\n",
    "# CNN\n",
    "# hp = cnn_hp(n_dconv_layers=0)\n",
    "# logits = cnn(input_placeholder, 919, hp)\n",
    "\n",
    "# Dilated CNN\n",
    "# hp = cnn_hp()\n",
    "# logits = cnn(input_placeholder, 919, hp)\n",
    "\n",
    "# RNN\n",
    "# hp = rnn_hp()\n",
    "# logits = rnn(input_placeholder, 919, hp)\n",
    "\n",
    "# Stacked RNN\n",
    "hp = rnn_hp()\n",
    "logits = srnn(input_placeholder, 919, hp)\n",
    "\n",
    "# Bidirectional RNN\n",
    "# hp = rnn_hp()\n",
    "# logits = birnn(input_placeholder, 919, hp)\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    logits=logits,labels=target_placeholder))\n",
    "optimizer = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "print(\"Graph compiled.\")\n",
    "n_parameters = np.sum([np.prod(v.shape) for v in tf.trainable_variables()])\n",
    "print(\"Number of parameters: %i\" % n_parameters)\n",
    "\n",
    "import logz\n",
    "\n",
    "experiment = 'srnn_0'\n",
    "logdir = os.path.join(\"/tmp/\", experiment)\n",
    "save_path = os.path.join(logdir, \"model.ckpt\")\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "logz.configure_output_dir(logdir)\n",
    "log_freq = 5\n",
    "save_freq = 100\n",
    "\n",
    "iterations = 10000\n",
    "batch_size = 32\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if glob.glob(save_path + '*'):\n",
    "        saver.restore(sess, save_path)\n",
    "        print(\"Model restored.\")\n",
    "    else:\n",
    "        sess.run(init_op)\n",
    "        print(\"Model initialized.\")\n",
    "    \n",
    "    batches = train_batches(batch_size, train_input, train_target)\n",
    "    \n",
    "    for i in xrange(iterations):\n",
    "        input_, target = batches.next()\n",
    "        _loss, _ = sess.run([loss, optimizer], feed_dict={\n",
    "            input_placeholder: input_,\n",
    "            target_placeholder: target,\n",
    "        })\n",
    "        if i % log_freq == 0:\n",
    "            # print 'Iteration %d: loss = %.4f' % (i, _loss)\n",
    "            logz.log_tabular('Iteration', i)\n",
    "            logz.log_tabular('Loss', _loss)\n",
    "            logz.dump_tabular()\n",
    "        if i % save_freq == 0:\n",
    "            save_path = saver.save(sess, save_path)\n",
    "            print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
