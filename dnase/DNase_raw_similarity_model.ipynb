{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import pyDNase\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics\n",
    "import kipoi\n",
    "import os\n",
    "import pybedtools\n",
    "import torch\n",
    "import h5sparse\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "from pyDNase import GenomicInterval\n",
    "from scipy.sparse import coo_matrix, vstack\n",
    "\n",
    "from scipy.fftpack import fft, ifft\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Paths for this user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## PATHS ###########################\n",
    "# path to where dnase bams are stored. Bams need to be sorted and indexed. See bin/download_dnase_encode.sh for\n",
    "# data processing\n",
    "_ENCODE_DATA_PREFIX =  \"/data/akmorrow/encode_data/\"\n",
    "\n",
    "deepsea_path = \"/data/akmorrow/epitome_data/deepsea_train/\"\n",
    "dnase_preprocessed_path = \"/data/akmorrow/epitome_data/processed_dnase/\"\n",
    "feature_path = \"/home/eecs/akmorrow/epitome/data/feature_name\"\n",
    "\n",
    "_DEEPSEA_GENOME_REGIONS_FILENAME = \"/home/eecs/akmorrow/epitome/data/allTFs.pos.bed\"\n",
    "\n",
    "# DNase filepath dictionary\n",
    "prefix_ =  \"/data/akmorrow/encode_data/\"\n",
    "\n",
    "\n",
    "# get TF logger\n",
    "log = logging.getLogger('tensorflow')\n",
    "tf_log = \"/home/eecs/akmorrow/epitome/dnase/tensorflow_combination.log\"\n",
    "fh = logging.FileHandler(tf_log)\n",
    "fh.setLevel(logging.INFO)\n",
    "log.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(open(\"./constants.py\").read())\n",
    "exec(open(\"./functions.py\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load DeepSEA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(408000, 4, 1000) (4000000, 4, 1000) (455024, 4, 1000)\n",
      "(919, 408000) (919, 4000000) (919, 455024)\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, test_data = load_deepsea_data(deepsea_path)\n",
    "\n",
    "print(valid_data[\"x\"].shape, train_data[\"x\"].shape, test_data[\"x\"].shape)\n",
    "print(valid_data[\"y\"].shape, train_data[\"y\"].shape, test_data[\"y\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Cut Sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load DNase data to h5sparse files\n",
    "dnase_train = h5sparse.File(0(dnase_preprocessed_path, \"processed_dnase_train_sparse.h5\"))\n",
    "dnase_valid = h5sparse.File(os.path.join(dnase_preprocessed_path, \"processed_dnase_valid_sparse.h5\"))\n",
    "dnase_test  = h5sparse.File(os.path.join(dnase_preprocessed_path, \"processed_dnase_test_sparse.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.6 s, sys: 33.2 s, total: 1min 19s\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This code moves around the results in validation and test so that the validation set is larger\n",
    "dnase_valid_dict = toSparseIndexedDictionary(dnase_train, dnase_valid,dnase_test, Dataset.VALID, normalize = True)\n",
    "dnase_test_dict = toSparseIndexedDictionary(dnase_train, dnase_valid,dnase_test, Dataset.TEST, normalize = True)\n",
    "dnase_train_dict = toSparseIndexedDictionary(dnase_train, dnase_valid,dnase_test, Dataset.TRAIN, normalize = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose cell types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['K562', 'GM12878', 'H1-hESC', 'HepG2', 'HeLa-S3', 'A549', 'HUVEC', 'GM12891', 'MCF-7', 'GM12892', 'HCT-116'] ['DNase', 'CTCF', 'Pol2', 'YY1', 'p300', 'TAF1', 'Pol2-4H8', 'c-Myc', 'Rad21', 'Max', 'NRSF', 'GABP', 'EZH2', 'CEBPB', 'c-Jun', 'ZBTB33', 'USF2', 'USF-1', 'TBP', 'RFX5']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# matrix is cell types by factors, contains indices in feature vector\n",
    "matrix, cellmap, assaymap = get_assays_from_feature_file()\n",
    "\n",
    "inv_assaymap = {v: k for k, v in assaymap.items()}\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.set_aspect('equal')\n",
    "plt.xticks(np.arange(len(assaymap)), rotation = 90)\n",
    "ax.set_xticklabels(assaymap.keys())\n",
    "plt.yticks(np.arange(len(cellmap)))\n",
    "ax.set_yticklabels(cellmap.keys())\n",
    "\n",
    "plt.imshow(matrix!=-1)\n",
    "print(list(cellmap.keys())[:11], list(assaymap.keys())[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select train, valid and test cell types\n",
    "\n",
    "- test on A549\n",
    "- train/validate on K562 and HepG2\n",
    "- use remaining cell types as feature input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A549 is for test. 12 available features\n",
    "test_celltypes = [\"A549\"]\n",
    "\n",
    "valid_y_indices = np.copy(matrix[cellmap['A549']])\n",
    "# indices that are unavailable for A549\n",
    "valid_holdout_indices = np.where(valid_y_indices == -1)[0]\n",
    "# TODO: need the vectors to be the same length, but you should not evaluate on these\n",
    "# Currently, set -1's to 0 these will all be evaluated on the DNase\n",
    "valid_y_indices[matrix[cellmap['A549']] ==-1]=0\n",
    "\n",
    "# K562, HepG2 indices is for train/valid. 19 available features\n",
    "evaluation_celltypes = [\"K562\", \"HepG2\"] \n",
    "y_indices = [matrix[cellmap['K562']][matrix[cellmap['K562']]!=-1], \n",
    "             matrix[cellmap['HepG2']][matrix[cellmap['HepG2']]!=-1]]\n",
    "\n",
    "# indices_mat is used to pull the remaining indices from cell types not used for prediction.\n",
    "# delete evaluation cell types from the matrix (A549, HepG2 and K562)\n",
    "indices_mat = np.delete(matrix, [0,3,5], axis=0)\n",
    "\n",
    "# get all feature locations for DNase for remaining cell types (just the first column in matrix)\n",
    "dnase_indices = indices_mat[:,0] # for all of the cell types (including the cell type we are evaluating)\n",
    "indices = indices_mat[indices_mat!=-1]\n",
    "\n",
    "# cell types not used in validation\n",
    "train_celltypes = ['GM12878', 'H1-hESC', 'HeLa-S3', 'HUVEC', 'GM12891', 'MCF-7', 'GM12892', 'HCT-116']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator with just processed DNase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Distance functions\n",
    "def gen(data, DATA_LABEL, radii=[1,3]):\n",
    "    def g():\n",
    "        \n",
    "        # determines which indices to use for labels\n",
    "        if (DATA_LABEL == Dataset.TEST):\n",
    "            y_index_vector = [valid_y_indices]\n",
    "        else:\n",
    "            y_index_vector = y_indices\n",
    "            \n",
    "        if (len(radii) > 0):\n",
    "            range_ = range(max(radii), data[\"y\"].shape[-1]-max(radii))\n",
    "        else: \n",
    "            range_ = range(0, data[\"y\"].shape[-1])\n",
    " \n",
    "        for i in range_: # for all records\n",
    "            for y_index in y_index_vector:\n",
    "                dnases = [] \n",
    "                for radius in radii:\n",
    "                    # within the radius, fraction of places where they are both 1\n",
    "                    dnase_double_positive = np.average(data[\"y\"][dnase_indices,i-radius:i+radius+1]*\n",
    "                                             data[\"y\"][y_index[0],i-radius:i+radius+1], axis=1)\n",
    "                    \n",
    "                    # within the radius, fraction of places where they are both equal (0 or 1)\n",
    "                    dnase_agreement = np.average(data[\"y\"][dnase_indices,i-radius:i+radius+1]==\n",
    "                                             data[\"y\"][y_index[0],i-radius:i+radius+1], axis=1)\n",
    "                    dnases.extend(dnase_double_positive)\n",
    "                    dnases.extend(dnase_agreement)\n",
    "                    \n",
    "                # Remove DNase from prediction indices. \n",
    "                # You should not predict on assays you use to calculate the distance metric.\n",
    "                y_index_no_dnase = np.delete(y_index, [0])\n",
    "                yield np.concatenate([data[\"y\"][indices,i],dnases]), data[\"y\"][y_index_no_dnase,i] \n",
    "    return g\n",
    "\n",
    "\n",
    "def make_dataset_for_celltype(cell, which_dataset, radii):\n",
    "    \"\"\"\n",
    "    Create data matrix for a specific cell type\n",
    "    \"\"\"\n",
    "    \n",
    "    y_vector_indices = None # TODO\n",
    "    \n",
    "\n",
    "def make_dataset(data,\n",
    "                 which_dataset,\n",
    "                 batch_size,\n",
    "                 shuffle_size,\n",
    "                 prefetch_size,\n",
    "                 radii):\n",
    "\n",
    "    generator = gen(data, which_dataset, radii)\n",
    "    \n",
    "    for x, y in generator():\n",
    "        break\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_types=(tf.float32,)*2,\n",
    "        output_shapes=(x.shape, y.shape,)\n",
    "    )\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.shuffle(shuffle_size)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.prefetch(prefetch_size)\n",
    "    return y.shape, dataset.make_one_shot_iterator()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw DNase Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gen_dnase(data, DATA_LABEL, radii, chunk_size = 1):\n",
    "    def g():\n",
    "        # determines which indices to use for labels\n",
    "        if (DATA_LABEL == Dataset.TEST):\n",
    "            celltypes = test_celltypes\n",
    "            y_index_vector = [valid_y_indices]\n",
    "        else:\n",
    "            celltypes = evaluation_celltypes\n",
    "            y_index_vector = y_indices\n",
    "            \n",
    "            \n",
    "        if (len(radii) > 0):\n",
    "            start = max(radii)\n",
    "            end = data[\"y\"].shape[-1]-max(radii)\n",
    "        else: \n",
    "            start = 0\n",
    "            end = data[\"y\"].shape[-1]\n",
    "            range_ = range(start, end)\n",
    "\n",
    "        start_ = start\n",
    "        while (start_ < end):\n",
    "            end_ = min(start + chunk_size, end)\n",
    "            range_ = range(start_, end_)\n",
    "            \n",
    "            # for all cells in either train or test\n",
    "            for cell_i in range(0, len(celltypes)): # for each y_index [K562, HepG2], or [A549] if test\n",
    "\n",
    "                celltype = celltypes[cell_i]\n",
    "                y_index = y_index_vector[cell_i]\n",
    " \n",
    "                # get dnase for this cell type\n",
    "                dnase_i = get_dnase_array_from_modified_dict(dnase_train_dict, dnase_valid_dict, dnase_test_dict,\n",
    "                                                         range_, celltype, DATA_LABEL=DATA_LABEL)\n",
    "\n",
    "                dnases = np.empty((len(range_), 0))\n",
    "\n",
    "                # compare to all other cell types\n",
    "                for train_celltype in train_celltypes: \n",
    "\n",
    "                    dnase_j = get_dnase_array_from_modified_dict(dnase_train_dict, dnase_valid_dict, dnase_test_dict, \n",
    "                                                            range_, train_celltype, DATA_LABEL=DATA_LABEL)\n",
    "\n",
    "                    var_difference = abs(np.var(dnase_i.toarray(), axis=1) - np.var(dnase_j.toarray(), axis=1))\n",
    "                    var_difference = np.reshape(var_difference, (-1,  1))\n",
    "                    fft_mse = np.mean((fft(dnase_i.toarray(), axis=1) - fft(dnase_j.toarray(), axis=1))**2, axis=1).real\n",
    "\n",
    "                    fft_mse = np.reshape(fft_mse, (-1,  1))\n",
    "                    concat= np.concatenate((var_difference,fft_mse), axis=1)\n",
    "                    dnases = np.append(dnases, concat, axis=1)\n",
    "\n",
    "                # Remove DNase from prediction indices. \n",
    "                # You should not predict on assays you use to calculate the distance metric.\n",
    "                y_index_no_dnase = np.delete(y_index, [0])\n",
    "\n",
    "                for i in range(start_, end_):\n",
    "                    yield np.concatenate([data[\"y\"][indices,i].T,dnases[i,:]]), data[\"y\"][y_index_no_dnase, i]\n",
    "                \n",
    "            start += chunk_size\n",
    "\n",
    "    return g\n",
    "\n",
    "def make_dataset_dnase(data,\n",
    "                 which_dataset, # Dataset enum\n",
    "                 batch_size,\n",
    "                 shuffle_size,\n",
    "                 prefetch_size,\n",
    "                 radii):\n",
    "    \n",
    "    generator = gen_dnase(data, which_dataset, radii, chunk_size=50000)\n",
    "    \n",
    "    for x, y in generator():\n",
    "        break\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_types=(tf.float32,)*2,\n",
    "        output_shapes=(x.shape, y.shape,)\n",
    "    )\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.shuffle(shuffle_size)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.prefetch(prefetch_size)\n",
    "    return y.shape, dataset.make_one_shot_iterator()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combination Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dnase_combination(data, DATA_LABEL, radii, chunk_size = 1):\n",
    "    def g():\n",
    "        # determines which indices to use for labels\n",
    "        if (DATA_LABEL == Dataset.TEST):\n",
    "            celltypes = test_celltypes\n",
    "            y_index_vector = [valid_y_indices]\n",
    "        else:\n",
    "            celltypes = evaluation_celltypes\n",
    "            y_index_vector = y_indices\n",
    "            \n",
    "            \n",
    "        if (len(radii) > 0):\n",
    "            start = max(radii)\n",
    "            end = data[\"y\"].shape[-1]-max(radii)\n",
    "        else: \n",
    "            start = 0\n",
    "            end = data[\"y\"].shape[-1]\n",
    "            range_ = range(start, end)\n",
    "\n",
    "        start_ = start\n",
    "        while (start_ < end):\n",
    "            end_ = min(start_ + chunk_size, end)\n",
    "            range_ = range(start_, end_)\n",
    "            \n",
    "            # for all cells in either train or test\n",
    "            for cell_i in range(0, len(celltypes)): # for each y_index [K562, HepG2], or [A549] if test\n",
    "\n",
    "                celltype = celltypes[cell_i]\n",
    "                y_index = y_index_vector[cell_i]\n",
    " \n",
    "                # get dnase for this cell type\n",
    "                dnase_i = get_dnase_array_from_modified_dict(dnase_train_dict, dnase_valid_dict, dnase_test_dict,\n",
    "                                                         range_,celltype, DATA_LABEL=DATA_LABEL)\n",
    "\n",
    "                dnases = np.empty((len(range_), 0))\n",
    "\n",
    "                # compare to all other cell types\n",
    "                for train_celltype in train_celltypes: \n",
    "\n",
    "                    dnase_j = get_dnase_array_from_modified_dict(dnase_train_dict, dnase_valid_dict, dnase_test_dict, \n",
    "                                                            range_, train_celltype, DATA_LABEL=DATA_LABEL)\n",
    "\n",
    "                    var_difference = abs(np.var(dnase_i.toarray(), axis=1) - np.var(dnase_j.toarray(), axis=1))\n",
    "                    var_difference = np.reshape(var_difference, (-1,  1))\n",
    "                    fft_mse = np.mean((fft(dnase_i.toarray(), axis=1) - fft(dnase_j.toarray(), axis=1))**2, axis=1).real\n",
    "\n",
    "                    fft_mse = np.reshape(fft_mse, (-1,  1))\n",
    "                    concat= np.concatenate((var_difference,fft_mse), axis=1)\n",
    "                    dnases = np.append(dnases, concat, axis=1)\n",
    "\n",
    "                \n",
    "                # Remove DNase from prediction indices. \n",
    "                # You should not predict on assays you use to calculate the distance metric.\n",
    "                y_index_no_dnase = np.delete(y_index, [0])\n",
    "\n",
    "                for i in range(start_, end_):\n",
    "                    \n",
    "                    dnases_radii = [] \n",
    "                    for radius in radii:\n",
    "                        # within the radius, fraction of places where they are both 1\n",
    "                        dnase_double_positive = np.average(data[\"y\"][dnase_indices,i-radius:i+radius+1]*\n",
    "                                                 data[\"y\"][y_index[0],i-radius:i+radius+1], axis=1)\n",
    "\n",
    "                        # within the radius, fraction of places where they are both equal (0 or 1)\n",
    "                        dnase_agreement = np.average(data[\"y\"][dnase_indices,i-radius:i+radius+1]==\n",
    "                                                 data[\"y\"][y_index[0],i-radius:i+radius+1], axis=1)\n",
    "                        dnases_radii.extend(dnase_double_positive)\n",
    "                        dnases_radii.extend(dnase_agreement)\n",
    "\n",
    "                    dnases_radii = np.array(dnases_radii)\n",
    "                    \n",
    "                    yield np.concatenate([data[\"y\"][indices,i].T,dnases[i-start_,:], dnases_radii]), data[\"y\"][y_index_no_dnase, i]\n",
    "                \n",
    "            start_ += chunk_size\n",
    "\n",
    "    return g\n",
    "\n",
    "def make_dataset_dnase_combination(data,\n",
    "                 which_dataset, # Dataset enum\n",
    "                 batch_size,\n",
    "                 shuffle_size,\n",
    "                 prefetch_size,\n",
    "                 radii):\n",
    "    \n",
    "    generator = gen_dnase_combination(data, which_dataset, radii, chunk_size=50000)\n",
    "    \n",
    "    for x, y in generator():\n",
    "        break\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_types=(tf.float32,)*2,\n",
    "        output_shapes=(x.shape, y.shape,)\n",
    "    )\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.shuffle(shuffle_size)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.prefetch(prefetch_size)\n",
    "    return y.shape, dataset.make_one_shot_iterator()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check how DeepSea does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-fd136027ad7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkipoi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DeepSEA/predict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/akmorrow/miniconda3/envs/EpitomeEnv/lib/python3.6/site-packages/kipoi/model.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(model, source, with_dataloader)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# pull the model & get the model directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0myaml_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpull_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0msource_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myaml_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/akmorrow/miniconda3/envs/EpitomeEnv/lib/python3.6/site-packages/kipoi/sources.py\u001b[0m in \u001b[0;36mpull_model\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpull_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pull_component\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpull_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/akmorrow/miniconda3/envs/EpitomeEnv/lib/python3.6/site-packages/kipoi/sources.py\u001b[0m in \u001b[0;36m_pull_component\u001b[0;34m(self, component, which)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mlfs_installed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraise_exception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pulled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpull_source\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mcomponent_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/akmorrow/miniconda3/envs/EpitomeEnv/lib/python3.6/site-packages/kipoi/sources.py\u001b[0m in \u001b[0;36mpull_source\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    370\u001b[0m                          \"pull\"],\n\u001b[1;32m    371\u001b[0m                         \u001b[0mcwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m                         env=dict(os.environ, GIT_LFS_SKIP_SMUDGE=\"1\"))\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pulled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/akmorrow/miniconda3/envs/EpitomeEnv/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0mretcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ls\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-l\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \"\"\"\n\u001b[0;32m--> 267\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/akmorrow/miniconda3/envs/EpitomeEnv/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[1;32m    707\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    710\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m             \u001b[0;31m# Cleanup if the child failed starting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/akmorrow/miniconda3/envs/EpitomeEnv/lib/python3.6/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1296\u001b[0m                 \u001b[0merrpipe_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1298\u001b[0;31m                     \u001b[0mpart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrpipe_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1299\u001b[0m                     \u001b[0merrpipe_data\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpart\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrpipe_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = kipoi.get_model('DeepSEA/predict')\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on all validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_preds = []\n",
    "for i in np.arange(0, len(valid_data[\"x\"]), batch_size):\n",
    "    batch = valid_data[\"x\"][i:i+batch_size]\n",
    "    batch = np.expand_dims(batch, 2)\n",
    "    batch = batch[:,[0,2,1,3]]\n",
    "    valid_preds.append(model.predict_on_batch(batch.astype(np.float32)))\n",
    "valid_preds = np.concatenate(valid_preds, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on all test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = []\n",
    "for i in np.arange(0, len(test_data[\"x\"]), batch_size):\n",
    "    batch = test_data[\"x\"][i:i+batch_size]\n",
    "    batch = np.expand_dims(batch, 2)\n",
    "    batch = batch[:,[0,2,1,3]]\n",
    "    test_preds.append(model.predict_on_batch(batch.astype(np.float32)))\n",
    "test_preds = np.concatenate(test_preds, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of DeepSea through averaging\n",
    "\n",
    "We want to average together the factors and see how it does on:\n",
    "1. HepG2/K562 (validation)\n",
    "2. A549 (test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. HepG2/K562 (validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro score from averaging 0.9108393281713489\n",
      "Micro score from averaging 0.9319089399333468\n",
      "0 0.8475896488335533\n",
      "1 0.9860134551918847\n",
      "2 0.9072402824226543\n",
      "3 0.9410950132803635\n",
      "4 0.7650114072533384\n",
      "5 0.9564704584616246\n",
      "6 0.8562495117523279\n",
      "7 0.9441793263529175\n",
      "8 0.9798146625765454\n",
      "9 0.9157246598860256\n",
      "10 0.8779347984716908\n",
      "11 0.9335361482585034\n",
      "12 0.878278458631916\n",
      "13 0.9352162015045606\n",
      "14 0.9184113000053894\n",
      "15 0.8906184636733092\n",
      "16 0.9718895826918873\n",
      "17 0.9543735392701003\n",
      "18 0.8869429616428643\n",
      "19 0.8701966832655255\n"
     ]
    }
   ],
   "source": [
    "weights = np.tile((indices_mat!=-1).reshape(indices_mat.shape + (1,)), (1, 1, valid_data[\"y\"].shape[-1]))\n",
    "\n",
    "# weight by all cell types except A549, HepG2, and K562\n",
    "average_preds = np.average(valid_preds.T[indices_mat], axis=0, weights=weights)\n",
    "average_preds = np.concatenate([average_preds,average_preds],axis=1)\n",
    "\n",
    "# y_indices are indices for K562 (0) and HepG2(1)\n",
    "truth = np.concatenate([valid_data[\"y\"][y_indices[0]],valid_data[\"y\"][y_indices[1]]],axis=1)\n",
    "\n",
    "print(\"Macro score from averaging\", sklearn.metrics.roc_auc_score(truth.T, average_preds.T, average='macro'))\n",
    "print(\"Micro score from averaging\", sklearn.metrics.roc_auc_score(truth.T, average_preds.T, average='micro'))\n",
    "for i in range(len(truth)):\n",
    "    assay = inv_assaymap[i]\n",
    "    print(assay[i], sklearn.metrics.roc_auc_score(truth[i].T, average_preds[i].T, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. A549 (test) through DeepSEA Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro score from averaging 0.8887567862655935\n",
      "Micro score from averaging 0.8933466077881789\n",
      "0 DNase nan\n",
      "1 CTCF 0.9732582618637076\n",
      "2 Pol2 0.9740896187256735\n",
      "3 YY1 0.8937753311227441\n",
      "4 p300 0.7829031969754814\n",
      "5 TAF1 0.9091835813538927\n",
      "6 Pol2-4H8 nan\n",
      "7 c-Myc nan\n",
      "8 Rad21 0.9874737314077517\n",
      "9 Max 0.9661944546335738\n",
      "10 NRSF 0.7467304399291882\n",
      "11 GABP 0.8429410069617789\n",
      "12 EZH2 nan\n",
      "13 CEBPB 0.957693187759048\n",
      "14 c-Jun nan\n",
      "15 ZBTB33 0.8062263300698922\n",
      "16 USF2 nan\n",
      "17 USF-1 0.8246122943843901\n",
      "18 TBP nan\n",
      "19 RFX5 nan\n"
     ]
    }
   ],
   "source": [
    "weights = np.tile((indices_mat!=-1).reshape(indices_mat.shape + (1,)), (1, 1, test_data[\"y\"].shape[-1]))\n",
    "\n",
    "average_preds = np.average(test_preds.T[indices_mat], axis=0, weights=weights)\n",
    "\n",
    "# filter out missing indices from average_preds as well as DNase at position 0\n",
    "average_preds = np.delete(average_preds, np.append(0, valid_holdout_indices), 0)\n",
    "\n",
    "truth = test_data[\"y\"]\n",
    "\n",
    "# filter out missing indices from truth that do not exist for A549, as well as DNase\n",
    "valid_y_indices_n = np.copy(matrix[cellmap['A549']])\n",
    "# remove DNase (first value)\n",
    "valid_y_indices_n[0] = -1\n",
    "valid_y_indices_n = valid_y_indices_n[valid_y_indices_n != -1]\n",
    "truth = truth[valid_y_indices_n,]\n",
    "\n",
    "print(\"Macro score from averaging\", sklearn.metrics.roc_auc_score(truth.T, average_preds.T, average='macro'))\n",
    "print(\"Micro score from averaging\", sklearn.metrics.roc_auc_score(truth.T, average_preds.T, average='micro'))\n",
    "\n",
    "j=0\n",
    "for i in range(0, 20): # eval on all assays except DNase and assays that are missing in A549\n",
    "    assay = inv_assaymap[i]\n",
    "\n",
    "    if (i not in valid_holdout_indices and i != 0):\n",
    "        print(i, assay, sklearn.metrics.roc_auc_score(truth[j].T, average_preds[j].T, average='macro'))\n",
    "        j = j + 1\n",
    "    else:\n",
    "        print(i, assay, np.NaN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self,\n",
    "                 train_data,\n",
    "                 valid_data,\n",
    "                 test_data,\n",
    "                 batch_size=64,\n",
    "                 shuffle_size=10000,\n",
    "                 prefetch_size=10,\n",
    "                 l1=0.,\n",
    "                 l2=0.,\n",
    "                 lr=1e-3,\n",
    "                 radii=[1,3]):\n",
    "        \n",
    "        self.graph = tf.Graph()\n",
    "\n",
    "        with self.graph.as_default() as graph:\n",
    "            tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "            output_shape, train_iter = make_dataset_dnase_combination(train_data,  Dataset.TRAIN, batch_size, shuffle_size, prefetch_size, radii)\n",
    "            _,            valid_iter = make_dataset_dnase_combination(valid_data, Dataset.VALID, batch_size, 1           , prefetch_size, radii)\n",
    "            _,            test_iter = make_dataset_dnase_combination(test_data, Dataset.TEST, batch_size, 1           , prefetch_size, radii)\n",
    "\n",
    "            ### Weston uncomment to run original method\n",
    "#             output_shape, train_iter = make_dataset(train_data,  Dataset.TRAIN, batch_size, shuffle_size, prefetch_size, radii)\n",
    "#             _,            valid_iter = make_dataset(valid_data, Dataset.VALID, batch_size, 1           , prefetch_size, radii)\n",
    "#             _,            test_iter = make_dataset(test_data, Dataset.TEST, batch_size, 1           , prefetch_size, radii)\n",
    "\n",
    "            self.train_handle = train_iter.string_handle()\n",
    "            self.valid_handle = valid_iter.string_handle()\n",
    "            self.test_handle = test_iter.string_handle()\n",
    "            \n",
    "            self.handle = tf.placeholder(tf.string, shape=[])\n",
    "            iterator = tf.data.Iterator.from_string_handle(\n",
    "                self.handle, train_iter.output_types, train_iter.output_shapes)\n",
    "            self.x, self.y = iterator.get_next()\n",
    "            \n",
    "            self.sess = tf.InteractiveSession(graph=graph)\n",
    "\n",
    "            self.num_outputs = output_shape[0]\n",
    "            self.l1, self.l2 = l1, l2\n",
    "            self.default_lr = lr\n",
    "            self.lr = tf.placeholder(tf.float32)\n",
    "            self.batch_size = batch_size\n",
    "            self.prefetch_size = prefetch_size\n",
    "\n",
    "            self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            self.logits = self.body_fn()\n",
    "            self.predictions = tf.sigmoid(self.logits)\n",
    "            self.loss = self.loss_fn()\n",
    "            self.min = self.minimizer_fn()\n",
    "            \n",
    "            self.closed = False\n",
    "                \n",
    "    def body_fn(self):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def loss_fn(self):\n",
    "        return tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(self.y, self.logits, 50))\n",
    "    \n",
    "    def minimizer_fn(self):\n",
    "        self.opt = tf.train.AdamOptimizer(self.lr)\n",
    "        return self.opt.minimize(self.loss, self.global_step)\n",
    "    \n",
    "    def close():\n",
    "        if not self.closed:\n",
    "            self.sess.close()\n",
    "        self.closed = True\n",
    "        \n",
    "    def train(self, num_steps, lr=None):\n",
    "        assert not self.closed\n",
    "        with self.graph.as_default():\n",
    "            if lr == None:\n",
    "                lr = self.default_lr\n",
    "            try:\n",
    "                self.sess.run(self.global_step)\n",
    "            except:\n",
    "                tf.logging.info(\"Initializing variables\")\n",
    "                self.sess.run(tf.global_variables_initializer())\n",
    "                self.train_handle = self.sess.run(self.train_handle)\n",
    "                self.valid_handle = self.sess.run(self.valid_handle)\n",
    "                self.test_handle = self.sess.run(self.test_handle)\n",
    "\n",
    "            max_steps = self.sess.run(self.global_step) + num_steps\n",
    "\n",
    "            tf.logging.info(\"Starting Training\")\n",
    "\n",
    "            while self.sess.run(self.global_step) < max_steps:\n",
    "                _, loss = self.sess.run([self.min, self.loss], {self.handle: self.train_handle, self.lr: lr})\n",
    "                step = self.sess.run(self.global_step)\n",
    "                if step % 1000 == 0:\n",
    "                    tf.logging.info(str(step) + \" \" + str(loss))\n",
    "                    tf.logging.info(\"On validation\")\n",
    "                    _, _, _, _, stop = self.test(40000, log=False)\n",
    "                    if stop: break\n",
    "                    tf.logging.info(\"\")\n",
    "                \n",
    "    def test(self, num_samples, mode = Dataset.VALID, log=False, iterator_handle=None):\n",
    "        \n",
    "        \n",
    "        if (mode == Dataset.VALID):\n",
    "            handle = self.valid_handle\n",
    "            validation_holdout_indices = np.array([]) \n",
    "        elif (mode == Dataset.TEST):\n",
    "            handle = self.test_handle\n",
    "            # indices of assays not available for A549, -1 because first DNAse col was removed\n",
    "            validation_holdout_indices = valid_holdout_indices - 1 \n",
    "        else:\n",
    "            raise\n",
    "            \n",
    "        assert not self.closed\n",
    "        with self.graph.as_default():\n",
    "            vals = []\n",
    "            for i in range(int(num_samples / self.batch_size)):\n",
    "                vals.append(\n",
    "                    self.sess.run([self.predictions, self.x, self.y],\n",
    "                             {self.handle: iterator_handle if iterator_handle else handle})\n",
    "                )\n",
    "            preds = np.concatenate([v[0] for v in vals])            \n",
    "            truth = np.concatenate([v[2] for v in vals])\n",
    "\n",
    "            # remove missing indices for computing macro/micro AUC\n",
    "            preds_r = np.delete(preds, validation_holdout_indices, axis=1)\n",
    "            truth_r = np.delete(truth, validation_holdout_indices, axis=1)\n",
    "            \n",
    "            macroAUC = sklearn.metrics.roc_auc_score(truth_r, preds_r, average='macro')\n",
    "            microAUC = sklearn.metrics.roc_auc_score(truth_r, preds_r, average='micro')\n",
    "            tf.logging.info(\"Our macro AUC:     \" + str(macroAUC))\n",
    "            tf.logging.info(\"Our micro AUC:     \" + str(microAUC))\n",
    "            if True: # if log:\n",
    "                j=0\n",
    "                \n",
    "                for i in range(matrix.shape[1]): # eval on all assays except DNase and assays that are missing in A549\n",
    "                    assay = inv_assaymap[i]\n",
    "\n",
    "                    if (i not in validation_holdout_indices+1 and i != 0):\n",
    "                        str_ = \"%s: %i, %s, %f\" % (str(datetime.datetime.now()), i, assay, sklearn.metrics.roc_auc_score(truth_r[:,j], preds_r[:,j], average='macro'))\n",
    "                        j = j + 1\n",
    "                    else:\n",
    "                        str_ = \"%s: %i, %s, NaN\" % (str(datetime.datetime.now()), i, assay)\n",
    "                        \n",
    "                    tf.logging.info(str_)\n",
    "\n",
    "                        \n",
    "                \n",
    "            return preds, truth, microAUC, macroAUC, False\n",
    "\n",
    "        \n",
    "    def evaluate(self, cell_type, mode = Dataset.VALID):\n",
    "        \"\"\" \n",
    "        Evaluate on a new dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        num_samples = len(dnase_vector)\n",
    "        \n",
    "        # take dnase_vector, and calculate a new handle that has all the train\n",
    "        # cell types and the corresponding distance\n",
    "        eval_iter = function_x(cell_type, mode)\n",
    "        handle = eval_iter.string_handle()\n",
    "        \n",
    "        assert not self.closed\n",
    "        with self.graph.as_default(): \n",
    "            vals = []\n",
    "            for i in range(int(num_samples / self.batch_size)):\n",
    "                vals.append(\n",
    "                    self.sess.run([self.predictions, self.x, self.y],\n",
    "                             {self.handle: iterator_handle if iterator_handle else handle})\n",
    "                )\n",
    "            preds = np.concatenate([v[0] for v in vals])            \n",
    "            truth = np.concatenate([v[2] for v in vals])\n",
    "\n",
    "            # remove missing indices for computing macro/micro AUC\n",
    "            preds_r = np.delete(preds, validation_holdout_indices, axis=1)\n",
    "            truth_r = np.delete(truth, validation_holdout_indices, axis=1)\n",
    "            \n",
    "            macroAUC = sklearn.metrics.roc_auc_score(truth_r, preds_r, average='macro')\n",
    "            microAUC = sklearn.metrics.roc_auc_score(truth_r, preds_r, average='micro')\n",
    "            tf.logging.info(\"Our macro AUC:     \" + str(macroAUC))\n",
    "            tf.logging.info(\"Our micro AUC:     \" + str(microAUC))\n",
    "            if True: # if log:\n",
    "                j=0\n",
    "                \n",
    "                for i in range(matrix.shape[1]): # eval on all assays except DNase and assays that are missing in A549\n",
    "                    assay = inv_assaymap[i]\n",
    "\n",
    "                    if (i not in validation_holdout_indices+1 and i != 0):\n",
    "                        str_ = \"%s: %i, %s, %f\" % (str(datetime.datetime.now()), i, assay, sklearn.metrics.roc_auc_score(truth_r[:,j], preds_r[:,j], average='macro'))\n",
    "                        j = j + 1\n",
    "                    else:\n",
    "                        str_ = \"%s: %i, %s, NaN\" % (str(datetime.datetime.now()), i, assay)\n",
    "                        \n",
    "                    tf.logging.info(str_)\n",
    "\n",
    "                        \n",
    "                \n",
    "            return preds, truth, microAUC, macroAUC, False\n",
    "\n",
    "        \n",
    "        \n",
    "class MLP(Model):\n",
    "    def __init__(self,\n",
    "             layers,\n",
    "             num_units,\n",
    "             activation,\n",
    "             *args,\n",
    "             **kwargs):\n",
    "\n",
    "        self.layers = layers\n",
    "        self.num_units = num_units\n",
    "        self.activation = activation\n",
    "        \n",
    "        Model.__init__(self, *args, **kwargs)\n",
    "            \n",
    "    def body_fn(self):\n",
    "        model = self.x\n",
    "        \n",
    "        if not isinstance(self.num_units, collections.Iterable):\n",
    "            self.num_units = [self.num_units] * self.layers\n",
    "            \n",
    "        for i in range(self.layers):\n",
    "            model = tf.layers.dense(model, self.num_units[i], self.activation)#, kernel_regularizer=tf.contrib.layers.l1_l2_regularizer(self.l1, self.l2))\n",
    "        \n",
    "        return tf.layers.dense(model, self.num_outputs, kernel_regularizer=tf.contrib.layers.l1_l2_regularizer(self.l1, self.l2))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run with DNase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "model = MLP(4, [100, 100, 100, 50], tf.tanh, train_data, valid_data, test_data, shuffle_size=2, radii=[1,3,10,30])\n",
    "model.train(20000)\n",
    "test_results = model.test(455024, mode = Dataset.TEST, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting Training\n",
      "INFO:tensorflow:21000 0.624256\n",
      "INFO:tensorflow:On validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/akmorrow/miniconda3/envs/EpitomeEnv/lib/python3.6/site-packages/ipykernel_launcher.py:122: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n",
      "/data/akmorrow/miniconda3/envs/EpitomeEnv/lib/python3.6/site-packages/ipykernel_launcher.py:123: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Our macro AUC:     0.9088915708580808\n",
      "INFO:tensorflow:Our micro AUC:     0.938567234712428\n",
      "INFO:tensorflow:2018-11-15 12:12:36.333658: 0, DNase, NaN\n",
      "INFO:tensorflow:2018-11-15 12:12:36.334646: 1, CTCF, 0.967939\n",
      "INFO:tensorflow:2018-11-15 12:12:36.349565: 2, Pol2, 0.949957\n",
      "INFO:tensorflow:2018-11-15 12:12:36.363388: 3, YY1, 0.942291\n",
      "INFO:tensorflow:2018-11-15 12:12:36.377434: 4, p300, 0.909964\n",
      "INFO:tensorflow:2018-11-15 12:12:36.391348: 5, TAF1, 0.976360\n",
      "INFO:tensorflow:2018-11-15 12:12:36.405034: 6, Pol2-4H8, 0.913674\n",
      "INFO:tensorflow:2018-11-15 12:12:36.418778: 7, c-Myc, 0.978126\n",
      "INFO:tensorflow:2018-11-15 12:12:36.433101: 8, Rad21, 0.993166\n",
      "INFO:tensorflow:2018-11-15 12:12:36.446987: 9, Max, 0.926872\n",
      "INFO:tensorflow:2018-11-15 12:12:36.460978: 10, NRSF, 0.931903\n",
      "INFO:tensorflow:2018-11-15 12:12:36.474759: 11, GABP, 0.940306\n",
      "INFO:tensorflow:2018-11-15 12:12:36.488605: 12, EZH2, 0.605848\n",
      "INFO:tensorflow:2018-11-15 12:12:36.502198: 13, CEBPB, 0.798913\n",
      "INFO:tensorflow:2018-11-15 12:12:36.516057: 14, c-Jun, 0.933734\n",
      "INFO:tensorflow:2018-11-15 12:12:36.529666: 15, ZBTB33, 0.826225\n",
      "INFO:tensorflow:2018-11-15 12:12:36.543427: 16, USF2, 0.952632\n",
      "INFO:tensorflow:2018-11-15 12:12:36.556925: 17, USF-1, 0.834219\n",
      "INFO:tensorflow:2018-11-15 12:12:36.570835: 18, TBP, 0.932043\n",
      "INFO:tensorflow:2018-11-15 12:12:36.585375: 19, RFX5, 0.954768\n",
      "INFO:tensorflow:\n",
      "INFO:tensorflow:22000 0.48795164\n",
      "INFO:tensorflow:On validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/akmorrow/miniconda3/envs/EpitomeEnv/lib/python3.6/site-packages/ipykernel_launcher.py:122: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n",
      "/data/akmorrow/miniconda3/envs/EpitomeEnv/lib/python3.6/site-packages/ipykernel_launcher.py:123: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Our macro AUC:     0.8916495950149012\n",
      "INFO:tensorflow:Our micro AUC:     0.9169341956192695\n",
      "INFO:tensorflow:2018-11-15 12:16:32.313454: 0, DNase, NaN\n",
      "INFO:tensorflow:2018-11-15 12:16:32.314481: 1, CTCF, 0.960718\n",
      "INFO:tensorflow:2018-11-15 12:16:32.328898: 2, Pol2, 0.912531\n",
      "INFO:tensorflow:2018-11-15 12:16:32.343079: 3, YY1, 0.939502\n",
      "INFO:tensorflow:2018-11-15 12:16:32.356731: 4, p300, 0.891198\n",
      "INFO:tensorflow:2018-11-15 12:16:32.370709: 5, TAF1, 0.959251\n",
      "INFO:tensorflow:2018-11-15 12:16:32.384828: 6, Pol2-4H8, 0.885690\n",
      "INFO:tensorflow:2018-11-15 12:16:32.398782: 7, c-Myc, 0.962050\n",
      "INFO:tensorflow:2018-11-15 12:16:32.412597: 8, Rad21, 0.980714\n",
      "INFO:tensorflow:2018-11-15 12:16:32.426431: 9, Max, 0.919313\n",
      "INFO:tensorflow:2018-11-15 12:16:32.440337: 10, NRSF, 0.927840\n",
      "INFO:tensorflow:2018-11-15 12:16:32.454227: 11, GABP, 0.924454\n",
      "INFO:tensorflow:2018-11-15 12:16:32.468198: 12, EZH2, 0.580354\n",
      "INFO:tensorflow:2018-11-15 12:16:32.481859: 13, CEBPB, 0.703528\n",
      "INFO:tensorflow:2018-11-15 12:16:32.495937: 14, c-Jun, 0.899272\n",
      "INFO:tensorflow:2018-11-15 12:16:32.509712: 15, ZBTB33, 0.910791\n",
      "INFO:tensorflow:2018-11-15 12:16:32.523517: 16, USF2, 0.886148\n",
      "INFO:tensorflow:2018-11-15 12:16:32.537376: 17, USF-1, 0.839703\n",
      "INFO:tensorflow:2018-11-15 12:16:32.551354: 18, TBP, 0.922529\n",
      "INFO:tensorflow:2018-11-15 12:16:32.565396: 19, RFX5, 0.935756\n",
      "INFO:tensorflow:\n",
      "INFO:tensorflow:23000 0.08645469\n",
      "INFO:tensorflow:On validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/akmorrow/miniconda3/envs/EpitomeEnv/lib/python3.6/site-packages/ipykernel_launcher.py:122: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n",
      "/data/akmorrow/miniconda3/envs/EpitomeEnv/lib/python3.6/site-packages/ipykernel_launcher.py:123: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Our macro AUC:     0.9181114018285776\n",
      "INFO:tensorflow:Our micro AUC:     0.9411264022734844\n",
      "INFO:tensorflow:2018-11-15 12:19:25.224150: 0, DNase, NaN\n",
      "INFO:tensorflow:2018-11-15 12:19:25.225066: 1, CTCF, 0.972497\n",
      "INFO:tensorflow:2018-11-15 12:19:25.239389: 2, Pol2, 0.890414\n",
      "INFO:tensorflow:2018-11-15 12:19:25.253155: 3, YY1, 0.954640\n",
      "INFO:tensorflow:2018-11-15 12:19:25.266904: 4, p300, 0.909225\n",
      "INFO:tensorflow:2018-11-15 12:19:25.280498: 5, TAF1, 0.971962\n",
      "INFO:tensorflow:2018-11-15 12:19:25.294459: 6, Pol2-4H8, 0.905185\n",
      "INFO:tensorflow:2018-11-15 12:19:25.308379: 7, c-Myc, 0.978708\n",
      "INFO:tensorflow:2018-11-15 12:19:25.322141: 8, Rad21, 0.984554\n",
      "INFO:tensorflow:2018-11-15 12:19:25.335917: 9, Max, 0.939017\n",
      "INFO:tensorflow:2018-11-15 12:19:25.349826: 10, NRSF, 0.956805\n",
      "INFO:tensorflow:2018-11-15 12:19:25.363856: 11, GABP, 0.959692\n",
      "INFO:tensorflow:2018-11-15 12:19:25.377840: 12, EZH2, 0.791051\n",
      "INFO:tensorflow:2018-11-15 12:19:25.391709: 13, CEBPB, 0.785691\n",
      "INFO:tensorflow:2018-11-15 12:19:25.405806: 14, c-Jun, 0.849960\n",
      "INFO:tensorflow:2018-11-15 12:19:25.419612: 15, ZBTB33, 0.936812\n",
      "INFO:tensorflow:2018-11-15 12:19:25.433365: 16, USF2, 0.894398\n",
      "INFO:tensorflow:2018-11-15 12:19:25.447024: 17, USF-1, 0.871242\n",
      "INFO:tensorflow:2018-11-15 12:19:25.461100: 18, TBP, 0.949380\n",
      "INFO:tensorflow:2018-11-15 12:19:25.474974: 19, RFX5, 0.942882\n",
      "INFO:tensorflow:\n"
     ]
    }
   ],
   "source": [
    "model.train(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Our macro AUC:     0.9191361344017676\n",
      "INFO:tensorflow:Our micro AUC:     0.9173131175923805\n",
      "INFO:tensorflow:2018-11-15 13:11:42.082294: 0, DNase, NaN\n",
      "INFO:tensorflow:2018-11-15 13:11:42.083695: 1, CTCF, 0.943762\n",
      "INFO:tensorflow:2018-11-15 13:11:42.434775: 2, Pol2, 0.961963\n",
      "INFO:tensorflow:2018-11-15 13:11:42.778216: 3, YY1, 0.920675\n",
      "INFO:tensorflow:2018-11-15 13:11:43.122367: 4, p300, 0.892504\n",
      "INFO:tensorflow:2018-11-15 13:11:43.475008: 5, TAF1, 0.907665\n",
      "INFO:tensorflow:2018-11-15 13:11:43.831148: 6, Pol2-4H8, NaN\n",
      "INFO:tensorflow:2018-11-15 13:11:43.832511: 7, c-Myc, NaN\n",
      "INFO:tensorflow:2018-11-15 13:11:43.833515: 8, Rad21, 0.992517\n",
      "INFO:tensorflow:2018-11-15 13:11:44.183052: 9, Max, 0.957339\n",
      "INFO:tensorflow:2018-11-15 13:11:44.527200: 10, NRSF, 0.901812\n",
      "INFO:tensorflow:2018-11-15 13:11:44.895121: 11, GABP, 0.925001\n",
      "INFO:tensorflow:2018-11-15 13:11:45.239097: 12, EZH2, NaN\n",
      "INFO:tensorflow:2018-11-15 13:11:45.240378: 13, CEBPB, 0.795382\n",
      "INFO:tensorflow:2018-11-15 13:11:45.583084: 14, c-Jun, NaN\n",
      "INFO:tensorflow:2018-11-15 13:11:45.584414: 15, ZBTB33, 0.901711\n",
      "INFO:tensorflow:2018-11-15 13:11:45.924524: 16, USF2, NaN\n",
      "INFO:tensorflow:2018-11-15 13:11:45.925478: 17, USF-1, 0.929303\n",
      "INFO:tensorflow:2018-11-15 13:11:46.274730: 18, TBP, NaN\n",
      "INFO:tensorflow:2018-11-15 13:11:46.275920: 19, RFX5, NaN\n"
     ]
    }
   ],
   "source": [
    "test_results = model.test(455024, mode = Dataset.TEST, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20k steps, take maximum macroAUC observed during training\n",
    "#2, [100, 50], [0], 0.9012855730483059\n",
    "#2, [100, 50], [1], 0.9185077245227484\n",
    "#2, [100, 50], [1,3], 0.9207317913067318\n",
    "#2, [100, 50], [1,3,10], 0.9206390991622936\n",
    "#2, [100, 50], [1,3,10,30], 0.9241529096459893\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
