{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATE DEFCOM AND EPITOME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook showcases all the defcom and epitome evaluation functions. It assumes you have installed defcom as specified by the evaluation/defcom/README.md file. Additionally, it assumes you have run the evaluation/defcom/get_defcom_data.py script. Some things in this notebook are subject to change based on a variety of things, such as where you downloaded your defcom data to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Paths\n",
    "## TODO: set path to where you generated data: 'get_defcom_data.py [-h] --output_path OUTPUT_PATH'\n",
    "defcom_data_path = \"/data/akmorrow/defcom_data/\"\n",
    "config_path = '../../config.yml' # epitome configuration path\n",
    "deepsea_labels_path = '../../data/deepsea_data/deepsea_labels_train/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do all the neccessary imports\n",
    "from constants import *\n",
    "from defcom_functions import *\n",
    "from visualization import *\n",
    "import pandas as pd\n",
    "import os\n",
    "# let's import the rest of the epitome library\n",
    "import sys\n",
    "sys.path.insert(0,'../../')\n",
    "\n",
    "# from epitome.constants import *\n",
    "# from epitome.models import *\n",
    "# from epitome.generators import *\n",
    "# from epitome.functions import *\n",
    "# import yaml\n",
    "#from epitome.viz import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create User Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make required directories\n",
    "\n",
    "defcom_config_dir = os.path.join(defcom_data_path, \"defcom_configs\")\n",
    "defcom_results_dir = os.path.join(defcom_data_path, \"defcom_results\")\n",
    "defcom_models_dir = os.path.join(defcom_data_path, \"defcom_models\")\n",
    "\n",
    "if not os.path.exists(defcom_config_dir):\n",
    "    os.mkdir(defcom_config_dir)\n",
    "if not os.path.exists(defcom_results_dir):\n",
    "    os.mkdir(defcom_results_dir)\n",
    "if not os.path.exists(defcom_models_dir):\n",
    "    os.mkdir(defcom_models_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Defcom model on defcom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "bam_files = {\n",
    "    'k562':    os.path.join(defcom_data_path, 'wgEncodeOpenChromDnaseK562AlnRep1.bam'),\n",
    "    'gm12878': os.path.join(defcom_data_path, 'wgEncodeOpenChromDnaseGm12878AlnRep2.bam'), \n",
    "    'hepg2':   os.path.join(defcom_data_path, 'wgEncodeOpenChromDnaseHepg2AlnRep3.bam'),\n",
    "    'h1hesc':  os.path.join(defcom_data_path, 'wgEncodeOpenChromDnaseH1hescAlnRep1.bam')\n",
    "            }\n",
    "\n",
    "query_cell_type = 'k562' # garbage placeholder for when we initially create the config\n",
    "\n",
    "for train_cell_type in DEFCOM_CELLS:\n",
    "    for tf in DEFCOM_TFS:\n",
    "        \n",
    "        if(tf == 'sp1' and train_cell_type == 'k562'): # defcom is missing data for K562 for Sp1\n",
    "            continue\n",
    "        \n",
    "        active_sites_file =    os.path.join(defcom_data_path, tf + '_' + train_cell_type + '_pos_train.bed')\n",
    "        inactive_sites_file =   os.path.join(defcom_data_path, tf + '_' + train_cell_type + '_neg_train.bed')\n",
    "        candidate_sites_file =  os.path.join(defcom_data_path, tf + '_' + query_cell_type + '_all_valid.bed')\n",
    "        training_bam_file =    bam_files[train_cell_type]\n",
    "        candidate_bam_file =   bam_files[query_cell_type]\n",
    "        model_out   =  os.path.join(defcom_data_path, 'defcom_models')\n",
    "        results_out = os.path.join(defcom_data_path, 'defcom_results')\n",
    "        config_out  = os.path.join(defcom_data_path, 'defcom_configs')\n",
    "        \n",
    "        # generate a config file\n",
    "        config_file, prediction_results_file = createConfigFile(train_cell_type,\n",
    "                                                        tf,\n",
    "                                                        query_cell_type, \n",
    "                                                        active_sites_file, \n",
    "                                                        inactive_sites_file, \n",
    "                                                        candidate_sites_file, \n",
    "                                                        training_bam_file, \n",
    "                                                        candidate_bam_file,\n",
    "                                                        model_out,\n",
    "                                                        results_out,\n",
    "                                                        config_out)\n",
    "\n",
    "        print('Training {} {}'.format(train_cell_type, tf))\n",
    "        train_defcom(config_file)\n",
    "        \n",
    "        for query_cell_type in DEFCOM_CELLS:\n",
    "            \n",
    "            if(tf == 'sp1' and query_cell_type == 'k562'): # defcom is missing data for K562 for Sp1\n",
    "                continue\n",
    "            \n",
    "            candidate_sites_file = os.path.join(defcom_data_path, tf + '_' + query_cell_type + '_all_valid.bed')\n",
    "            candidate_bam_file =   bam_files[query_cell_type]\n",
    "            # generate a config file\n",
    "            config_file, prediction_results_file = createConfigFile(train_cell_type,\n",
    "                                                            tf,\n",
    "                                                            query_cell_type, \n",
    "                                                            active_sites_file, \n",
    "                                                            inactive_sites_file, \n",
    "                                                            candidate_sites_file, \n",
    "                                                            training_bam_file, \n",
    "                                                            candidate_bam_file,\n",
    "                                                            model_out,\n",
    "                                                            results_out,\n",
    "                                                            config_out)\n",
    "            \n",
    "\n",
    "            print('Predicting {} {} '.format(query_cell_type, tf))\n",
    "            predict_defcom(config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load in Epitome Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# load in user paths\n",
    "with open(config_path) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "train_data, valid_data, test_data = load_deepsea_label_data(deepsea_labels_path)\n",
    "dataset_split = {Dataset.TRAIN: train_data, Dataset.VALID: valid_data, Dataset.TEST: test_data}\n",
    "#dataset_names = {Dataset.TRAIN: \"train\", # i could probalby put this in the constants file\n",
    "#                Dataset.VALID: 'cross_validation',\n",
    "#                Dataset.TEST: 'test'}\n",
    "\n",
    "# these are the old indices train_indices = [(0, 2159308), (2159309, 2169309), (2169309, 2536878)]\n",
    "indices = [(0, 2169308), (2169309, 2309366), (2309367, 2608182)]\n",
    "# train start, train end, cv end, test end\n",
    "# (start, chrom_6 - 10,000), (chrom_6 - 10,000, rest of chrom_6), (chrom 7, 8, 9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Evaluate Epitome model on defcom data\n",
    "#\n",
    "\n",
    "# we also need to iterate through the epitome and defcom cells\n",
    "for tf_EPITOME_NAME, tf_DEFCOM_NAME in EPITOME_AND_DEFCOM_TFS:\n",
    "    \n",
    "    epitome_tf = [tf_EPITOME_NAME]\n",
    "    matrix, cellmap, assaymap = get_assays_from_feature_file( \n",
    "                                    eligible_assays = epitome_tf,\n",
    "                                    eligible_cells = EPITOME_CELLS, \n",
    "                                    min_cells_per_assay = 2, \n",
    "                                    min_assays_per_cell= 2)\n",
    "\n",
    "    for query_cell_EPITOME_NAME, query_cell_DEFCOM_NAME in EPITOME_AND_DEFCOM_CELLS:\n",
    "\n",
    "        epitome_model = MLP(data = dataset_split, # I'm assuming this is data\n",
    "                    test_celltypes = [query_cell_EPITOME_NAME], # cell line reserved for testing\n",
    "                    matrix = matrix,\n",
    "                    assaymap = assaymap,\n",
    "                    cellmap = cellmap,\n",
    "                    shuffle_size=2,\n",
    "                    prefetch_size = 64,\n",
    "                    debug = False, \n",
    "                    batch_size = 64, \n",
    "                    radii=[1,3,10,30],\n",
    "                    split_indices = indices\n",
    "                    )\n",
    "\n",
    "        print('Training {} | {}'.format('joint', tf_EPITOME_NAME))\n",
    "        epitome_model.train(100)\n",
    "\n",
    "        print('Predicting {} | {}'.format(query_cell_EPITOME_NAME, tf_EPITOME_NAME))\n",
    "        # path to DeFCoM bed file:\n",
    "        peak_file = os.path.join(defcom_data_path,'{}_{}_all_valid.bed'.format(tf_DEFCOM_NAME, query_cell_DEFCOM_NAME))\n",
    "\n",
    "        peak_result = epitome_model.score_peak_file(peak_file) # score the peak file\n",
    "        peak_result.to_csv(os.path.join(defcom_results_dir,'epitome_single_tf_model_preds/{}_{}_{}.csv'.format('joint', tf_DEFCOM_NAME, query_cell_DEFCOM_NAME)), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Evaluate the Defcom model in this cell\n",
    "#\n",
    "\n",
    "# this code assumes you already have made a folder named defcom results in data\n",
    "# it also assumes you outputted your defcom data to data/defcom_data, please change things as appropriate\n",
    "\n",
    "# let's create a dataframe with the evaluation results. It will tell us the auROC and auPR for a given evaluation\n",
    "eval_results_df = pd.DataFrame(columns=['model','training_cell','transcription_factor','query_cell','auROC','auPR'])\n",
    "\n",
    "# for each cell type\n",
    "for train_cell in DEFCOM_CELLS:\n",
    "    # for each transcription factor\n",
    "    for tf in DEFCOM_TFS:        \n",
    "        for query_cell in DEFCOM_CELLS:\n",
    "\n",
    "            # we are missing data for sp1 in K562...\n",
    "            if((tf == 'sp1' and query_cell == 'k562') or (tf == 'sp1' and query_cell == 'k562')):\n",
    "                continue\n",
    "                \n",
    "            # get names of files we'll need\n",
    "            prediction_results_file = os.path.join(defcom_data_path, 'defcom_results/{}_{}_{}_results.bed'.format(train_cell, tf, query_cell))\n",
    "            pos_file = os.path.join(defcom_data_path, '{}_{}_pos_valid.bed'.format(tf, query_cell))\n",
    "            # evaluate the model\n",
    "            auROC, auPRC = evaluateDefcomResults(tf, prediction_results_file, pos_file)\n",
    "            # append to the rest of the results\n",
    "            eval_results_df = eval_results_df.append({'model':'defcom_{}'.format(train_cell),\n",
    "                                'training_cell': train_cell, \n",
    "                                'transcription_factor': tf,\n",
    "                                'query_cell': query_cell,\n",
    "                                'auROC': auROC, \n",
    "                                'auPR': auPRC}, ignore_index=True)\n",
    "                                \n",
    "                                \n",
    "            \n",
    "\n",
    "print(eval_results_df)\n",
    "\n",
    "eval_results_df.to_csv(os.path.join(defcom_results_dir, \"eval_results\",\"defcom_functions_test_results.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results_df = pd.read_csv(os.path.join(defcom_results_dir, \"eval_results\", \"defcom_functions_test_results.csv\"))\n",
    "    \n",
    "# for all the query cell types\n",
    "for query_cell_epitome_name, query_cell in EPITOME_AND_DEFCOM_CELLS:\n",
    "    # for all the transcription factors\n",
    "    for tf_epitome_name, tf in EPITOME_AND_DEFCOM_TFS:\n",
    "        # get the files we'll need\n",
    "        prediction_results_file = '../../results/epitome_single_tf_model_preds/{}_{}_{}.csv'.format('joint', tf, query_cell)\n",
    "        pos_file = '../../data/defcom_data/{}_{}_pos_valid.bed'.format(tf, query_cell)\n",
    "        # try just incase we are missing data for one\n",
    "        try:\n",
    "            # evaluate the model\n",
    "            auROC, auPR = evaluateEpitomeResults(tf_epitome_name, prediction_results_file, pos_file)\n",
    "            # append the results\n",
    "            eval_results_df = eval_results_df.append({'model':'epitome',\n",
    "                        'training_cell': 'joint', \n",
    "                        'transcription_factor': tf,\n",
    "                        'query_cell': query_cell,\n",
    "                        'auROC': auROC, \n",
    "                        'auPR': auPR}, ignore_index=True)\n",
    "        # if something goes wrong we should know about it\n",
    "        except Exception as e:\n",
    "            print('Unable to score predictions for TF: {} and query cell type {}. Exception: {}'.format(tf, query_cell, e))\n",
    "\n",
    "print(eval_results_df)\n",
    "eval_results_df.to_csv('../../results/eval_results/defcom_functions_test_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to start: let's go ahead and load in the data\n",
    "defcom_eval_results = '../../results/eval_results/defcom_functions_test_results.csv'\n",
    "\n",
    "results = pd.read_csv(defcom_eval_results, index_col=False)\n",
    "results = results.loc[ (results['transcription_factor'] != 'Ep300') & (results['transcription_factor'] != 'Gabpa')]\n",
    "# we're going to generate one boxplot with 16 boxes\n",
    "#results['model'] = results['model'] + '_' + results['training_cell']\n",
    "#results.loc[results['model'].str.contains('epitome'), 'model'] = 'epitome'\n",
    "#results.loc[results['model'].str.contains('epitome'), 'training_cell'] = 'joint'\n",
    "\n",
    "save_plot_path = '../../results/plots/defcom_functions_test'\n",
    "generateScatterPlots(results, 'auROC', save_plot_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# to start: let's go ahead and load in the data\n",
    "defcom_eval_results = 'results/eval_results/defcom_functions_test_results.csv'\n",
    "\n",
    "results = pd.read_csv(defcom_eval_results, index_col=False)\n",
    "results = results.loc[ (results['transcription_factor'] != 'Ep300') & (results['transcription_factor'] != 'Gabpa')]\n",
    "# we're going to generate one boxplot with 16 boxes\n",
    "#results['model'] = results['model'] + '_' + results['training_cell']\n",
    "#results.loc[results['model'].str.contains('epitome'), 'model'] = 'epitome'\n",
    "#results.loc[results['model'].str.contains('epitome'), 'training_cell'] = 'joint'\n",
    "\n",
    "save_plot_path = 'results/plots/defcom_functions_test/comparative_boxplot_auROC.png'\n",
    "generateComparativeBoxplot(results, 'auROC', 'results/plots/defcom_functions_test/comparative_boxplot_auROC.png')\n",
    "generateComparativeBoxplot(results, 'auPR', 'results/plots/defcom_functions_test/comparative_boxplot_auPR.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DefcomEnv",
   "language": "python",
   "name": "defcomenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
