{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sess.close()\n",
    "except:\n",
    "    pass\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = {\n",
    "    \"deepsea_outputs\": np.load(\"train_outputs.npy\"),\n",
    "    \"deepsea_logits\": np.load(\"train_logits.npy\"),\n",
    "    \"y\": np.load(\"train_targets.npy\")\n",
    "}\n",
    "\n",
    "valid_data = {\n",
    "    \"deepsea_outputs\": np.load(\"valid_outputs.npy\"),\n",
    "    \"deepsea_logits\": np.load(\"valid_logits.npy\"),\n",
    "    \"y\": np.load(\"valid_targets.npy\")\n",
    "}\n",
    "\n",
    "test_data = {\n",
    "    \"deepsea_outputs\": np.load(\"test_outputs.npy\"),\n",
    "    \"deepsea_logits\": np.load(\"test_logits.npy\"),\n",
    "    \"y\": np.load(\"test_targets.npy\")\n",
    "}\n",
    "\n",
    "# The validation set isn't big enough, so we take some from the training data.\n",
    "\n",
    "valid_data = {\n",
    "    \"deepsea_outputs\": np.concatenate([train_data[\"deepsea_outputs\"][2200000:2400000],train_data[\"deepsea_outputs\"][4200000:4400000],valid_data[\"deepsea_outputs\"]], axis=0),\n",
    "    \"deepsea_logits\": np.concatenate([train_data[\"deepsea_logits\"][2200000:2400000],train_data[\"deepsea_logits\"][4200000:4400000],valid_data[\"deepsea_logits\"]], axis=0),\n",
    "    \"y\": np.concatenate([train_data[\"y\"][2200000:2400000],train_data[\"y\"][4200000:4400000],valid_data[\"y\"]], axis=0),\n",
    "}\n",
    "\n",
    "train_data = {\n",
    "    \"deepsea_outputs\": np.concatenate([train_data[\"deepsea_outputs\"][0:2200000],train_data[\"deepsea_outputs\"][2400000:4200000]], axis=0),\n",
    "    \"deepsea_logits\": np.concatenate([train_data[\"deepsea_logits\"][0:2200000],train_data[\"deepsea_logits\"][2400000:4200000]], axis=0),\n",
    "    \"y\": np.concatenate([train_data[\"y\"][0:2200000],train_data[\"y\"][2400000:4200000]], axis=0),\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(data,\n",
    "                 batch_size,\n",
    "                 shuffle_size,\n",
    "                 prefetch_size,\n",
    "                 generator_fn):\n",
    "    \n",
    "    x_shape, gen = generator_fn(data)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        output_types=(tf.float32,)*3,\n",
    "        output_shapes=(data[\"y\"].shape[1], x_shape, data[\"y\"].shape[1],)\n",
    "    )\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.shuffle(shuffle_size)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.prefetch(prefetch_size)\n",
    "    return dataset.make_one_shot_iterator()\n",
    "\n",
    "# Generators output (DeepSea's probs, model inputs, ground truth)\n",
    "\n",
    "def identity_gen(data):\n",
    "    def g():\n",
    "        for probs, logits, y in zip(data[\"deepsea_outputs\"], data[\"deepsea_logits\"], data[\"y\"]):\n",
    "            yield probs, logits, y\n",
    "    return data[\"deepsea_logits\"].shape[1], g\n",
    "\n",
    "def random_mask(p, shape):\n",
    "    return lambda: np.random.binomial(1, p, shape)\n",
    "\n",
    "def dnase_mask():\n",
    "    return np.array([1] + 18 * [0])\n",
    "\n",
    "def position_mask(ids):\n",
    "    out = np.zeros(19)\n",
    "    out[ids] = 1\n",
    "    return lambda: out\n",
    "\n",
    "def imputation_gen(data, mask_fn=random_mask(.1, 19), x_shape=19*3):\n",
    "    def g():\n",
    "        for probs, logits, y in zip(data[\"deepsea_outputs\"], data[\"deepsea_logits\"], data[\"y\"]):\n",
    "            mask = mask_fn()\n",
    "            x = y * mask + probs * (1 - mask)\n",
    "            x = np.concatenate([logits, x, mask], axis=0)\n",
    "            yield probs, x, y\n",
    "    return x_shape, g\n",
    "            \n",
    "def paramaterized_imputation_gen(mask_fn, x_shape):\n",
    "    return lambda data: imputation_gen(data, mask_fn, x_shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self,\n",
    "                 train_data,\n",
    "                 valid_data,\n",
    "                 num_outputs,\n",
    "                 batch_size=64,\n",
    "                 shuffle_size=10000,\n",
    "                 prefetch_size=10,\n",
    "                 l1=0.,\n",
    "                 l2=0.,\n",
    "                 lr=1e-3,\n",
    "                 generator_fn=identity_gen,\n",
    "                 masking=True):\n",
    "        \n",
    "        tf.logging.set_verbosity(tf.logging.INFO)\n",
    "        \n",
    "        train_iter = make_dataset(train_data, batch_size, shuffle_size, prefetch_size, generator_fn)\n",
    "        valid_iter = make_dataset(valid_data, batch_size, 1           , prefetch_size, generator_fn)\n",
    "        self.train_handle = train_iter.string_handle()\n",
    "        self.valid_handle = valid_iter.string_handle()\n",
    "        self.handle = tf.placeholder(tf.string, shape=[])\n",
    "        iterator = tf.data.Iterator.from_string_handle(\n",
    "            self.handle, train_iter.output_types, train_iter.output_shapes)\n",
    "        self.deepsea_pred, self.x, self.y = iterator.get_next()\n",
    "        \n",
    "        self.num_outputs = num_outputs\n",
    "        self.l1, self.l2 = l1, l2\n",
    "        self.default_lr = lr\n",
    "        self.lr = tf.placeholder(tf.float32)\n",
    "        self.batch_size = batch_size\n",
    "        self.prefetch_size = prefetch_size\n",
    "        self.masking = masking\n",
    "        \n",
    "        self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        self.logits = self.body_fn()\n",
    "        self.predictions = tf.sigmoid(self.logits)\n",
    "        self.loss = self.loss_fn()\n",
    "        self.min = self.minimizer_fn()\n",
    "        \n",
    "    def make_alt_dataset(self, sess, data, batch_size, shuffle_size, prefetch_size, generator_fn):\n",
    "        h = make_dataset(data, batch_size, shuffle_size, prefetch_size, generator_fn).string_handle()\n",
    "        return sess.run(h)\n",
    "                \n",
    "    def body_fn(self):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def loss_fn(self):\n",
    "        return tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(self.y, self.logits, 50))\n",
    "    \n",
    "    def minimizer_fn(self):\n",
    "        self.opt = tf.train.AdamOptimizer(self.lr)\n",
    "        return self.opt.minimize(self.loss, self.global_step)\n",
    "        \n",
    "    def train(self, sess, num_steps, lr=None):\n",
    "        if lr == None:\n",
    "            lr = self.default_lr\n",
    "        try:\n",
    "            sess.run(self.global_step)\n",
    "        except:\n",
    "            tf.logging.info(\"Initializing variables\")\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            self.train_handle = sess.run(self.train_handle)\n",
    "            self.valid_handle = sess.run(self.valid_handle)\n",
    "\n",
    "        max_steps = sess.run(self.global_step) + num_steps\n",
    "\n",
    "        tf.logging.info(\"Starting Training\")\n",
    "\n",
    "        while sess.run(self.global_step) < max_steps:\n",
    "            _, loss = sess.run([self.min, self.loss], {self.handle: self.train_handle, self.lr: lr})\n",
    "            step = sess.run(self.global_step)\n",
    "            if step % 1000 == 0:\n",
    "                tf.logging.info(str(step) + \" \" + str(loss))\n",
    "                tf.logging.info(\"On validation\")\n",
    "                _, _, _, stop = self.test(sess, 8000, log=True, masking=self.masking)\n",
    "                if stop: break\n",
    "                tf.logging.info(\"\")\n",
    "                \n",
    "    def test(self, sess, num_samples, log=False, iterator_handle=None, masking=True):\n",
    "        vals = []\n",
    "        for i in range(int(num_samples / self.batch_size)):\n",
    "            vals.append(\n",
    "                sess.run([self.deepsea_pred, self.predictions, self.x, self.y],\n",
    "                         {self.handle: iterator_handle if iterator_handle else self.valid_handle})\n",
    "            )\n",
    "        deepsea = np.concatenate([v[0] for v in vals])\n",
    "        preds = np.concatenate([v[1] for v in vals])\n",
    "        truth = np.concatenate([v[3] for v in vals])\n",
    "        if log and masking:\n",
    "            mask = 1 - np.concatenate([v[2][:,19*2:] for v in vals]).reshape((-1))\n",
    "            \n",
    "            our_score = sklearn.metrics.roc_auc_score(truth.reshape((-1)), preds.reshape((-1)),   sample_weight=mask)\n",
    "            deepsea_score = sklearn.metrics.roc_auc_score(truth.reshape((-1)), deepsea.reshape((-1)), sample_weight=mask)\n",
    "            \n",
    "            # Masked micro average\n",
    "            tf.logging.info(\"Our AUC:     \" + str(our_score))\n",
    "            tf.logging.info(\"DeepSea AUC: \" + str(deepsea_score))\n",
    "            \n",
    "#             return deepsea, preds, truth, our_score > deepsea_score\n",
    "        \n",
    "        return deepsea, preds, truth, False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic(Model):\n",
    "    def body_fn(self):\n",
    "        return tf.layers.dense(self.x, self.num_outputs, kernel_regularizer=tf.contrib.layers.l1_l2_regularizer(self.l1, self.l2))\n",
    "    \n",
    "class MLP(Model):\n",
    "    def __init__(self,\n",
    "             layers,\n",
    "             num_units,\n",
    "             activation,\n",
    "             *args,\n",
    "             **kwargs):\n",
    "\n",
    "        self.layers = layers\n",
    "        self.num_units = num_units\n",
    "        self.activation = activation\n",
    "        \n",
    "        Model.__init__(self, *args, **kwargs)\n",
    "            \n",
    "    def body_fn(self):\n",
    "        model = self.x\n",
    "        \n",
    "        if not isinstance(self.num_units, collections.Iterable):\n",
    "            self.num_units = [self.num_units] * self.layers\n",
    "            \n",
    "        for i in range(self.layers):\n",
    "            model = tf.layers.dense(model, self.num_units[i], self.activation, kernel_regularizer=tf.contrib.layers.l1_l2_regularizer(self.l1, self.l2))\n",
    "            \n",
    "        return tf.layers.dense(model, self.num_outputs, kernel_regularizer=tf.contrib.layers.l1_l2_regularizer(self.l1, self.l2))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imputation_fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-210a55de99b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogistic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m19\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimputation_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlogistic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimp8\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_alt_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparamaterized_imputation_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m19\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdnase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_alt_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparamaterized_imputation_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdnase_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_alt_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparamaterized_imputation_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m19\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'imputation_fn' is not defined"
     ]
    }
   ],
   "source": [
    "logistic = Logistic(train_data, valid_data, 19, shuffle_size=1, generator_fn=imputation_fn)\n",
    "logistic.train(sess, 10000, lr=1e-4)\n",
    "imp8 = logistic.make_alt_dataset(sess, valid_data, 64, 1, 10, paramaterized_imputation_gen(random_mask(.8, 19), 3*19))\n",
    "dnase = logistic.make_alt_dataset(sess, valid_data, 64, 1, 10, paramaterized_imputation_gen(dnase_mask, 3*19))\n",
    "none = logistic.make_alt_dataset(sess, valid_data, 64, 1, 10, paramaterized_imputation_gen(random_mask(0, 19), 3*19))\n",
    "\n",
    "# _ = logistic.test(sess, 40000, log=True)\n",
    "# _ = logistic.test(sess, 40000, log=True, iterator_handle=imp8)\n",
    "# _ = logistic.test(sess, 40000, log=True, iterator_handle=dnase)\n",
    "# _ = logistic.test(sess, 40000, log=True, iterator_handle=none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Our AUC:     0.9764405709451255\n",
      "INFO:tensorflow:DeepSea AUC: 0.9692375728754375\n",
      "INFO:tensorflow:Our AUC:     0.96106445226617\n",
      "INFO:tensorflow:DeepSea AUC: 0.9558341948720454\n",
      "INFO:tensorflow:Our AUC:     0.9749487263329542\n",
      "INFO:tensorflow:DeepSea AUC: 0.9703153792822881\n"
     ]
    }
   ],
   "source": [
    "# mlp = MLP(2, 20, tf.tanh, train_data, valid_data, 19, shuffle_size=1, generator_fn=imputation_fgen)\n",
    "# mlp.train(sess, 20000, lr=1e-3)\n",
    "# mlp.train(sess, 20000, lr=1e-4)\n",
    "# mlp.train(sess, 20000, lr=1e-5)\n",
    "# imp8 = mlp.make_alt_dataset(sess, valid_data, 64, 1, 10, paramaterized_imputation_gen(random_mask(.8, 19), 3*19))\n",
    "# dnase = mlp.make_alt_dataset(sess, valid_data, 64, 1, 10, paramaterized_imputation_gen(dnase_mask, 3*19))\n",
    "# _ = mlp.test(sess, 16000, log=True)\n",
    "for _ in range(3):\n",
    "    _ = mlp.test(sess, 16000, log=True, iterator_handle=imp8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Initializing variables\n",
      "INFO:tensorflow:Starting Training\n",
      "INFO:tensorflow:1000 0.25679076\n",
      "INFO:tensorflow:On validation\n",
      "INFO:tensorflow:Our AUC:     0.9505799394538017\n",
      "INFO:tensorflow:DeepSea AUC: 0.9659993520392898\n",
      "INFO:tensorflow:\n",
      "INFO:tensorflow:2000 0.0693297\n",
      "INFO:tensorflow:On validation\n",
      "INFO:tensorflow:Our AUC:     0.950915429979005\n",
      "INFO:tensorflow:DeepSea AUC: 0.9696383207749399\n",
      "INFO:tensorflow:\n",
      "INFO:tensorflow:3000 0.9588029\n",
      "INFO:tensorflow:On validation\n",
      "INFO:tensorflow:Our AUC:     0.9611603753138751\n",
      "INFO:tensorflow:DeepSea AUC: 0.9674656211640514\n",
      "INFO:tensorflow:\n",
      "INFO:tensorflow:4000 0.059683196\n",
      "INFO:tensorflow:On validation\n",
      "INFO:tensorflow:Our AUC:     0.9716469544871517\n",
      "INFO:tensorflow:DeepSea AUC: 0.9804013362687495\n",
      "INFO:tensorflow:\n",
      "INFO:tensorflow:5000 0.26715124\n",
      "INFO:tensorflow:On validation\n",
      "INFO:tensorflow:Our AUC:     0.9481474271537953\n",
      "INFO:tensorflow:DeepSea AUC: 0.9561572637079102\n",
      "INFO:tensorflow:\n",
      "INFO:tensorflow:6000 0.486679\n",
      "INFO:tensorflow:On validation\n",
      "INFO:tensorflow:Our AUC:     0.9515413277298138\n",
      "INFO:tensorflow:DeepSea AUC: 0.9624254657226413\n",
      "INFO:tensorflow:\n",
      "INFO:tensorflow:7000 0.04003674\n",
      "INFO:tensorflow:On validation\n",
      "INFO:tensorflow:Our AUC:     0.9697869712224231\n",
      "INFO:tensorflow:DeepSea AUC: 0.9758324066546488\n",
      "INFO:tensorflow:\n",
      "INFO:tensorflow:8000 0.22408937\n",
      "INFO:tensorflow:On validation\n",
      "INFO:tensorflow:Our AUC:     0.9677270116468726\n",
      "INFO:tensorflow:DeepSea AUC: 0.9729545051535441\n",
      "INFO:tensorflow:\n",
      "INFO:tensorflow:9000 0.04127088\n",
      "INFO:tensorflow:On validation\n",
      "INFO:tensorflow:Our AUC:     0.9727284400695845\n",
      "INFO:tensorflow:DeepSea AUC: 0.9780647467508929\n",
      "INFO:tensorflow:\n",
      "INFO:tensorflow:10000 0.3382347\n",
      "INFO:tensorflow:On validation\n",
      "INFO:tensorflow:Our AUC:     0.9783686421157772\n",
      "INFO:tensorflow:DeepSea AUC: 0.9781379450916543\n",
      "INFO:tensorflow:\n",
      "INFO:tensorflow:11000 0.3378563\n",
      "INFO:tensorflow:On validation\n",
      "INFO:tensorflow:Our AUC:     0.9597884504579028\n",
      "INFO:tensorflow:DeepSea AUC: 0.9675698057878523\n",
      "INFO:tensorflow:\n",
      "INFO:tensorflow:12000 0.18459809\n",
      "INFO:tensorflow:On validation\n",
      "INFO:tensorflow:Our AUC:     0.9653194028895173\n",
      "INFO:tensorflow:DeepSea AUC: 0.975651313224623\n",
      "INFO:tensorflow:\n",
      "INFO:tensorflow:13000 0.46605104\n",
      "INFO:tensorflow:On validation\n",
      "INFO:tensorflow:Our AUC:     0.9683389505914338\n",
      "INFO:tensorflow:DeepSea AUC: 0.9752603700038416\n",
      "INFO:tensorflow:\n",
      "INFO:tensorflow:14000 0.37022656\n",
      "INFO:tensorflow:On validation\n",
      "INFO:tensorflow:Our AUC:     0.9772729534012912\n",
      "INFO:tensorflow:DeepSea AUC: 0.9807971839194922\n",
      "INFO:tensorflow:\n",
      "INFO:tensorflow:15000 0.34092474\n",
      "INFO:tensorflow:On validation\n",
      "INFO:tensorflow:Our AUC:     0.9639790623146842\n",
      "INFO:tensorflow:DeepSea AUC: 0.9685719849480945\n",
      "INFO:tensorflow:\n",
      "INFO:tensorflow:16000 0.40403575\n",
      "INFO:tensorflow:On validation\n",
      "INFO:tensorflow:Our AUC:     0.9698998211805213\n",
      "INFO:tensorflow:DeepSea AUC: 0.9718567707014425\n",
      "INFO:tensorflow:\n",
      "INFO:tensorflow:17000 0.14359866\n",
      "INFO:tensorflow:On validation\n",
      "INFO:tensorflow:Our AUC:     0.9442444874774368\n",
      "INFO:tensorflow:DeepSea AUC: 0.9535429843969566\n",
      "INFO:tensorflow:\n",
      "INFO:tensorflow:18000 0.13397917\n",
      "INFO:tensorflow:On validation\n",
      "INFO:tensorflow:Our AUC:     0.9652940186359407\n",
      "INFO:tensorflow:DeepSea AUC: 0.9684926988014874\n",
      "INFO:tensorflow:\n",
      "INFO:tensorflow:19000 0.06638811\n",
      "INFO:tensorflow:On validation\n",
      "INFO:tensorflow:Our AUC:     0.942906552280235\n",
      "INFO:tensorflow:DeepSea AUC: 0.9472671400283345\n",
      "INFO:tensorflow:\n",
      "INFO:tensorflow:20000 0.06356317\n",
      "INFO:tensorflow:On validation\n",
      "INFO:tensorflow:Our AUC:     0.9689453300470499\n",
      "INFO:tensorflow:DeepSea AUC: 0.9722992174746908\n",
      "INFO:tensorflow:\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP(2, 20, tf.tanh, train_data, valid_data, 19, shuffle_size=1, generator_fn=paramaterized_imputation_gen(dnase_mask, 3*19))\n",
    "mlp.train(sess, 20000, lr=1e-3)\n",
    "# for i in range(19):\n",
    "#     tf.logging.info(i)\n",
    "#     mask = mlp.make_alt_dataset(sess, valid_data, 64, 1, 10, paramaterized_imputation_gen(position_mask([i]), 3*19))\n",
    "#     _ = mlp.test(sess, 30000, log=True, iterator_handle=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
