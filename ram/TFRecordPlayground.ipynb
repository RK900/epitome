{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dna_encoder(seq, bases='ACTG'):\n",
    "    # one-hot-encoding for sequence data\n",
    "    # enumerates base in a sequence\n",
    "    indices = [\n",
    "        bases.index(x) if x in bases else -1\n",
    "        for x in seq\n",
    "    ]\n",
    "    # one extra index for unknown\n",
    "    eye = np.eye(len(bases) + 1)\n",
    "    return eye[indices].astype(np.float32)\n",
    "\n",
    "def tf_dna_encoder(seq, bases='ACTG'):\n",
    "    # wraps `dna_encoder` with a `py_func`\n",
    "    return tf.py_func(dna_encoder, [seq, bases], [tf.float32])[0]\n",
    "\n",
    "def dataset_input_fn(filenames,\n",
    "                     buffer_size=10000,\n",
    "                     batch_size=32,\n",
    "                     num_epochs=20,\n",
    "                     ):\n",
    "    dataset = tf.contrib.data.TFRecordDataset(filenames)\n",
    "    \n",
    "    # Use `tf.parse_single_example()` to extract data from a `tf.Example`\n",
    "    # protocol buffer, and perform any additional per-record preprocessing.\n",
    "    def parser(record):\n",
    "        keys_to_features = {\n",
    "            \"sequence\": tf.FixedLenFeature((), tf.string),\n",
    "            \"atacCounts\": tf.FixedLenFeature((1000,), tf.int64),\n",
    "            \"Labels\": tf.FixedLenFeature((1,), tf.int64),\n",
    "        }\n",
    "        parsed = tf.parse_single_example(record, keys_to_features)\n",
    "\n",
    "        # Perform additional preprocessing on the parsed data.\n",
    "        seq = tf_dna_encoder(parsed[\"sequence\"])\n",
    "        seq = tf.reshape(seq, [1000, 5])\n",
    "        atac = parsed[\"atacCounts\"]\n",
    "        label = parsed[\"Labels\"]\n",
    "\n",
    "        # add more here if needed\n",
    "        return {'seq': seq, 'atac': atac}, label\n",
    "    \n",
    "    # Use `Dataset.map()` to build a pair of a feature dictionary and a label\n",
    "    # tensor for each example.\n",
    "    dataset = dataset.map(parser)\n",
    "    dataset = dataset.shuffle(buffer_size)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    # `features` is a dictionary in which each value is a batch of values for\n",
    "    # that feature; `labels` is a batch of labels.\n",
    "    features, labels = iterator.get_next()\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data pipeline parameters\n",
    "buffer_size = 10000\n",
    "batch_size = 64\n",
    "num_epochs = 1\n",
    "\n",
    "# reset the graph because it might be finalized\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# sharded tfrecord filenames\n",
    "filenames = glob.glob('../../deleteme/CEBPB-A549-hg38.txt/part-r-*')\n",
    "features, labels = dataset_input_fn(filenames=filenames,\n",
    "                                    buffer_size=buffer_size,\n",
    "                                    num_epochs=num_epochs)\n",
    "\n",
    "sy_seq_n = tf.cast(features['seq'], tf.float32)\n",
    "sy_atac_n = tf.cast(features['atac'], tf.float32)\n",
    "sy_label_n = tf.cast(labels, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.train.MonitoredSession() as sess:\n",
    "    # `sess.should_stop()` will indicate if the dataflow runs out\n",
    "    # loop breaks once a certain number of epochs has passed\n",
    "    while not sess.should_stop():\n",
    "        print(np.mean(sess.run(sy_atac_n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
